{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/jori152b-medinf/KP_MedInf\n",
      "/data/horse/ws/jori152b-medinf/KP_MedInf/model_development\n"
     ]
    }
   ],
   "source": [
    "# only for taurus\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"model_development\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths and separator\n",
    "DATA_PATH_stages = \"data/extracted/kdigo_stages_measured.csv\"\n",
    "DATA_PATH_labs = \"data/extracted/labs_original.csv\"\n",
    "DATA_PATH_labs_extended = \"data/extracted/labs_extended.csv\"\n",
    "DATA_PATH_labs_new = \"data/extracted/labs_new.csv\"\n",
    "# DATA_PATH_vitals = \"../data/extracted/vitals-kdigo_stages_measured.csv\"\n",
    "DATA_PATH_vitals = \"data/extracted/vitals.csv\"\n",
    "# DATA_PATH_vents = \"../data/extracted/vents-vasopressor-sedatives-kdigo_stages_measured.csv\"\n",
    "DATA_PATH_vents = \"data/extracted/vents_vasopressor_sedatives.csv\"\n",
    "# DATA_PATH_detail = \"../data/extracted/icustay_detail-kdigo_stages_measured.csv\"\n",
    "DATA_PATH_detail = \"data/extracted/icustay_detail.csv\"\n",
    "DATA_PATH_heightweight = \"data/extracted/heightweight.csv\"\n",
    "DATA_PATH_calcium = \"data/extracted/calcium.csv\"\n",
    "DATA_PATH_inr_max = \"data/extracted/inr_max.csv\"\n",
    "SEPARATOR = \";\"\n",
    "\n",
    "# Constants\n",
    "IMPUTE_EACH_ID = False\n",
    "IMPUTE_COLUMN = False\n",
    "TESTING = False\n",
    "TEST_SIZE = 0.05\n",
    "SPLIT_SIZE = 0.2\n",
    "MAX_DAYS = 35\n",
    "CLASS1 = True\n",
    "ALL_STAGES = False\n",
    "MAX_FEATURE_SET = True\n",
    "FIRST_TURN_POS = True\n",
    "TIME_SAMPLING = True\n",
    "SAMPLING_INTERVAL = '6H'\n",
    "RESAMPLE_LIMIT = 16\n",
    "MOST_COMMON = False\n",
    "IMPUTE_METHOD = 'most_frequent'\n",
    "FILL_VALUE = 0\n",
    "ADULTS_MIN_AGE = 18\n",
    "ADULTS_MAX_AGE = 120\n",
    "NORMALIZATION = 'min-max'\n",
    "HOURS_AHEAD = 48\n",
    "NORM_TYPE = 'min_max'\n",
    "RANDOM = 42\n",
    "\n",
    "def filter_by_length_of_stay(X):\n",
    "    drop_list = []\n",
    "    long_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) > MAX_DAYS)\n",
    "\n",
    "    for icustay_id, is_long in long_stays.items():\n",
    "        if is_long:\n",
    "            max_time = X[X['icustay_id'] == icustay_id]['charttime'].max() - pd.to_timedelta(MAX_DAYS, unit='D')\n",
    "            X = X[~((X['icustay_id'] == icustay_id) & (X['charttime'] < max_time))]\n",
    "\n",
    "    short_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) < (HOURS_AHEAD/24))\n",
    "    drop_list = short_stays[short_stays].index.tolist()\n",
    "\n",
    "    X = X[~X.icustay_id.isin(drop_list)]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "3737147\n",
      "aki_stage\n",
      "0    3000795\n",
      "2     353266\n",
      "3     207759\n",
      "1     175327\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "X = pd.read_csv(DATA_PATH_stages, sep=SEPARATOR)\n",
    "X.drop([\"aki_stage_creat\", \"aki_stage_uo\"], axis=1, inplace=True)\n",
    "X = X.dropna(how='all', subset=['creat', 'uo_rt_6hr', 'uo_rt_12hr', 'uo_rt_24hr', 'aki_stage'])\n",
    "X['charttime'] = pd.to_datetime(X['charttime'])\n",
    "\n",
    "print(len(X))\n",
    "print(X['aki_stage'].value_counts())\n",
    "\n",
    "dataset_detail = pd.read_csv(DATA_PATH_detail, sep=SEPARATOR)\n",
    "# original data\n",
    "# out data\n",
    "dataset_detail.drop(['dod', 'admittime', 'dischtime', 'los_hospital', 'ethnicity', \n",
    "                     'hospital_expire_flag', 'hospstay_seq', 'first_hosp_stay', 'intime', \n",
    "                     'outtime', 'los_icu', 'icustay_seq', 'first_icu_stay', 'ethnicity_grouped'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "dataset_labs = pd.read_csv(DATA_PATH_labs, sep=SEPARATOR)\n",
    "dataset_labs = dataset_labs.dropna(subset=['charttime']).dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "dataset_labs['charttime'] = pd.to_datetime(dataset_labs['charttime'])\n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'])\n",
    "dataset_labs.drop(['albumin_min', 'albumin_max','bilirubin_min', 'bilirubin_max','bands_min', 'bands_max',\n",
    "                   'lactate_min', 'lactate_max','platelet_min', 'platelet_max','ptt_min', 'ptt_max', \n",
    "                   'inr_min', 'inr_max', 'pt_min', 'pt_max'], axis = 1, inplace = True)\n",
    "# Calculate mean for each pair and drop original columns\n",
    "column_pairs = [('aniongap_min', 'aniongap_max'), ('albumin_min', 'albumin_max'), \n",
    "                ('bands_min', 'bands_max'), ('bicarbonate_min', 'bicarbonate_max'), \n",
    "                ('bilirubin_min', 'bilirubin_max'), ('creatinine_min', 'creatinine_max'), \n",
    "                ('chloride_min', 'chloride_max'), ('glucose_min', 'glucose_max'), \n",
    "                ('hematocrit_min', 'hematocrit_max'), ('hemoglobin_min', 'hemoglobin_max'), \n",
    "                ('lactate_min', 'lactate_max'), ('platelet_min', 'platelet_max'), \n",
    "                ('potassium_min', 'potassium_max'), ('ptt_min', 'ptt_max'), \n",
    "                ('inr_min', 'inr_max'), ('pt_min', 'pt_max'), ('sodium_min', 'sodium_max'), \n",
    "                ('bun_min', 'bun_max'), ('wbc_min', 'wbc_max')]\n",
    "\n",
    "for min_col, max_col in column_pairs:\n",
    "    try:\n",
    "        mean_col = min_col.rsplit('_', 1)[0] + '_mean'\n",
    "        dataset_labs[mean_col] = dataset_labs[[min_col, max_col]].mean(axis=1)\n",
    "        dataset_labs.drop([min_col, max_col], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "dataset_labs_extended = pd.read_csv(DATA_PATH_labs_extended, sep=SEPARATOR)\n",
    "dataset_labs_extended = dataset_labs_extended.dropna(subset=['charttime']).dropna(subset=dataset_labs_extended.columns[4:], how='all')\n",
    "dataset_labs_extended['charttime'] = pd.to_datetime(dataset_labs_extended['charttime'])\n",
    "dataset_labs_extended = dataset_labs_extended.sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "column_pairs_extended = [('aniongap_min', 'aniongap_max'), ('albumin_min', 'albumin_max'), \n",
    "                ('bands_min', 'bands_max'), ('bicarbonate_min', 'bicarbonate_max'), \n",
    "                ('bilirubin_min', 'bilirubin_max'), ('creatinine_min', 'creatinine_max'), \n",
    "                ('chloride_min', 'chloride_max'), ('glucose_min', 'glucose_max'), \n",
    "                ('hematocrit_min', 'hematocrit_max'), ('hemoglobin_min', 'hemoglobin_max'), \n",
    "                ('lactate_min', 'lactate_max'), ('platelet_min', 'platelet_max'), \n",
    "                ('potassium_min', 'potassium_max'), ('ptt_min', 'ptt_max'), \n",
    "                ('inr_min', 'inr_max'), ('pt_min', 'pt_max'), ('sodium_min', 'sodium_max'), \n",
    "                ('bun_min', 'bun_max'), ('wbc_min', 'wbc_max'), \n",
    "                ('gfr_min', 'gfr_max'), ('phosphate_min', 'phosphate_max'),('uric_acid_min', 'uric_acid_max'), \n",
    "                ('calcium_min', 'calcium_max')]\n",
    "\n",
    "\n",
    "for min_col, max_col in column_pairs_extended:\n",
    "    mean_col = min_col.rsplit('_', 1)[0] + '_mean'\n",
    "    dataset_labs_extended[mean_col] = dataset_labs_extended[[min_col, max_col]].mean(axis=1)\n",
    "    dataset_labs_extended.drop([min_col, max_col], axis=1, inplace=True)\n",
    "dataset_labs_extended.drop(['gfr_mean'], axis=1, inplace=True)\n",
    "\n",
    "dataset_vitals = pd.read_csv(DATA_PATH_vitals, sep=SEPARATOR)\n",
    "dataset_vents = pd.read_csv(DATA_PATH_vents, sep=SEPARATOR)\n",
    "dataset_vitals.drop([\"heartrate_min\", \"heartrate_max\", \"sysbp_min\", \"sysbp_max\", \"diasbp_min\", \"diasbp_max\",\n",
    "                        'meanbp_min', 'meanbp_max', 'tempc_min', 'tempc_max', \"resprate_min\", \"resprate_max\", \n",
    "                        \"spo2_min\", \"spo2_max\", \"glucose_min\", \"glucose_max\"], axis=1, inplace=True)\n",
    "dataset_vitals['charttime'] = pd.to_datetime(dataset_vitals['charttime'])\n",
    "dataset_vents['charttime'] = pd.to_datetime(dataset_vents['charttime'])\n",
    "dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')\n",
    "dataset_vitals = dataset_vitals.sort_values(by=['icustay_id', 'charttime'])\n",
    "dataset_vents = dataset_vents.sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "dataset_heightweight = pd.read_csv(DATA_PATH_heightweight, sep=SEPARATOR)\n",
    "dataset_heightweight = dataset_heightweight.dropna(subset=['icustay_id', 'height_first', 'weight_first'], how='all')\n",
    "dataset_heightweight = dataset_heightweight.sort_values(by=['icustay_id'])\n",
    "\n",
    "dataset_calcium = pd.read_csv(DATA_PATH_calcium, sep=SEPARATOR)\n",
    "dataset_calcium.drop([\"hadm_id\"], axis=1, inplace=True)\n",
    "dataset_calcium['charttime'] = pd.to_datetime(dataset_calcium['charttime'])\n",
    "dataset_calcium = dataset_calcium.sort_values(by=['icustay_id', 'charttime'])\n",
    "dataset_calcium.drop([\"subject_id\"], axis=1, inplace=True)\n",
    "\n",
    "dataset_inr_max = pd.read_csv(DATA_PATH_inr_max, sep=SEPARATOR)\n",
    "dataset_inr_max.drop([\"hadm_id\", \"subject_id\"], axis=1, inplace=True)\n",
    "dataset_inr_max = dataset_inr_max.sort_values(by=['icustay_id'])\n",
    "\n",
    "\n",
    "\n",
    "# Merge datasets\n",
    "if MAX_FEATURE_SET:\n",
    "    \n",
    "    # Perform merge operations and then drop the 'subject_id' column\n",
    "    X_extended = X.merge(dataset_labs_extended, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "                   .merge(dataset_vitals, on=[\"icustay_id\", \"charttime\", \"subject_id\", \"hadm_id\"], how=\"outer\") \\\n",
    "                   .merge(dataset_vents, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "                   .merge(dataset_calcium, on=[\"icustay_id\", \"charttime\"], how=\"outer\")\n",
    "    X_extended.drop([\"subject_id\"], axis=1, inplace=True)\n",
    "    \n",
    "    X_original = X.merge(dataset_labs, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "                  .merge(dataset_vitals, on=[\"icustay_id\", \"charttime\", \"subject_id\", \"hadm_id\"], how=\"outer\") \\\n",
    "                  .merge(dataset_vents, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "                  .merge(dataset_calcium, on=[\"icustay_id\", \"charttime\"], how=\"outer\")\n",
    "    X_original.drop([\"subject_id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use subset of X for testing\n",
    "unique_ids_subset = X['icustay_id'].unique()[:100]  # Get the first 100 unique icustay_id values\n",
    "X = X[X['icustay_id'].isin(unique_ids_subset)]  # Filter rows in X\n",
    "X_extended = X_extended[X_extended['icustay_id'].isin(unique_ids_subset)]\n",
    "X_original = X_original[X_original['icustay_id'].isin(unique_ids_subset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering patients by age and length of stay...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1312113/1803750554.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  long_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) > MAX_DAYS)\n",
      "/tmp/ipykernel_1312113/1803750554.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  short_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) < (HOURS_AHEAD/24))\n",
      "/tmp/ipykernel_1312113/1803750554.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  long_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) > MAX_DAYS)\n",
      "/tmp/ipykernel_1312113/1803750554.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  short_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) < (HOURS_AHEAD/24))\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering patients by age and length of stay...\")\n",
    "# Filtering patients by age and length of stay\n",
    "dataset_detail = dataset_detail[dataset_detail['admission_age'] >= ADULTS_MIN_AGE]\n",
    "adults_icustay_id_list = dataset_detail['icustay_id'].unique()\n",
    "X = X[X.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id', 'charttime'])\n",
    "X_extended = X_extended[X_extended.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id', 'charttime'])\n",
    "X_original = X_original[X_original.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "# X = filter_by_length_of_stay(X)\n",
    "X_extended = filter_by_length_of_stay(X_extended)\n",
    "X_original = filter_by_length_of_stay(X_original)\n",
    "dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['aki_stage']\n",
    "skip = ['icustay_id', 'charttime', 'aki_stage']\n",
    "discrete_feat = ['sedative', 'vasopressor', 'vent', 'hadm_id']\n",
    "skip.extend(discrete_feat)    \n",
    "numeric_feat = list(X.columns.difference(skip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "X_original.to_csv('data/analysis/data_original_preprocessed_filtered_before_resampling.csv', index=False)\n",
    "X_extended.to_csv('data/analysis/data_extended_preprocessed_filtered_before_resampling.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../data/analysis/data_preprocessed_extended__filtered_before_resampling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create unresampled shifted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_extended.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11478\n"
     ]
    }
   ],
   "source": [
    "print(X['aki_stage'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option1: fill all labels since past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1312113/378773068.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  X_shifted = grouped.apply(shift_aki_stage)\n"
     ]
    }
   ],
   "source": [
    "# binarize aki_stage labels\n",
    "X['aki_stage'] = (X['aki_stage'] > 0).astype(int)\n",
    "# create X_shifted where, for each icuststay_id, an aki_stage > 0 is also applied to all timestamps in the 48 hours before\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X is your original dataframe\n",
    "# First, make sure X is sorted by icustay_id and charttime\n",
    "X = X.sort_values(['icustay_id', 'charttime'])\n",
    "\n",
    "# Create a copy of X to modify\n",
    "X_shifted = X.copy()\n",
    "\n",
    "# Convert charttime to datetime if it's not already\n",
    "X_shifted['charttime'] = pd.to_datetime(X_shifted['charttime'])\n",
    "\n",
    "# Group by icustay_id\n",
    "grouped = X_shifted.groupby('icustay_id')\n",
    "\n",
    "# Function to shift aki_stage\n",
    "def shift_aki_stage(group):\n",
    "    # Find where aki_stage > 0\n",
    "    aki_events = group[group['aki_stage'] > 0]\n",
    "    \n",
    "    for _, event in aki_events.iterrows():\n",
    "        # Calculate the time 48 hours before the event\n",
    "        time_before = event['charttime'] - pd.Timedelta(hours=48)\n",
    "        \n",
    "        # Set aki_stage to 1 for all rows in the 48-hour window before the event\n",
    "        mask = (group['charttime'] >= time_before) & (group['charttime'] <= event['charttime'])\n",
    "        group.loc[mask, 'aki_stage'] = 1\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Apply the function to each group\n",
    "X_shifted = grouped.apply(shift_aki_stage)\n",
    "\n",
    "# Reset index if needed\n",
    "X_shifted = X_shifted.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "7536107\n"
     ]
    }
   ],
   "source": [
    "# print number of rows in X where aki_stage is nan\n",
    "print(X_shifted['aki_stage'].isna().sum())\n",
    "print(X['aki_stage'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aki_stage\n",
      "0.0    79.691991\n",
      "2.0     9.808070\n",
      "3.0     5.596306\n",
      "1.0     4.903633\n",
      "Name: proportion, dtype: float64\n",
      "aki_stage\n",
      "1    51.789364\n",
      "0    48.210636\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X['aki_stage'].value_counts(normalize=True) * 100)\n",
    "print(X_shifted['aki_stage'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "X_shifted.to_csv('data/analysis/data_shifted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option2: fill only window in the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1312113/181169435.py:41: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  X_shifted_window = grouped.apply(shift_aki_stage)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X is your original dataframe\n",
    "# First, make sure X is sorted by icustay_id and charttime\n",
    "X = X.sort_values(['icustay_id', 'charttime'])\n",
    "\n",
    "# Create a copy of X to modify\n",
    "X_shifted_window = X.copy()\n",
    "\n",
    "# Convert charttime to datetime if it's not already\n",
    "X_shifted_window['charttime'] = pd.to_datetime(X_shifted_window['charttime'])\n",
    "\n",
    "# Group by icustay_id\n",
    "grouped = X_shifted_window.groupby('icustay_id')\n",
    "\n",
    "# Function to shift aki_stage\n",
    "def shift_aki_stage(group):\n",
    "    # Find where aki_stage > 0\n",
    "    aki_events = group[group['aki_stage'] > 0].copy()\n",
    "    \n",
    "    for idx, event in aki_events.iterrows():\n",
    "        # Calculate the time 48 and 40 hours before the event\n",
    "        time_before_48 = event['charttime'] - pd.Timedelta(hours=48)\n",
    "        time_before_40 = event['charttime'] - pd.Timedelta(hours=40)\n",
    "        \n",
    "        # Find rows in the 40-48 hour window before the event\n",
    "        mask = (group['charttime'] >= time_before_48) & (group['charttime'] <= time_before_40)\n",
    "        rows_to_update = group[mask]\n",
    "        \n",
    "        if not rows_to_update.empty:\n",
    "            # Update the aki_stage for these rows\n",
    "            group.loc[rows_to_update.index, 'aki_stage'] = 1\n",
    "            \n",
    "            # Remove the original event\n",
    "            group.loc[idx, 'aki_stage'] = 0\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Apply the function to each group\n",
    "X_shifted_window = grouped.apply(shift_aki_stage)\n",
    "\n",
    "# Reset index if needed\n",
    "X_shifted_window = X_shifted_window.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5970197\n",
      "7536107\n"
     ]
    }
   ],
   "source": [
    "# print number of rows in X where aki_stage is nan\n",
    "print(X_shifted_window['aki_stage'].isna().sum())\n",
    "print(X['aki_stage'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aki_stage\n",
      "0.0    79.691991\n",
      "2.0     9.808070\n",
      "3.0     5.596306\n",
      "1.0     4.903633\n",
      "Name: proportion, dtype: float64\n",
      "aki_stage\n",
      "0.0    51.128208\n",
      "1.0    47.391171\n",
      "2.0     1.198435\n",
      "3.0     0.282187\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X['aki_stage'].value_counts(normalize=True) * 100)\n",
    "print(X_shifted_window['aki_stage'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shifted_window.to_csv('data/analysis/data_shifted_window.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## looped dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_extended.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_INTERVALS = ['1H', '2H', '4H', '6H', '8H', '12H', '24H']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_detail = dataset_detail[dataset_detail['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "try:\n",
    "    # theirs\n",
    "    dataset_detail = pd.get_dummies(dataset_detail, columns=['gender', 'ethnicity_grouped'])\n",
    "except:\n",
    "    #ours\n",
    "    dataset_detail = pd.get_dummies(dataset_detail, columns=['gender'])\n",
    "dataset_detail.drop(['subject_id', 'hadm_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['icustay_id', 'admission_age', 'gender_F', 'gender_M'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_detail.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1312113/3901902011.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  X_resampled = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging sampled features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1312113/3901902011.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  X_resampled = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging sampled features\n"
     ]
    }
   ],
   "source": [
    "# saving every sampling interval\n",
    "# Resampling\n",
    "for SAMPLING_INTERVAL in SAMPLING_INTERVALS:\n",
    "    if TIME_SAMPLING:\n",
    "        \n",
    "        # Set index and group by 'icustay_id' before resampling\n",
    "        X_resampled = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n",
    "        \n",
    "        # Resample and aggregate features\n",
    "        if MAX_FEATURE_SET:\n",
    "            X_discrete = X_resampled[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "        X_numeric = X_resampled[numeric_feat].mean()\n",
    "        X_label = X_resampled['aki_stage'].max()\n",
    "\n",
    "        print(\"Merging sampled features\")\n",
    "        try:\n",
    "            X_resampled = pd.concat([X_numeric, X_discrete, X_label], axis=1).reset_index()\n",
    "        except:\n",
    "            X_resampled = pd.concat([X_numeric, X_label], axis=1).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "    # Forward fill again after resampling\n",
    "    X_resampled['aki_stage'] = X_resampled.groupby('icustay_id')['aki_stage'].ffill(limit=RESAMPLE_LIMIT).fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "    # Ensure binary values (convert any positive number to 1)\n",
    "    X_resampled['aki_stage'] = (X_resampled['aki_stage'] > 0).astype(int)\n",
    "\n",
    "    # Shifting labels\n",
    "    shift_steps = HOURS_AHEAD // int(SAMPLING_INTERVAL[:-1])\n",
    "    X_resampled['aki_stage'] = X_resampled.groupby('icustay_id')['aki_stage'].shift(-shift_steps)\n",
    "    X_resampled = X_resampled.dropna(subset=['aki_stage'])\n",
    "\n",
    "    # Merging not time-dependent data\n",
    "\n",
    "    X_resampled = X_resampled.merge(dataset_detail, on='icustay_id')\n",
    "    X_resampled = X_resampled.merge(dataset_heightweight, on='icustay_id')\n",
    "    X_resampled = X_resampled.merge(dataset_inr_max, on='icustay_id')\n",
    "\n",
    "    # If no imputation method selected or only impute each id, for the remaining nan impute direclty with FILL_VALUE\n",
    "    X_resampled = X_resampled.fillna(FILL_VALUE) \n",
    "\n",
    "\n",
    "    # Save preprocessed data\n",
    "    X_resampled.to_csv(f'data/preprocessed/preprocessed_data_extended_{SAMPLING_INTERVAL}.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normal ds creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_924951/2522398281.py:11: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging sampled features\n"
     ]
    }
   ],
   "source": [
    "# Resampling\n",
    "if TIME_SAMPLING:\n",
    "    \n",
    "    label = ['aki_stage']\n",
    "    skip = ['icustay_id', 'charttime', 'aki_stage']\n",
    "    discrete_feat = ['sedative', 'vasopressor', 'vent', 'hadm_id']\n",
    "    skip.extend(discrete_feat)    \n",
    "    numeric_feat = list(X.columns.difference(skip))\n",
    "    \n",
    "    # Set index and group by 'icustay_id' before resampling\n",
    "    X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n",
    "    \n",
    "    # Resample and aggregate features\n",
    "    if MAX_FEATURE_SET:\n",
    "        X_discrete = X[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "    X_numeric = X[numeric_feat].mean()\n",
    "    X_label = X['aki_stage'].max()\n",
    "\n",
    "    print(\"Merging sampled features\")\n",
    "    try:\n",
    "        X = pd.concat([X_numeric, X_discrete, X_label], axis=1).reset_index()\n",
    "    except:\n",
    "        X = pd.concat([X_numeric, X_label], axis=1).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# Forward fill again after resampling\n",
    "X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].ffill(limit=RESAMPLE_LIMIT).fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# Ensure binary values (convert any positive number to 1)\n",
    "X['aki_stage'] = (X['aki_stage'] > 0).astype(int)\n",
    "\n",
    "# Shifting labels\n",
    "shift_steps = HOURS_AHEAD // int(SAMPLING_INTERVAL[:-1])\n",
    "X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].shift(-shift_steps)\n",
    "X = X.dropna(subset=['aki_stage'])\n",
    "\n",
    "# Merging not time-dependent data\n",
    "dataset_detail_merging = dataset_detail[dataset_detail['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "# theirs\n",
    "# dataset_detail = pd.get_dummies(dataset_detail, columns=['gender', 'ethnicity_grouped'])\n",
    "#ours\n",
    "dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender'])\n",
    "dataset_detail_merging.drop(['subject_id', 'hadm_id'], axis=1, inplace=True)\n",
    "X = X.merge(dataset_detail_merging, on='icustay_id')\n",
    "X = X.merge(dataset_heightweight, on='icustay_id')\n",
    "X = X.merge(dataset_inr_max, on='icustay_id')\n",
    "\n",
    "# If no imputation method selected or only impute each id, for the remaining nan impute direclty with FILL_VALUE\n",
    "X = X.fillna(FILL_VALUE) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save preprocessed data\n",
    "X.to_csv('data/preprocessed/preprocessed_data_original.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medinf",
   "language": "python",
   "name": "medinf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
