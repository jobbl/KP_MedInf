{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths and separator\n",
    "\n",
    "data_path = r\"/home/jori152b/DIR/horse/jori152b-medinf/KP_MedInf/model_development/data\"\n",
    "\n",
    "DATA_PATH_stages = os.path.join(data_path, \"extracted\", \"kdigo_stages_measured.csv\")\n",
    "DATA_PATH_labs = os.path.join(data_path, \"extracted\", \"labs_original.csv\")\n",
    "DATA_PATH_labs_extended = os.path.join(data_path, \"extracted\", \"labs_extended.csv\")\n",
    "DATA_PATH_labs_new = os.path.join(data_path, \"extracted\", \"labs_new.csv\")\n",
    "DATA_PATH_vitals = os.path.join(data_path, \"extracted\", \"vitals.csv\")\n",
    "DATA_PATH_vents = os.path.join(data_path, \"extracted\", \"vents_vasopressor_sedatives.csv\")\n",
    "DATA_PATH_detail = os.path.join(data_path, \"extracted\", \"icustay_detail.csv\")\n",
    "DATA_PATH_heightweight = os.path.join(data_path, \"extracted\", \"heightweight.csv\")\n",
    "DATA_PATH_calcium = os.path.join(data_path, \"extracted\", \"calcium.csv\")\n",
    "DATA_PATH_inr_max = os.path.join(data_path, \"extracted\", \"inr_max.csv\")\n",
    "\n",
    "SEPARATOR = \";\"\n",
    "\n",
    "# Constants\n",
    "IMPUTE_EACH_ID = False\n",
    "IMPUTE_COLUMN = False\n",
    "TESTING = False\n",
    "TEST_SIZE = 0.05\n",
    "SPLIT_SIZE = 0.2\n",
    "MAX_DAYS = 35\n",
    "CLASS1 = True\n",
    "ALL_STAGES = False\n",
    "MAX_FEATURE_SET = True\n",
    "FIRST_TURN_POS = True\n",
    "TIME_SAMPLING = True\n",
    "SAMPLING_INTERVAL = '6H'\n",
    "RESAMPLE_LIMIT = 16\n",
    "MOST_COMMON = False\n",
    "IMPUTE_METHOD = 'most_frequent'\n",
    "FILL_VALUE = 0\n",
    "ADULTS_MIN_AGE = 18\n",
    "ADULTS_MAX_AGE = 120\n",
    "NORMALIZATION = 'min-max'\n",
    "HOURS_AHEAD = 48\n",
    "NORM_TYPE = 'min_max'\n",
    "RANDOM = 42\n",
    "\n",
    "def filter_by_length_of_stay(X):\n",
    "    drop_list = []\n",
    "    long_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) > MAX_DAYS)\n",
    "\n",
    "    for icustay_id, is_long in long_stays.items():\n",
    "        if is_long:\n",
    "            max_time = X[X['icustay_id'] == icustay_id]['charttime'].max() - pd.to_timedelta(MAX_DAYS, unit='D')\n",
    "            X = X[~((X['icustay_id'] == icustay_id) & (X['charttime'] < max_time))]\n",
    "\n",
    "    short_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) < (HOURS_AHEAD/24))\n",
    "    drop_list = short_stays[short_stays].index.tolist()\n",
    "\n",
    "    X = X[~X.icustay_id.isin(drop_list)]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "X = pd.read_csv(DATA_PATH_stages, sep=SEPARATOR)\n",
    "print(\"X\", X.columns)\n",
    "X.drop([\"aki_stage_creat\", \"aki_stage_uo\"], axis=1, inplace=True)\n",
    "X = X.dropna(how='all', subset=['creat', 'uo_rt_6hr', 'uo_rt_12hr', 'uo_rt_24hr', 'aki_stage'])\n",
    "X['charttime'] = pd.to_datetime(X['charttime'])\n",
    "\n",
    "print(len(X))\n",
    "print(X['aki_stage'].value_counts())\n",
    "\n",
    "dataset_detail = pd.read_csv(DATA_PATH_detail, sep=SEPARATOR)\n",
    "print(\"dataset_detail\", dataset_detail.columns)\n",
    "# original data\n",
    "# out data\n",
    "dataset_detail.drop(['dod', 'admittime', 'dischtime', 'los_hospital', 'ethnicity', \n",
    "                     'hospital_expire_flag', 'hospstay_seq', 'first_hosp_stay', 'intime', \n",
    "                     'outtime', 'los_icu', 'icustay_seq', 'first_icu_stay', 'ethnicity_grouped'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "dataset_labs = pd.read_csv(DATA_PATH_labs, sep=SEPARATOR)\n",
    "dataset_labs.drop(['glucose_min', 'glucose_max','creatinine_min', 'creatinine_max'], axis = 1, inplace = True)\n",
    "print(\"dataset_labs\", dataset_labs.columns) \n",
    "dataset_labs = dataset_labs.dropna(subset=['charttime']).dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "dataset_labs['charttime'] = pd.to_datetime(dataset_labs['charttime'])\n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'])\n",
    "dataset_labs.drop(['albumin_min', 'albumin_max','bilirubin_min', 'bilirubin_max','bands_min', 'bands_max',\n",
    "                   'lactate_min', 'lactate_max','platelet_min', 'platelet_max','ptt_min', 'ptt_max', \n",
    "                   'inr_min', 'inr_max', 'pt_min', 'pt_max'], axis = 1, inplace = True)\n",
    "# Calculate mean for each pair and drop original columns\n",
    "column_pairs = [('aniongap_min', 'aniongap_max'), ('albumin_min', 'albumin_max'), \n",
    "                ('bands_min', 'bands_max'), ('bicarbonate_min', 'bicarbonate_max'), \n",
    "                ('bilirubin_min', 'bilirubin_max'), \n",
    "                ('chloride_min', 'chloride_max'), \n",
    "                ('hematocrit_min', 'hematocrit_max'), ('hemoglobin_min', 'hemoglobin_max'), \n",
    "                ('lactate_min', 'lactate_max'), ('platelet_min', 'platelet_max'), \n",
    "                ('potassium_min', 'potassium_max'), ('ptt_min', 'ptt_max'), \n",
    "                ('inr_min', 'inr_max'), ('pt_min', 'pt_max'), ('sodium_min', 'sodium_max'), \n",
    "                ('bun_min', 'bun_max'), ('wbc_min', 'wbc_max')]\n",
    "\n",
    "for min_col, max_col in column_pairs:\n",
    "    try:\n",
    "        mean_col = min_col.rsplit('_', 1)[0] + '_mean'\n",
    "        dataset_labs[mean_col] = dataset_labs[[min_col, max_col]].mean(axis=1)\n",
    "        dataset_labs.drop([min_col, max_col], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "dataset_labs_extended = pd.read_csv(DATA_PATH_labs_extended, sep=SEPARATOR)\n",
    "dataset_labs_extended.drop(['glucose_min', 'glucose_max','creatinine_min', 'creatinine_max'], axis = 1, inplace = True)\n",
    "\n",
    "print(\"dataset_labs_extended\", dataset_labs_extended.columns)\n",
    "dataset_labs_extended = dataset_labs_extended.dropna(subset=['charttime']).dropna(subset=dataset_labs_extended.columns[4:], how='all')\n",
    "dataset_labs_extended['charttime'] = pd.to_datetime(dataset_labs_extended['charttime'])\n",
    "dataset_labs_extended = dataset_labs_extended.sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "column_pairs_extended = [('aniongap_min', 'aniongap_max'), ('albumin_min', 'albumin_max'), \n",
    "                ('bands_min', 'bands_max'), ('bicarbonate_min', 'bicarbonate_max'), \n",
    "                ('bilirubin_min', 'bilirubin_max'), \n",
    "                ('chloride_min', 'chloride_max'), \n",
    "                ('hematocrit_min', 'hematocrit_max'), ('hemoglobin_min', 'hemoglobin_max'), \n",
    "                ('lactate_min', 'lactate_max'), ('platelet_min', 'platelet_max'), \n",
    "                ('potassium_min', 'potassium_max'), ('ptt_min', 'ptt_max'), \n",
    "                ('inr_min', 'inr_max'), ('pt_min', 'pt_max'), ('sodium_min', 'sodium_max'), \n",
    "                ('bun_min', 'bun_max'), ('wbc_min', 'wbc_max'), \n",
    "                ('gfr_min', 'gfr_max'), ('phosphate_min', 'phosphate_max'),('uric_acid_min', 'uric_acid_max'), \n",
    "                ('calcium_min', 'calcium_max')]\n",
    "\n",
    "\n",
    "for min_col, max_col in column_pairs_extended:\n",
    "    mean_col = min_col.rsplit('_', 1)[0] + '_mean'\n",
    "    dataset_labs_extended[mean_col] = dataset_labs_extended[[min_col, max_col]].mean(axis=1)\n",
    "    dataset_labs_extended.drop([min_col, max_col], axis=1, inplace=True)\n",
    "dataset_labs_extended.drop(['gfr_mean'], axis=1, inplace=True)\n",
    "\n",
    "dataset_vitals = pd.read_csv(DATA_PATH_vitals, sep=SEPARATOR)\n",
    "print(\"dataset_vitals\", dataset_vitals.columns)\n",
    "dataset_vitals.drop([\"glucose_min\", \"glucose_max\"], axis=1, inplace=True)\n",
    "dataset_vents = pd.read_csv(DATA_PATH_vents, sep=SEPARATOR)\n",
    "print(\"dataset_vents\", dataset_vents.columns)\n",
    "dataset_vitals.drop([\"heartrate_min\", \"heartrate_max\", \"sysbp_min\", \"sysbp_max\", \"diasbp_min\", \"diasbp_max\",\n",
    "                        'meanbp_min', 'meanbp_max', 'tempc_min', 'tempc_max', \"resprate_min\", \"resprate_max\", \n",
    "                        \"spo2_min\", \"spo2_max\"], axis=1, inplace=True)\n",
    "dataset_vitals['charttime'] = pd.to_datetime(dataset_vitals['charttime'])\n",
    "dataset_vents['charttime'] = pd.to_datetime(dataset_vents['charttime'])\n",
    "dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')\n",
    "dataset_vitals = dataset_vitals.sort_values(by=['icustay_id', 'charttime'])\n",
    "dataset_vents = dataset_vents.sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "dataset_heightweight = pd.read_csv(DATA_PATH_heightweight, sep=SEPARATOR)\n",
    "print(\"dataset_heightweight\", dataset_heightweight.columns)\n",
    "dataset_heightweight = dataset_heightweight.dropna(subset=['icustay_id', 'height_first', 'weight_first'], how='all')\n",
    "dataset_heightweight = dataset_heightweight.sort_values(by=['icustay_id'])\n",
    "\n",
    "dataset_calcium = pd.read_csv(DATA_PATH_calcium, sep=SEPARATOR)\n",
    "print(\"dataset_calcium\", dataset_calcium.columns)\n",
    "dataset_calcium.drop([\"hadm_id\"], axis=1, inplace=True)\n",
    "dataset_calcium['charttime'] = pd.to_datetime(dataset_calcium['charttime'])\n",
    "dataset_calcium = dataset_calcium.sort_values(by=['icustay_id', 'charttime'])\n",
    "dataset_calcium.drop([\"subject_id\"], axis=1, inplace=True)\n",
    "\n",
    "dataset_inr_max = pd.read_csv(DATA_PATH_inr_max, sep=SEPARATOR)\n",
    "print(\"dataset_inr_max\", dataset_inr_max.columns)\n",
    "dataset_inr_max.drop([\"hadm_id\", \"subject_id\"], axis=1, inplace=True)\n",
    "dataset_inr_max = dataset_inr_max.sort_values(by=['icustay_id'])\n",
    "\n",
    "# Merge datasets\n",
    "if MAX_FEATURE_SET:\n",
    "    \n",
    "    # Perform merge operations and then drop the 'subject_id' column\n",
    "    X_extended = X.merge(dataset_labs_extended, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "                   .merge(dataset_vitals, on=[\"icustay_id\", \"charttime\", \"subject_id\", \"hadm_id\"], how=\"outer\") \\\n",
    "                   .merge(dataset_vents, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "                   .merge(dataset_calcium, on=[\"icustay_id\", \"charttime\"], how=\"outer\")\n",
    "    X_extended.drop([\"subject_id\"], axis=1, inplace=True)\n",
    "    \n",
    "    X_original = X.merge(dataset_labs, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "                  .merge(dataset_vitals, on=[\"icustay_id\", \"charttime\", \"subject_id\", \"hadm_id\"], how=\"outer\") \\\n",
    "                  .merge(dataset_vents, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "                #   .merge(dataset_calcium, on=[\"icustay_id\", \"charttime\"], how=\"outer\")\n",
    "    X_original.drop([\"subject_id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_extended\", X_extended.columns)\n",
    "print(\"X_original\", X_original.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filtering patients by age and length of stay...\")\n",
    "# Filtering patients by age and length of stay\n",
    "dataset_detail = dataset_detail[dataset_detail['admission_age'] >= ADULTS_MIN_AGE]\n",
    "adults_icustay_id_list = dataset_detail['icustay_id'].unique()\n",
    "X = X[X.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id', 'charttime'])\n",
    "X_extended = X_extended[X_extended.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id', 'charttime'])\n",
    "X_original = X_original[X_original.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "X = filter_by_length_of_stay(X)\n",
    "X_extended = filter_by_length_of_stay(X_extended)\n",
    "X_original = filter_by_length_of_stay(X_original)\n",
    "dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical features\n",
    "dataset_detail_ours = pd.read_csv(DATA_PATH_detail, sep=SEPARATOR)\n",
    "# original data\n",
    "# out data\n",
    "dataset_detail_ours.drop(['dod', 'admittime', 'dischtime', 'los_hospital', 'ethnicity', \n",
    "                     'hospital_expire_flag', 'hospstay_seq', 'first_hosp_stay', 'intime', \n",
    "                     'outtime', 'los_icu', 'icustay_seq', 'first_icu_stay', 'ethnicity_grouped'], axis=1, inplace=True)\n",
    "\n",
    "# categorical features\n",
    "dataset_detail_theirs = pd.read_csv(DATA_PATH_detail, sep=SEPARATOR)\n",
    "# original data\n",
    "# out data\n",
    "dataset_detail_theirs.drop(['dod', 'admittime', 'dischtime', 'los_hospital', 'ethnicity', \n",
    "                     'hospital_expire_flag', 'hospstay_seq', 'first_hosp_stay', 'intime', \n",
    "                     'outtime', 'los_icu', 'icustay_seq', 'first_icu_stay'], axis=1, inplace=True)\n",
    "\n",
    "dataset_heightweight = pd.read_csv(DATA_PATH_heightweight, sep=SEPARATOR)\n",
    "dataset_heightweight = dataset_heightweight.dropna(subset=['icustay_id', 'height_first', 'weight_first'], how='all')\n",
    "dataset_heightweight = dataset_heightweight.sort_values(by=['icustay_id'])\n",
    "dataset_inr_max = pd.read_csv(DATA_PATH_inr_max, sep=SEPARATOR)\n",
    "dataset_inr_max.drop([\"hadm_id\", \"subject_id\"], axis=1, inplace=True)\n",
    "dataset_inr_max = dataset_inr_max.sort_values(by=['icustay_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets unresampled for data exploration\n",
    "datasets = {'X_original': X_original, 'X_extended': X_extended}\n",
    "for dataset, X in datasets.items():\n",
    "    X = datasets[dataset].copy()\n",
    "\n",
    "    if dataset == \"X_original\":\n",
    "        print('dataset is X_original')\n",
    "        # Merging not time-dependent data\n",
    "        dataset_detail_merging = dataset_detail_theirs.copy()\n",
    "        print(dataset_detail_merging.columns)\n",
    "        dataset_detail_merging = dataset_detail_merging[dataset_detail_merging['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "        dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender', 'ethnicity_grouped'])\n",
    "        dataset_detail_merging.drop(['subject_id', 'hadm_id'], axis=1, inplace=True)\n",
    "        X = X.merge(dataset_detail_merging, on='icustay_id')\n",
    "        print(X.columns)\n",
    "\n",
    "    elif dataset == \"X_extended\":\n",
    "        print('dataset is X_extended')\n",
    "        dataset_detail_merging = dataset_detail_ours.copy()\n",
    "        dataset_detail_merging = dataset_detail_merging[dataset_detail_merging['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "        dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender'])\n",
    "        dataset_detail_merging.drop(['subject_id', 'hadm_id'], axis=1, inplace=True)\n",
    "        X = X.merge(dataset_detail_merging, on='icustay_id')\n",
    "        X = X.merge(dataset_heightweight, on='icustay_id')\n",
    "        X = X.merge(dataset_inr_max, on='icustay_id')   \n",
    "        print(X.columns)\n",
    "        \n",
    "    # save preprocessed data\n",
    "    # X.to_csv(os.path.join(data_path, 'preprocessed', f'{dataset}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets = {'X_original':X_original, 'X_extended':X_extended}\n",
    "SAMPLING_INTERVALS = ['2H', '4H', '6H', '8H', '12H', '24H']\n",
    "\n",
    "label = ['aki_stage']\n",
    "skip = ['icustay_id', 'charttime', 'aki_stage']\n",
    "discrete_feat = ['sedative', 'vasopressor', 'vent']\n",
    "skip.extend(discrete_feat)    \n",
    "\n",
    "resampled_dir = 'resampled_correct'\n",
    "if not os.path.exists(os.path.join(data_path, resampled_dir)):\n",
    "    os.makedirs(os.path.join(data_path, resampled_dir))\n",
    "    \n",
    "for dataset in datasets:\n",
    "    numeric_feat = list(datasets[dataset].columns.difference(skip))\n",
    "    for SAMPLING_INTERVAL in SAMPLING_INTERVALS:\n",
    "        RESAMPLE_LIMIT = int(SAMPLING_INTERVAL[:-1]) * 96//int(SAMPLING_INTERVAL[:-1])\n",
    "        X = datasets[dataset].copy()\n",
    "        # Resampling\n",
    "        if TIME_SAMPLING:\n",
    "            \n",
    "            # Set index and group by 'icustay_id' before resampling\n",
    "            X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n",
    "            \n",
    "            # Resample and aggregate features\n",
    "            if MAX_FEATURE_SET:\n",
    "                X_discrete = X[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "            X_numeric = X[numeric_feat].mean()\n",
    "            X_label = X['aki_stage'].max()\n",
    "\n",
    "            print(\"Merging sampled features\")\n",
    "            try:\n",
    "                X = pd.concat([X_numeric, X_discrete, X_label], axis=1).reset_index()\n",
    "            except:\n",
    "                X = pd.concat([X_numeric, X_label], axis=1).reset_index()\n",
    "\n",
    "\n",
    "        # Forward fill again after resampling\n",
    "        X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].ffill(limit=RESAMPLE_LIMIT).fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "        # Ensure binary values (convert any positive number to 1)\n",
    "        X['aki_stage'] = (X['aki_stage'] > 0).astype(int)\n",
    "\n",
    "        # Shifting labels\n",
    "        shift_steps = HOURS_AHEAD // int(SAMPLING_INTERVAL[:-1])\n",
    "        X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].shift(-shift_steps)\n",
    "        X = X.dropna(subset=['aki_stage'])\n",
    "\n",
    "        if dataset is \"X_original\":\n",
    "            print('dataset is X_original')\n",
    "            # Merging not time-dependent data\n",
    "            dataset_detail_merging = dataset_detail_theirs.copy()\n",
    "            print(dataset_detail_merging.columns)\n",
    "            dataset_detail_merging = dataset_detail_merging[dataset_detail_merging['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "            dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender', 'ethnicity_grouped'])\n",
    "            dataset_detail_merging.drop(['subject_id', 'hadm_id'], axis=1, inplace=True)\n",
    "            X = X.merge(dataset_detail_merging, on='icustay_id')\n",
    "\n",
    "        elif dataset is \"X_extended\":\n",
    "            print('dataset is X_extended')\n",
    "            dataset_detail_merging = dataset_detail_ours.copy()\n",
    "            dataset_detail_merging = dataset_detail_merging[dataset_detail_merging['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "            dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender'])\n",
    "            dataset_detail_merging.drop(['subject_id', 'hadm_id'], axis=1, inplace=True)\n",
    "            X = X.merge(dataset_detail_merging, on='icustay_id')\n",
    "            X = X.merge(dataset_heightweight, on='icustay_id')\n",
    "            X = X.merge(dataset_inr_max, on='icustay_id')   \n",
    "\n",
    "        X = X.fillna(FILL_VALUE) \n",
    "        # save the data\n",
    "        X.to_csv(os.path.join(data_path, resampled_dir, f'aki_stage_{dataset}_{SAMPLING_INTERVAL}.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel (for hpc)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Number of CPU cores to use\n",
    "n_jobs = 32  # You can adjust this if necessary\n",
    "\n",
    "datasets = {'X_original': X_original, 'X_extended': X_extended}\n",
    "SAMPLING_INTERVALS = ['1H', '2H', '4H', '6H', '8H', '12H', '24H']\n",
    "\n",
    "label = ['aki_stage']\n",
    "skip = ['icustay_id', 'charttime', 'aki_stage']\n",
    "discrete_feat = ['sedative', 'vasopressor', 'vent']\n",
    "skip.extend(discrete_feat)\n",
    "\n",
    "resampled_dir = 'resampled_correct'\n",
    "if not os.path.exists(os.path.join(data_path, resampled_dir)):\n",
    "    os.makedirs(os.path.join(data_path, resampled_dir))\n",
    "\n",
    "\n",
    "def process_resampling(dataset_name, sampling_interval):\n",
    "    dataset = datasets[dataset_name]\n",
    "    numeric_feat = list(dataset.columns.difference(skip))\n",
    "\n",
    "    RESAMPLE_LIMIT = int(sampling_interval[:-1]) * 96 // int(sampling_interval[:-1])\n",
    "    X = dataset.copy()\n",
    "\n",
    "    # Resampling\n",
    "    if TIME_SAMPLING:\n",
    "        # Set index and group by 'icustay_id' before resampling\n",
    "        X = X.set_index('charttime').groupby('icustay_id').resample(sampling_interval)\n",
    "\n",
    "        # Resample and aggregate features\n",
    "        if MAX_FEATURE_SET:\n",
    "            X_discrete = X[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "        X_numeric = X[numeric_feat].mean()\n",
    "        X_label = X['aki_stage'].max()\n",
    "\n",
    "        # Merge sampled features\n",
    "        try:\n",
    "            X = pd.concat([X_numeric, X_discrete, X_label], axis=1).reset_index()\n",
    "        except:\n",
    "            X = pd.concat([X_numeric, X_label], axis=1).reset_index()\n",
    "\n",
    "    # Forward fill again after resampling\n",
    "    X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].ffill(limit=RESAMPLE_LIMIT).fillna(0)\n",
    "\n",
    "    # Ensure binary values (convert any positive number to 1)\n",
    "    X['aki_stage'] = (X['aki_stage'] > 0).astype(int)\n",
    "\n",
    "    # Shifting labels\n",
    "    shift_steps = HOURS_AHEAD // int(sampling_interval[:-1])\n",
    "    X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].shift(-shift_steps)\n",
    "    X = X.dropna(subset=['aki_stage'])\n",
    "\n",
    "    if dataset_name == \"X_original\":\n",
    "        # Merging not time-dependent data\n",
    "        dataset_detail_merging = dataset_detail_theirs.copy()\n",
    "        dataset_detail_merging = dataset_detail_merging[dataset_detail_merging['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "        dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender', 'ethnicity_grouped'])\n",
    "        dataset_detail_merging.drop(['subject_id', 'hadm_id'], axis=1, inplace=True)\n",
    "        X = X.merge(dataset_detail_merging, on='icustay_id')\n",
    "\n",
    "    elif dataset_name == \"X_extended\":\n",
    "        dataset_detail_merging = dataset_detail_ours.copy()\n",
    "        dataset_detail_merging = dataset_detail_merging[dataset_detail_merging['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "        dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender'])\n",
    "        dataset_detail_merging.drop(['subject_id', 'hadm_id'], axis=1, inplace=True)\n",
    "        X = X.merge(dataset_detail_merging, on='icustay_id')\n",
    "        X = X.merge(dataset_heightweight, on='icustay_id')\n",
    "        X = X.merge(dataset_inr_max, on='icustay_id')\n",
    "\n",
    "    X = X.fillna(FILL_VALUE)\n",
    "\n",
    "    # Save the data\n",
    "    file_path = os.path.join(data_path, resampled_dir, f'aki_stage_{dataset_name}_{sampling_interval}.csv')\n",
    "    X.to_csv(file_path, index=False)\n",
    "\n",
    "\n",
    "# Parallel processing of the resampling\n",
    "Parallel(n_jobs=n_jobs)(\n",
    "    delayed(process_resampling)(dataset_name, sampling_interval)\n",
    "    for dataset_name in datasets.keys()\n",
    "    for sampling_interval in SAMPLING_INTERVALS\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
