{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Uni\\KP_MedInf\\continuous-aki-predict\\model_development\\notebooks\n",
      "d:\\Uni\\KP_MedInf\\continuous-aki-predict\\model_development\n"
     ]
    }
   ],
   "source": [
    "# only for taurus\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"model_development\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths and separator\n",
    "DATA_PATH_stages = \"data/extracted/kdigo_stages_measured.csv\"\n",
    "DATA_PATH_labs = \"data/extracted/labs_original.csv\"\n",
    "DATA_PATH_labs_extended = \"data/extracted/labs_extended.csv\"\n",
    "DATA_PATH_labs_new = \"data/extracted/labs_new.csv\"\n",
    "# DATA_PATH_vitals = \"../data/extracted/vitals-kdigo_stages_measured.csv\"\n",
    "DATA_PATH_vitals = \"data/extracted/vitals.csv\"\n",
    "# DATA_PATH_vents = \"../data/extracted/vents-vasopressor-sedatives-kdigo_stages_measured.csv\"\n",
    "DATA_PATH_vents = \"data/extracted/vents_vasopressor_sedatives.csv\"\n",
    "# DATA_PATH_detail = \"../data/extracted/icustay_detail-kdigo_stages_measured.csv\"\n",
    "DATA_PATH_detail = \"data/extracted/icustay_detail.csv\"\n",
    "DATA_PATH_heightweight = \"data/extracted/heightweight.csv\"\n",
    "DATA_PATH_calcium = \"data/extracted/calcium.csv\"\n",
    "DATA_PATH_inr_max = \"data/extracted/inr_max.csv\"\n",
    "SEPARATOR = \";\"\n",
    "\n",
    "# Constants\n",
    "IMPUTE_EACH_ID = False\n",
    "IMPUTE_COLUMN = False\n",
    "TESTING = False\n",
    "TEST_SIZE = 0.05\n",
    "SPLIT_SIZE = 0.2\n",
    "MAX_DAYS = 35\n",
    "CLASS1 = True\n",
    "ALL_STAGES = False\n",
    "MAX_FEATURE_SET = True\n",
    "FIRST_TURN_POS = True\n",
    "TIME_SAMPLING = True\n",
    "SAMPLING_INTERVAL = '6H'\n",
    "RESAMPLE_LIMIT = 16\n",
    "MOST_COMMON = False\n",
    "IMPUTE_METHOD = 'most_frequent'\n",
    "FILL_VALUE = 0\n",
    "ADULTS_MIN_AGE = 18\n",
    "ADULTS_MAX_AGE = 120\n",
    "NORMALIZATION = 'min-max'\n",
    "HOURS_AHEAD = 48\n",
    "NORM_TYPE = 'min_max'\n",
    "RANDOM = 42\n",
    "\n",
    "def filter_by_length_of_stay(X):\n",
    "    drop_list = []\n",
    "    long_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) > MAX_DAYS)\n",
    "\n",
    "    for icustay_id, is_long in long_stays.items():\n",
    "        if is_long:\n",
    "            max_time = X[X['icustay_id'] == icustay_id]['charttime'].max() - pd.to_timedelta(MAX_DAYS, unit='D')\n",
    "            X = X[~((X['icustay_id'] == icustay_id) & (X['charttime'] < max_time))]\n",
    "\n",
    "    short_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) < (HOURS_AHEAD/24))\n",
    "    drop_list = short_stays[short_stays].index.tolist()\n",
    "\n",
    "    X = X[~X.icustay_id.isin(drop_list)]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "3737147\n",
      "aki_stage\n",
      "0    3000795\n",
      "2     353266\n",
      "3     207759\n",
      "1     175327\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "X = pd.read_csv(DATA_PATH_stages, sep=SEPARATOR)\n",
    "X.drop([\"aki_stage_creat\", \"aki_stage_uo\"], axis=1, inplace=True)\n",
    "X = X.dropna(how='all', subset=['creat', 'uo_rt_6hr', 'uo_rt_12hr', 'uo_rt_24hr', 'aki_stage'])\n",
    "X['charttime'] = pd.to_datetime(X['charttime'])\n",
    "\n",
    "print(len(X))\n",
    "print(X['aki_stage'].value_counts())\n",
    "\n",
    "dataset_detail = pd.read_csv(DATA_PATH_detail, sep=SEPARATOR)\n",
    "# original data\n",
    "# out data\n",
    "dataset_detail.drop(['dod', 'admittime', 'dischtime', 'los_hospital', 'ethnicity', \n",
    "                     'hospital_expire_flag', 'hospstay_seq', 'first_hosp_stay', 'intime', \n",
    "                     'outtime', 'los_icu', 'icustay_seq', 'first_icu_stay', 'ethnicity_grouped'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "dataset_labs = pd.read_csv(DATA_PATH_labs, sep=SEPARATOR)\n",
    "dataset_labs = dataset_labs.dropna(subset=['charttime']).dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "dataset_labs['charttime'] = pd.to_datetime(dataset_labs['charttime'])\n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'])\n",
    "dataset_labs.drop(['albumin_min', 'albumin_max','bilirubin_min', 'bilirubin_max','bands_min', 'bands_max',\n",
    "                   'lactate_min', 'lactate_max','platelet_min', 'platelet_max','ptt_min', 'ptt_max', \n",
    "                   'inr_min', 'inr_max', 'pt_min', 'pt_max'], axis = 1, inplace = True)\n",
    "# Calculate mean for each pair and drop original columns\n",
    "column_pairs = [('aniongap_min', 'aniongap_max'), ('albumin_min', 'albumin_max'), \n",
    "                ('bands_min', 'bands_max'), ('bicarbonate_min', 'bicarbonate_max'), \n",
    "                ('bilirubin_min', 'bilirubin_max'), ('creatinine_min', 'creatinine_max'), \n",
    "                ('chloride_min', 'chloride_max'), ('glucose_min', 'glucose_max'), \n",
    "                ('hematocrit_min', 'hematocrit_max'), ('hemoglobin_min', 'hemoglobin_max'), \n",
    "                ('lactate_min', 'lactate_max'), ('platelet_min', 'platelet_max'), \n",
    "                ('potassium_min', 'potassium_max'), ('ptt_min', 'ptt_max'), \n",
    "                ('inr_min', 'inr_max'), ('pt_min', 'pt_max'), ('sodium_min', 'sodium_max'), \n",
    "                ('bun_min', 'bun_max'), ('wbc_min', 'wbc_max')]\n",
    "\n",
    "for min_col, max_col in column_pairs:\n",
    "    try:\n",
    "        mean_col = min_col.rsplit('_', 1)[0] + '_mean'\n",
    "        dataset_labs[mean_col] = dataset_labs[[min_col, max_col]].mean(axis=1)\n",
    "        dataset_labs.drop([min_col, max_col], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "dataset_labs_extended = pd.read_csv(DATA_PATH_labs_extended, sep=SEPARATOR)\n",
    "dataset_labs_extended = dataset_labs_extended.dropna(subset=['charttime']).dropna(subset=dataset_labs_extended.columns[4:], how='all')\n",
    "dataset_labs_extended['charttime'] = pd.to_datetime(dataset_labs_extended['charttime'])\n",
    "dataset_labs_extended = dataset_labs_extended.sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "column_pairs_extended = [('aniongap_min', 'aniongap_max'), ('albumin_min', 'albumin_max'), \n",
    "                ('bands_min', 'bands_max'), ('bicarbonate_min', 'bicarbonate_max'), \n",
    "                ('bilirubin_min', 'bilirubin_max'), ('creatinine_min', 'creatinine_max'), \n",
    "                ('chloride_min', 'chloride_max'), ('glucose_min', 'glucose_max'), \n",
    "                ('hematocrit_min', 'hematocrit_max'), ('hemoglobin_min', 'hemoglobin_max'), \n",
    "                ('lactate_min', 'lactate_max'), ('platelet_min', 'platelet_max'), \n",
    "                ('potassium_min', 'potassium_max'), ('ptt_min', 'ptt_max'), \n",
    "                ('inr_min', 'inr_max'), ('pt_min', 'pt_max'), ('sodium_min', 'sodium_max'), \n",
    "                ('bun_min', 'bun_max'), ('wbc_min', 'wbc_max'), \n",
    "                ('gfr_min', 'gfr_max'), ('phosphate_min', 'phosphate_max'),('uric_acid_min', 'uric_acid_max'), \n",
    "                ('calcium_min', 'calcium_max')]\n",
    "\n",
    "\n",
    "for min_col, max_col in column_pairs_extended:\n",
    "    mean_col = min_col.rsplit('_', 1)[0] + '_mean'\n",
    "    dataset_labs_extended[mean_col] = dataset_labs_extended[[min_col, max_col]].mean(axis=1)\n",
    "    dataset_labs_extended.drop([min_col, max_col], axis=1, inplace=True)\n",
    "dataset_labs_extended.drop(['gfr_mean'], axis=1, inplace=True)\n",
    "\n",
    "dataset_vitals = pd.read_csv(DATA_PATH_vitals, sep=SEPARATOR)\n",
    "dataset_vents = pd.read_csv(DATA_PATH_vents, sep=SEPARATOR)\n",
    "dataset_vitals.drop([\"heartrate_min\", \"heartrate_max\", \"sysbp_min\", \"sysbp_max\", \"diasbp_min\", \"diasbp_max\",\n",
    "                        'meanbp_min', 'meanbp_max', 'tempc_min', 'tempc_max', \"resprate_min\", \"resprate_max\", \n",
    "                        \"spo2_min\", \"spo2_max\", \"glucose_min\", \"glucose_max\"], axis=1, inplace=True)\n",
    "dataset_vitals['charttime'] = pd.to_datetime(dataset_vitals['charttime'])\n",
    "dataset_vents['charttime'] = pd.to_datetime(dataset_vents['charttime'])\n",
    "dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')\n",
    "dataset_vitals = dataset_vitals.sort_values(by=['icustay_id', 'charttime'])\n",
    "dataset_vents = dataset_vents.sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "dataset_heightweight = pd.read_csv(DATA_PATH_heightweight, sep=SEPARATOR)\n",
    "dataset_heightweight = dataset_heightweight.dropna(subset=['icustay_id', 'height_first', 'weight_first'], how='all')\n",
    "dataset_heightweight = dataset_heightweight.sort_values(by=['icustay_id'])\n",
    "\n",
    "dataset_calcium = pd.read_csv(DATA_PATH_calcium, sep=SEPARATOR)\n",
    "dataset_calcium.drop([\"hadm_id\"], axis=1, inplace=True)\n",
    "dataset_calcium['charttime'] = pd.to_datetime(dataset_calcium['charttime'])\n",
    "dataset_calcium = dataset_calcium.sort_values(by=['icustay_id', 'charttime'])\n",
    "dataset_calcium.drop([\"subject_id\"], axis=1, inplace=True)\n",
    "\n",
    "dataset_inr_max = pd.read_csv(DATA_PATH_inr_max, sep=SEPARATOR)\n",
    "dataset_inr_max.drop([\"hadm_id\", \"subject_id\"], axis=1, inplace=True)\n",
    "dataset_inr_max = dataset_inr_max.sort_values(by=['icustay_id'])\n",
    "\n",
    "\n",
    "\n",
    "# Merge datasets\n",
    "if MAX_FEATURE_SET:\n",
    "    \n",
    "    # Perform merge operations and then drop the 'subject_id' column\n",
    "    X_extended = X.merge(dataset_labs_extended, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "                   .merge(dataset_vitals, on=[\"icustay_id\", \"charttime\", \"subject_id\", \"hadm_id\"], how=\"outer\") \\\n",
    "                   .merge(dataset_vents, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "                   .merge(dataset_calcium, on=[\"icustay_id\", \"charttime\"], how=\"outer\")\n",
    "    X_extended.drop([\"subject_id\"], axis=1, inplace=True)\n",
    "    \n",
    "    # X_original = X.merge(dataset_labs, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "    #               .merge(dataset_vitals, on=[\"icustay_id\", \"charttime\", \"subject_id\", \"hadm_id\"], how=\"outer\") \\\n",
    "    #               .merge(dataset_vents, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "    #               .merge(dataset_calcium, on=[\"icustay_id\", \"charttime\"], how=\"outer\")\n",
    "    # X_original.drop([\"subject_id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use subset of X for testing\n",
    "unique_ids_subset = X['icustay_id'].unique()[:100]  # Get the first 100 unique icustay_id values\n",
    "X = X[X['icustay_id'].isin(unique_ids_subset)]  # Filter rows in X\n",
    "X_extended = X_extended[X_extended['icustay_id'].isin(unique_ids_subset)]\n",
    "X_original = X_original[X_original['icustay_id'].isin(unique_ids_subset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "42\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(len(X.columns))\n",
    "print(len(X_extended.columns))\n",
    "print(len(X_original.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering patients by age and length of stay...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_23848\\1803750554.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  long_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) > MAX_DAYS)\n",
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_23848\\1803750554.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  short_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) < (HOURS_AHEAD/24))\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering patients by age and length of stay...\")\n",
    "# Filtering patients by age and length of stay\n",
    "dataset_detail = dataset_detail[dataset_detail['admission_age'] >= ADULTS_MIN_AGE]\n",
    "adults_icustay_id_list = dataset_detail['icustay_id'].unique()\n",
    "X = X[X.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id', 'charttime'])\n",
    "X_extended = X_extended[X_extended.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id', 'charttime'])\n",
    "# X_original = X_original[X_original.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "# X = filter_by_length_of_stay(X)\n",
    "X_extended = filter_by_length_of_stay(X_extended)\n",
    "# X_original = filter_by_length_of_stay(X_original)\n",
    "dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['aki_stage']\n",
    "skip = ['icustay_id', 'charttime', 'aki_stage']\n",
    "discrete_feat = ['sedative', 'vasopressor', 'vent', 'hadm_id']\n",
    "skip.extend(discrete_feat)    \n",
    "numeric_feat = list(X_extended.columns.difference(skip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(len(numeric_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "X_original.to_csv('data/analysis/data_original_preprocessed_filtered_before_resampling.csv', index=False)\n",
    "X_extended.to_csv('data/analysis/data_extended_preprocessed_filtered_before_resampling.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/analysis/data_extended_preprocessed_filtered_before_resampling.csv')\n",
    "# column_test = pd.read_csv('data\\preprocessed\\preprocessed_data_24H.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['icustay_id', 'charttime', 'aniongap_mean', 'bicarbonate_mean',\n",
       "       'bun_mean', 'calcium', 'chloride_mean', 'creat', 'creatinine_mean',\n",
       "       'diasbp_mean', 'glucose_mean_x', 'glucose_mean_y', 'heartrate_mean',\n",
       "       'hematocrit_mean', 'hemoglobin_mean', 'meanbp_mean', 'potassium_mean',\n",
       "       'resprate_mean', 'sodium_mean', 'spo2_mean', 'sysbp_mean', 'tempc_mean',\n",
       "       'uo_rt_12hr', 'uo_rt_24hr', 'uo_rt_6hr', 'wbc_mean', 'sedative',\n",
       "       'vasopressor', 'vent', 'hadm_id', 'aki_stage', 'admission_age',\n",
       "       'gender_F', 'gender_M', 'weight_first', 'height_first', 'inr_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_test.drop(['icustay_id', 'charttime', 'aki_stage', 'hadm_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(column_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical features\n",
    "dataset_detail = pd.read_csv(DATA_PATH_detail, sep=SEPARATOR)\n",
    "# original data\n",
    "# out data\n",
    "dataset_detail.drop(['dod', 'admittime', 'dischtime', 'los_hospital', 'ethnicity', \n",
    "                     'hospital_expire_flag', 'hospstay_seq', 'first_hosp_stay', 'intime', \n",
    "                     'outtime', 'los_icu', 'icustay_seq', 'first_icu_stay', 'ethnicity_grouped'], axis=1, inplace=True)\n",
    "dataset_heightweight = pd.read_csv(DATA_PATH_heightweight, sep=SEPARATOR)\n",
    "dataset_heightweight = dataset_heightweight.dropna(subset=['icustay_id', 'height_first', 'weight_first'], how='all')\n",
    "dataset_heightweight = dataset_heightweight.sort_values(by=['icustay_id'])\n",
    "dataset_inr_max = pd.read_csv(DATA_PATH_inr_max, sep=SEPARATOR)\n",
    "dataset_inr_max.drop([\"hadm_id\", \"subject_id\"], axis=1, inplace=True)\n",
    "dataset_inr_max = dataset_inr_max.sort_values(by=['icustay_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging not time-dependent data\n",
    "dataset_detail_merging = dataset_detail[dataset_detail['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "# theirs\n",
    "# dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender', 'ethnicity_grouped'])\n",
    "#ours\n",
    "dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender'])\n",
    "# dataset_detail_merging.drop(['subject_id', 'hadm_id'], axis=1, inplace=True)\n",
    "\n",
    "X = X.merge(dataset_detail_merging, on='icustay_id')\n",
    "# X = X.merge(dataset_heightweight, on='icustay_id')\n",
    "# X = X.merge(dataset_inr_max, on='icustay_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['subject_id', 'hadm_id_x', 'hadm_id_y', 'aki_stage'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['icustay_id', 'charttime', 'aniongap_mean', 'bicarbonate_mean',\n",
      "       'bun_mean', 'calcium', 'chloride_mean', 'creat', 'creatinine_mean',\n",
      "       'diasbp_mean', 'glucose_mean_x', 'glucose_mean_y', 'heartrate_mean',\n",
      "       'hematocrit_mean', 'hemoglobin_mean', 'meanbp_mean', 'potassium_mean',\n",
      "       'resprate_mean', 'sodium_mean', 'spo2_mean', 'sysbp_mean', 'tempc_mean',\n",
      "       'uo_rt_12hr', 'uo_rt_24hr', 'uo_rt_6hr', 'wbc_mean', 'sedative',\n",
      "       'vasopressor', 'vent', 'aki_stage', 'admission_age', 'gender_F',\n",
      "       'gender_M'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(column_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['icustay_id', 'charttime', 'creat', 'uo_rt_6hr', 'uo_rt_12hr',\n",
      "       'uo_rt_24hr', 'aniongap_mean', 'bicarbonate_mean', 'creatinine_mean',\n",
      "       'chloride_mean', 'glucose_mean_x', 'hematocrit_mean', 'hemoglobin_mean',\n",
      "       'potassium_mean', 'sodium_mean', 'bun_mean', 'wbc_mean',\n",
      "       'heartrate_mean', 'sysbp_mean', 'diasbp_mean', 'meanbp_mean',\n",
      "       'resprate_mean', 'tempc_mean', 'spo2_mean', 'glucose_mean_y', 'vent',\n",
      "       'vasopressor', 'sedative', 'calcium', 'admission_age', 'gender_F',\n",
      "       'gender_M'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# print difference between X.columns and column_test.columns\n",
    "print(set(X.columns) - set(column_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by icustay_id and save all rows of 10 random ids ommiting the icustay_id\n",
    "X_grouped = X.groupby('icustay_id')\n",
    "# pick 10 random ids\n",
    "ids = np.random.choice(X['icustay_id'].unique(), 10)\n",
    "os.makedirs('data/dummy_data_lab_values', exist_ok=True)\n",
    "for id in ids:\n",
    "    # remove icustay_id from the group\n",
    "    group = X_grouped.get_group(id).drop('icustay_id', axis=1)\n",
    "    # save to csv\n",
    "    group.to_csv(f'data/dummy_data_lab_values/{id}_original_preprocessed_filtered_before_resampling.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create unresampled shifted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_extended.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11478\n"
     ]
    }
   ],
   "source": [
    "print(X['aki_stage'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option1: fill all labels since past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1312113/378773068.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  X_shifted = grouped.apply(shift_aki_stage)\n"
     ]
    }
   ],
   "source": [
    "# binarize aki_stage labels\n",
    "X['aki_stage'] = (X['aki_stage'] > 0).astype(int)\n",
    "# create X_shifted where, for each icuststay_id, an aki_stage > 0 is also applied to all timestamps in the 48 hours before\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X is your original dataframe\n",
    "# First, make sure X is sorted by icustay_id and charttime\n",
    "X = X.sort_values(['icustay_id', 'charttime'])\n",
    "\n",
    "# Create a copy of X to modify\n",
    "X_shifted = X.copy()\n",
    "\n",
    "# Convert charttime to datetime if it's not already\n",
    "X_shifted['charttime'] = pd.to_datetime(X_shifted['charttime'])\n",
    "\n",
    "# Group by icustay_id\n",
    "grouped = X_shifted.groupby('icustay_id')\n",
    "\n",
    "# Function to shift aki_stage\n",
    "def shift_aki_stage(group):\n",
    "    # Find where aki_stage > 0\n",
    "    aki_events = group[group['aki_stage'] > 0]\n",
    "    \n",
    "    for _, event in aki_events.iterrows():\n",
    "        # Calculate the time 48 hours before the event\n",
    "        time_before = event['charttime'] - pd.Timedelta(hours=48)\n",
    "        \n",
    "        # Set aki_stage to 1 for all rows in the 48-hour window before the event\n",
    "        mask = (group['charttime'] >= time_before) & (group['charttime'] <= event['charttime'])\n",
    "        group.loc[mask, 'aki_stage'] = 1\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Apply the function to each group\n",
    "X_shifted = grouped.apply(shift_aki_stage)\n",
    "\n",
    "# Reset index if needed\n",
    "X_shifted = X_shifted.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "7536107\n"
     ]
    }
   ],
   "source": [
    "# print number of rows in X where aki_stage is nan\n",
    "print(X_shifted['aki_stage'].isna().sum())\n",
    "print(X['aki_stage'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aki_stage\n",
      "0.0    79.691991\n",
      "2.0     9.808070\n",
      "3.0     5.596306\n",
      "1.0     4.903633\n",
      "Name: proportion, dtype: float64\n",
      "aki_stage\n",
      "1    51.789364\n",
      "0    48.210636\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X['aki_stage'].value_counts(normalize=True) * 100)\n",
    "print(X_shifted['aki_stage'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "X_shifted.to_csv('data/analysis/data_shifted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option2: fill only window in the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1312113/181169435.py:41: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  X_shifted_window = grouped.apply(shift_aki_stage)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X is your original dataframe\n",
    "# First, make sure X is sorted by icustay_id and charttime\n",
    "X = X.sort_values(['icustay_id', 'charttime'])\n",
    "\n",
    "# Create a copy of X to modify\n",
    "X_shifted_window = X.copy()\n",
    "\n",
    "# Convert charttime to datetime if it's not already\n",
    "X_shifted_window['charttime'] = pd.to_datetime(X_shifted_window['charttime'])\n",
    "\n",
    "# Group by icustay_id\n",
    "grouped = X_shifted_window.groupby('icustay_id')\n",
    "\n",
    "# Function to shift aki_stage\n",
    "def shift_aki_stage(group):\n",
    "    # Find where aki_stage > 0\n",
    "    aki_events = group[group['aki_stage'] > 0].copy()\n",
    "    \n",
    "    for idx, event in aki_events.iterrows():\n",
    "        # Calculate the time 48 and 40 hours before the event\n",
    "        time_before_48 = event['charttime'] - pd.Timedelta(hours=48)\n",
    "        time_before_40 = event['charttime'] - pd.Timedelta(hours=40)\n",
    "        \n",
    "        # Find rows in the 40-48 hour window before the event\n",
    "        mask = (group['charttime'] >= time_before_48) & (group['charttime'] <= time_before_40)\n",
    "        rows_to_update = group[mask]\n",
    "        \n",
    "        if not rows_to_update.empty:\n",
    "            # Update the aki_stage for these rows\n",
    "            group.loc[rows_to_update.index, 'aki_stage'] = 1\n",
    "            \n",
    "            # Remove the original event\n",
    "            group.loc[idx, 'aki_stage'] = 0\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Apply the function to each group\n",
    "X_shifted_window = grouped.apply(shift_aki_stage)\n",
    "\n",
    "# Reset index if needed\n",
    "X_shifted_window = X_shifted_window.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5970197\n",
      "7536107\n"
     ]
    }
   ],
   "source": [
    "# print number of rows in X where aki_stage is nan\n",
    "print(X_shifted_window['aki_stage'].isna().sum())\n",
    "print(X['aki_stage'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aki_stage\n",
      "0.0    79.691991\n",
      "2.0     9.808070\n",
      "3.0     5.596306\n",
      "1.0     4.903633\n",
      "Name: proportion, dtype: float64\n",
      "aki_stage\n",
      "0.0    51.128208\n",
      "1.0    47.391171\n",
      "2.0     1.198435\n",
      "3.0     0.282187\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X['aki_stage'].value_counts(normalize=True) * 100)\n",
    "print(X_shifted_window['aki_stage'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shifted_window.to_csv('data/analysis/data_shifted_window.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## looped dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1714"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del X\n",
    "# del X_original\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_extended.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take the first 10000 rows of X for test\n",
    "X = X[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Columns 42\n"
     ]
    }
   ],
   "source": [
    "print('X Columns', len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_INTERVALS = ['1H', '2H', '4H', '6H', '8H', '12H', '24H']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_numeric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_numeric\u001b[38;5;241m.\u001b[39mcolumns))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_numeric' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(X_numeric.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_23848\\1200676958.py:9: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  X_resampled.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.25 GiB for an array with shape (40, 10893536) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TIME_SAMPLING:\n\u001b[0;32m      6\u001b[0m     \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Set index and group by 'icustay_id' before resampling\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     X_resampled \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m----> 9\u001b[0m     X_resampled\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharttime\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mresample(SAMPLING_INTERVAL)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Resample and aggregate features\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m MAX_FEATURE_SET:\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:3743\u001b[0m, in \u001b[0;36mGroupBy.resample\u001b[1;34m(self, rule, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3739\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_resampler_for_grouping\n\u001b[0;32m   3741\u001b[0m \u001b[38;5;66;03m# mypy flags that include_groups could be specified via `*args` or `**kwargs`\u001b[39;00m\n\u001b[0;32m   3742\u001b[0m \u001b[38;5;66;03m# GH#54961 would resolve.\u001b[39;00m\n\u001b[1;32m-> 3743\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_resampler_for_grouping(  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   3744\u001b[0m     \u001b[38;5;28mself\u001b[39m, rule, \u001b[38;5;241m*\u001b[39margs, include_groups\u001b[38;5;241m=\u001b[39minclude_groups, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   3745\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\resample.py:2072\u001b[0m, in \u001b[0;36mget_resampler_for_grouping\u001b[1;34m(groupby, rule, how, fill_method, limit, kind, on, include_groups, **kwargs)\u001b[0m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;66;03m# .resample uses 'on' similar to how .groupby uses 'key'\u001b[39;00m\n\u001b[0;32m   2071\u001b[0m tg \u001b[38;5;241m=\u001b[39m TimeGrouper(freq\u001b[38;5;241m=\u001b[39mrule, key\u001b[38;5;241m=\u001b[39mon, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 2072\u001b[0m resampler \u001b[38;5;241m=\u001b[39m tg\u001b[38;5;241m.\u001b[39m_get_resampler(groupby\u001b[38;5;241m.\u001b[39mobj, kind\u001b[38;5;241m=\u001b[39mkind)\n\u001b[0;32m   2073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resampler\u001b[38;5;241m.\u001b[39m_get_resampler_for_grouping(\n\u001b[0;32m   2074\u001b[0m     groupby\u001b[38;5;241m=\u001b[39mgroupby, include_groups\u001b[38;5;241m=\u001b[39minclude_groups, key\u001b[38;5;241m=\u001b[39mtg\u001b[38;5;241m.\u001b[39mkey\n\u001b[0;32m   2075\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\resample.py:2231\u001b[0m, in \u001b[0;36mTimeGrouper._get_resampler\u001b[1;34m(self, obj, kind)\u001b[0m\n\u001b[0;32m   2229\u001b[0m _, ax, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_grouper(obj, gpr_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   2230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ax, DatetimeIndex):\n\u001b[1;32m-> 2231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatetimeIndexResampler(\n\u001b[0;32m   2232\u001b[0m         obj,\n\u001b[0;32m   2233\u001b[0m         timegrouper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2234\u001b[0m         kind\u001b[38;5;241m=\u001b[39mkind,\n\u001b[0;32m   2235\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis,\n\u001b[0;32m   2236\u001b[0m         group_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_keys,\n\u001b[0;32m   2237\u001b[0m         gpr_index\u001b[38;5;241m=\u001b[39max,\n\u001b[0;32m   2238\u001b[0m     )\n\u001b[0;32m   2239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ax, PeriodIndex) \u001b[38;5;129;01mor\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperiod\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ax, PeriodIndex):\n\u001b[0;32m   2241\u001b[0m         \u001b[38;5;66;03m# GH#53481\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\resample.py:184\u001b[0m, in \u001b[0;36mResampler.__init__\u001b[1;34m(self, obj, timegrouper, axis, kind, gpr_index, group_keys, selection, include_groups)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_groups \u001b[38;5;241m=\u001b[39m include_groups\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39max, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timegrouper\u001b[38;5;241m.\u001b[39m_set_grouper(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_obj(obj), sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, gpr_index\u001b[38;5;241m=\u001b[39mgpr_index\n\u001b[0;32m    186\u001b[0m )\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinner, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_binner()\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;241m=\u001b[39m selection\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\resample.py:2529\u001b[0m, in \u001b[0;36mTimeGrouper._set_grouper\u001b[1;34m(self, obj, sort, gpr_index)\u001b[0m\n\u001b[0;32m   2526\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_grouper\u001b[39m(\n\u001b[0;32m   2527\u001b[0m     \u001b[38;5;28mself\u001b[39m, obj: NDFrameT, sort: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, gpr_index: Index \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2528\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[NDFrameT, Index, npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m-> 2529\u001b[0m     obj, ax, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_set_grouper(obj, sort, gpr_index\u001b[38;5;241m=\u001b[39mgpr_index)\n\u001b[0;32m   2530\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ax\u001b[38;5;241m.\u001b[39mdtype, ArrowDtype) \u001b[38;5;129;01mand\u001b[39;00m ax\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2531\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arrow_dtype \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:407\u001b[0m, in \u001b[0;36mGrouper._set_grouper\u001b[1;34m(self, obj, sort, gpr_index)\u001b[0m\n\u001b[0;32m    403\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indexer_deprecated \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39margsort(\n\u001b[0;32m    404\u001b[0m         kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmergesort\u001b[39m\u001b[38;5;124m\"\u001b[39m, na_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    405\u001b[0m     )\n\u001b[0;32m    406\u001b[0m     ax \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 407\u001b[0m     obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mtake(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m    409\u001b[0m \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# \"NDFrameT\", variable has type \"None\")\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_deprecated \u001b[38;5;241m=\u001b[39m obj  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4128\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4131\u001b[0m     )\n\u001b[1;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mtake(\n\u001b[0;32m   4134\u001b[0m     indices,\n\u001b[0;32m   4135\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_block_manager_axis(axis),\n\u001b[0;32m   4136\u001b[0m     verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   4137\u001b[0m )\n\u001b[0;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4140\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    895\u001b[0m     new_axis\u001b[38;5;241m=\u001b[39mnew_labels,\n\u001b[0;32m    896\u001b[0m     indexer\u001b[38;5;241m=\u001b[39mindexer,\n\u001b[0;32m    897\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    898\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    899\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    900\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:687\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[0;32m    690\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    691\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    692\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[0;32m    693\u001b[0m             ),\n\u001b[0;32m    694\u001b[0m         )\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:688\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[0;32m    690\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    691\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    692\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[0;32m    693\u001b[0m             ),\n\u001b[0;32m    694\u001b[0m         )\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m algos\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m   1308\u001b[0m     values, indexer, axis\u001b[38;5;241m=\u001b[39maxis, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m   1309\u001b[0m )\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:157\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    155\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[0;32m    162\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.25 GiB for an array with shape (40, 10893536) and data type float64"
     ]
    }
   ],
   "source": [
    "# saving every sampling interval\n",
    "original = False\n",
    "# Resampling\n",
    "for SAMPLING_INTERVAL in SAMPLING_INTERVALS:\n",
    "    if TIME_SAMPLING:\n",
    "        \n",
    "        # Set index and group by 'icustay_id' before resampling\n",
    "        X_resampled = X.copy()\n",
    "        X_resampled.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n",
    "        \n",
    "        # Resample and aggregate features\n",
    "        if MAX_FEATURE_SET:\n",
    "            X_discrete = X_resampled[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "        X_numeric = X_resampled[numeric_feat].mean()\n",
    "        X_label = X_resampled['aki_stage'].max()\n",
    "\n",
    "        print(\"Merging sampled features\")\n",
    "        try:\n",
    "            X_resampled = pd.concat([X_numeric, X_discrete, X_label], axis=1).reset_index()\n",
    "        except:\n",
    "            X_resampled = pd.concat([X_numeric, X_label], axis=1).reset_index()\n",
    "\n",
    "    # Forward fill again after resampling\n",
    "    X_resampled['aki_stage'] = X_resampled.groupby('icustay_id')['aki_stage'].ffill(limit=RESAMPLE_LIMIT).fillna(0)\n",
    "    print('X Columns', len(X_resampled.columns))\n",
    "\n",
    "    # Ensure binary values (convert any positive number to 1)\n",
    "    X_resampled['aki_stage'] = (X_resampled['aki_stage'] > 0).astype(int)\n",
    "\n",
    "    # Shifting labels\n",
    "    shift_steps = HOURS_AHEAD // int(SAMPLING_INTERVAL[:-1])\n",
    "    X_resampled['aki_stage'] = X_resampled.groupby('icustay_id')['aki_stage'].shift(-shift_steps)\n",
    "    X_resampled = X_resampled.dropna(subset=['aki_stage'])\n",
    "    print('X Columns', len(X_resampled.columns))\n",
    "    \n",
    "    # Merging not time-dependent data\n",
    "    dataset_detail_merging = dataset_detail[dataset_detail['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "    if original:\n",
    "        # theirs\n",
    "        dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender', 'ethnicity_grouped'])\n",
    "    else:\n",
    "        dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender'])\n",
    "        X_resampled = X_resampled.merge(dataset_heightweight, on='icustay_id')\n",
    "        X_resampled = X_resampled.merge(dataset_inr_max, on='icustay_id')\n",
    "\n",
    "    X_resampled = X_resampled.merge(dataset_detail_merging, on='icustay_id')\n",
    "    print('X Columns', len(X_resampled.columns))\n",
    "\n",
    "\n",
    "    # If no imputation method selected or only impute each id, for the remaining nan impute direclty with FILL_VALUE\n",
    "    X_resampled = X_resampled.fillna(FILL_VALUE) \n",
    "\n",
    "\n",
    "    # Save preprocessed data\n",
    "    X_resampled.to_csv(f'data/preprocessed/preprocessed_data_extended_{SAMPLING_INTERVAL}.csv', index=False)\n",
    "    print('X Columns', len(X_resampled.columns))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normal ds creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_924951/2522398281.py:11: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging sampled features\n"
     ]
    }
   ],
   "source": [
    "# Resampling\n",
    "if TIME_SAMPLING:\n",
    "    \n",
    "    label = ['aki_stage']\n",
    "    skip = ['icustay_id', 'charttime', 'aki_stage']\n",
    "    discrete_feat = ['sedative', 'vasopressor', 'vent', 'hadm_id']\n",
    "    skip.extend(discrete_feat)    \n",
    "    numeric_feat = list(X.columns.difference(skip))\n",
    "    \n",
    "    # Set index and group by 'icustay_id' before resampling\n",
    "    X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n",
    "    \n",
    "    # Resample and aggregate features\n",
    "    if MAX_FEATURE_SET:\n",
    "        X_discrete = X[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "    X_numeric = X[numeric_feat].mean()\n",
    "    X_label = X['aki_stage'].max()\n",
    "\n",
    "    print(\"Merging sampled features\")\n",
    "    try:\n",
    "        X = pd.concat([X_numeric, X_discrete, X_label], axis=1).reset_index()\n",
    "    except:\n",
    "        X = pd.concat([X_numeric, X_label], axis=1).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# Forward fill again after resampling\n",
    "X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].ffill(limit=RESAMPLE_LIMIT).fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# Ensure binary values (convert any positive number to 1)\n",
    "X['aki_stage'] = (X['aki_stage'] > 0).astype(int)\n",
    "\n",
    "# Shifting labels\n",
    "shift_steps = HOURS_AHEAD // int(SAMPLING_INTERVAL[:-1])\n",
    "X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].shift(-shift_steps)\n",
    "X = X.dropna(subset=['aki_stage'])\n",
    "\n",
    "# Merging not time-dependent data\n",
    "dataset_detail_merging = dataset_detail[dataset_detail['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "# theirs\n",
    "# dataset_detail = pd.get_dummies(dataset_detail, columns=['gender', 'ethnicity_grouped'])\n",
    "#ours\n",
    "dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender'])\n",
    "dataset_detail_merging.drop(['subject_id', 'hadm_id'], axis=1, inplace=True)\n",
    "X = X.merge(dataset_detail_merging, on='icustay_id')\n",
    "X = X.merge(dataset_heightweight, on='icustay_id')\n",
    "X = X.merge(dataset_inr_max, on='icustay_id')\n",
    "\n",
    "# If no imputation method selected or only impute each id, for the remaining nan impute direclty with FILL_VALUE\n",
    "X = X.fillna(FILL_VALUE) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save preprocessed data\n",
    "X.to_csv('data/preprocessed/preprocessed_data_original.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
