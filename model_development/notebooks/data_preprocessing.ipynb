{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Uni\\KP_MedInf\\continuous-aki-predict\\model_development\n",
      "d:\\Uni\\KP_MedInf\\continuous-aki-predict\n"
     ]
    }
   ],
   "source": [
    "# only for taurus\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"model_development\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths and separator\n",
    "DATA_PATH_stages = \"data/extracted/kdigo_stages_measured.csv\"\n",
    "DATA_PATH_labs = \"data/extracted/labs_original.csv\"\n",
    "DATA_PATH_labs_extended = \"data/extracted/labs_extended.csv\"\n",
    "DATA_PATH_labs_new = \"data/extracted/labs_new.csv\"\n",
    "# DATA_PATH_vitals = \"../data/extracted/vitals-kdigo_stages_measured.csv\"\n",
    "DATA_PATH_vitals = \"data/extracted/vitals.csv\"\n",
    "# DATA_PATH_vents = \"../data/extracted/vents-vasopressor-sedatives-kdigo_stages_measured.csv\"\n",
    "DATA_PATH_vents = \"data/extracted/vents_vasopressor_sedatives.csv\"\n",
    "# DATA_PATH_detail = \"../data/extracted/icustay_detail-kdigo_stages_measured.csv\"\n",
    "DATA_PATH_detail = \"data/extracted/icustay_detail.csv\"\n",
    "DATA_PATH_heightweight = \"data/extracted/heightweight.csv\"\n",
    "DATA_PATH_calcium = \"data/extracted/calcium.csv\"\n",
    "DATA_PATH_inr_max = \"data/extracted/inr_max.csv\"\n",
    "SEPARATOR = \";\"\n",
    "\n",
    "# Constants\n",
    "IMPUTE_EACH_ID = False\n",
    "IMPUTE_COLUMN = False\n",
    "TESTING = False\n",
    "TEST_SIZE = 0.05\n",
    "SPLIT_SIZE = 0.2\n",
    "MAX_DAYS = 35\n",
    "CLASS1 = True\n",
    "ALL_STAGES = False\n",
    "MAX_FEATURE_SET = True\n",
    "FIRST_TURN_POS = True\n",
    "TIME_SAMPLING = True\n",
    "SAMPLING_INTERVAL = '6H'\n",
    "RESAMPLE_LIMIT = 16\n",
    "MOST_COMMON = False\n",
    "IMPUTE_METHOD = 'most_frequent'\n",
    "FILL_VALUE = 0\n",
    "ADULTS_MIN_AGE = 18\n",
    "ADULTS_MAX_AGE = 120\n",
    "NORMALIZATION = 'min-max'\n",
    "HOURS_AHEAD = 48\n",
    "NORM_TYPE = 'min_max'\n",
    "RANDOM = 42\n",
    "\n",
    "def filter_by_length_of_stay(X):\n",
    "    drop_list = []\n",
    "    long_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) > MAX_DAYS)\n",
    "\n",
    "    for icustay_id, is_long in long_stays.items():\n",
    "        if is_long:\n",
    "            max_time = X[X['icustay_id'] == icustay_id]['charttime'].max() - pd.to_timedelta(MAX_DAYS, unit='D')\n",
    "            X = X[~((X['icustay_id'] == icustay_id) & (X['charttime'] < max_time))]\n",
    "\n",
    "    short_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) < (HOURS_AHEAD/24))\n",
    "    drop_list = short_stays[short_stays].index.tolist()\n",
    "\n",
    "    X = X[~X.icustay_id.isin(drop_list)]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "3737147\n",
      "aki_stage\n",
      "0    3000795\n",
      "2     353266\n",
      "3     207759\n",
      "1     175327\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 102\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Merge datasets\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m MAX_FEATURE_SET:\n\u001b[0;32m     98\u001b[0m     \n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# Perform merge operations and then drop the 'subject_id' column\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     X_extended \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mmerge(dataset_labs_extended, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharttime\u001b[39m\u001b[38;5;124m\"\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m    101\u001b[0m                    \u001b[38;5;241m.\u001b[39mmerge(dataset_vitals, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharttime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhadm_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m--> 102\u001b[0m                    \u001b[38;5;241m.\u001b[39mmerge(dataset_vents, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharttime\u001b[39m\u001b[38;5;124m\"\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m    103\u001b[0m                    \u001b[38;5;241m.\u001b[39mmerge(dataset_calcium, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharttime\u001b[39m\u001b[38;5;124m\"\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m     X_extended\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    106\u001b[0m     X_original \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mmerge(dataset_labs, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharttime\u001b[39m\u001b[38;5;124m\"\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m    107\u001b[0m                   \u001b[38;5;241m.\u001b[39mmerge(dataset_vitals, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharttime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhadm_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m    108\u001b[0m                   \u001b[38;5;241m.\u001b[39mmerge(dataset_vents, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharttime\u001b[39m\u001b[38;5;124m\"\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m    109\u001b[0m                   \u001b[38;5;241m.\u001b[39mmerge(dataset_calcium, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharttime\u001b[39m\u001b[38;5;124m\"\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[0;32m  10833\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10834\u001b[0m         right,\n\u001b[0;32m  10835\u001b[0m         how\u001b[38;5;241m=\u001b[39mhow,\n\u001b[0;32m  10836\u001b[0m         on\u001b[38;5;241m=\u001b[39mon,\n\u001b[0;32m  10837\u001b[0m         left_on\u001b[38;5;241m=\u001b[39mleft_on,\n\u001b[0;32m  10838\u001b[0m         right_on\u001b[38;5;241m=\u001b[39mright_on,\n\u001b[0;32m  10839\u001b[0m         left_index\u001b[38;5;241m=\u001b[39mleft_index,\n\u001b[0;32m  10840\u001b[0m         right_index\u001b[38;5;241m=\u001b[39mright_index,\n\u001b[0;32m  10841\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m  10842\u001b[0m         suffixes\u001b[38;5;241m=\u001b[39msuffixes,\n\u001b[0;32m  10843\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m  10844\u001b[0m         indicator\u001b[38;5;241m=\u001b[39mindicator,\n\u001b[0;32m  10845\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m  10846\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m         left_df,\n\u001b[0;32m    172\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m    886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[1;32m--> 888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_and_concat(\n\u001b[0;32m    889\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    890\u001b[0m )\n\u001b[0;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:879\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    877\u001b[0m left\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m llabels\n\u001b[0;32m    878\u001b[0m right\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m rlabels\n\u001b[1;32m--> 879\u001b[0m result \u001b[38;5;241m=\u001b[39m concat([left, right], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[0;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m    686\u001b[0m )\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concat_axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m     mgrs \u001b[38;5;241m=\u001b[39m _maybe_reindex_columns_na_proxy(axes, mgrs_indexers, needs_copy)\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconcat_horizontal(mgrs, axes)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnblocks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[0m, in \u001b[0;36m_maybe_reindex_columns_na_proxy\u001b[1;34m(axes, mgrs_indexers, needs_copy)\u001b[0m\n\u001b[0;32m    220\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    221\u001b[0m             axes[i],\n\u001b[0;32m    222\u001b[0m             indexers[i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m             use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[1;32m--> 230\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    232\u001b[0m     new_mgrs\u001b[38;5;241m.\u001b[39mappend(mgr)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:604\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    601\u001b[0m         res\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 604\u001b[0m     res\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1788\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m _consolidate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks)\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2267\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2269\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m _merge_blocks(\n\u001b[0;32m   2270\u001b[0m         \u001b[38;5;28mlist\u001b[39m(group_blocks), dtype\u001b[38;5;241m=\u001b[39mdtype, can_consolidate\u001b[38;5;241m=\u001b[39m_can_consolidate\n\u001b[0;32m   2271\u001b[0m     )\n\u001b[0;32m   2272\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2294\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2287\u001b[0m new_values: ArrayLike\n\u001b[0;32m   2289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m   2290\u001b[0m     \u001b[38;5;66;03m# error: List comprehension has incompatible type List[Union[ndarray,\u001b[39;00m\n\u001b[0;32m   2291\u001b[0m     \u001b[38;5;66;03m# ExtensionArray]]; expected List[Union[complex, generic,\u001b[39;00m\n\u001b[0;32m   2292\u001b[0m     \u001b[38;5;66;03m# Sequence[Union[int, float, complex, str, bytes, generic]],\u001b[39;00m\n\u001b[0;32m   2293\u001b[0m     \u001b[38;5;66;03m# Sequence[Sequence[Any]], SupportsArray]]\u001b[39;00m\n\u001b[1;32m-> 2294\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([b\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m blocks])  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   2295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2296\u001b[0m     bvals \u001b[38;5;241m=\u001b[39m [blk\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m blocks]\n",
      "File \u001b[1;32mc:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\numpy\\core\\shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    288\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "X = pd.read_csv(DATA_PATH_stages, sep=SEPARATOR)\n",
    "X.drop([\"aki_stage_creat\", \"aki_stage_uo\"], axis=1, inplace=True)\n",
    "X = X.dropna(how='all', subset=['creat', 'uo_rt_6hr', 'uo_rt_12hr', 'uo_rt_24hr', 'aki_stage'])\n",
    "X['charttime'] = pd.to_datetime(X['charttime'])\n",
    "\n",
    "print(len(X))\n",
    "print(X['aki_stage'].value_counts())\n",
    "\n",
    "dataset_detail = pd.read_csv(DATA_PATH_detail, sep=SEPARATOR)\n",
    "# original data\n",
    "# out data\n",
    "dataset_detail.drop(['dod', 'admittime', 'dischtime', 'los_hospital', 'ethnicity', \n",
    "                     'hospital_expire_flag', 'hospstay_seq', 'first_hosp_stay', 'intime', \n",
    "                     'outtime', 'los_icu', 'icustay_seq', 'first_icu_stay', 'ethnicity_grouped'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "dataset_labs = pd.read_csv(DATA_PATH_labs, sep=SEPARATOR)\n",
    "dataset_labs = dataset_labs.dropna(subset=['charttime']).dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "dataset_labs['charttime'] = pd.to_datetime(dataset_labs['charttime'])\n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'])\n",
    "dataset_labs.drop(['albumin_min', 'albumin_max','bilirubin_min', 'bilirubin_max','bands_min', 'bands_max',\n",
    "                   'lactate_min', 'lactate_max','platelet_min', 'platelet_max','ptt_min', 'ptt_max', \n",
    "                   'inr_min', 'inr_max', 'pt_min', 'pt_max'], axis = 1, inplace = True)\n",
    "# Calculate mean for each pair and drop original columns\n",
    "column_pairs = [('aniongap_min', 'aniongap_max'), ('albumin_min', 'albumin_max'), \n",
    "                ('bands_min', 'bands_max'), ('bicarbonate_min', 'bicarbonate_max'), \n",
    "                ('bilirubin_min', 'bilirubin_max'), ('creatinine_min', 'creatinine_max'), \n",
    "                ('chloride_min', 'chloride_max'), ('glucose_min', 'glucose_max'), \n",
    "                ('hematocrit_min', 'hematocrit_max'), ('hemoglobin_min', 'hemoglobin_max'), \n",
    "                ('lactate_min', 'lactate_max'), ('platelet_min', 'platelet_max'), \n",
    "                ('potassium_min', 'potassium_max'), ('ptt_min', 'ptt_max'), \n",
    "                ('inr_min', 'inr_max'), ('pt_min', 'pt_max'), ('sodium_min', 'sodium_max'), \n",
    "                ('bun_min', 'bun_max'), ('wbc_min', 'wbc_max')]\n",
    "\n",
    "for min_col, max_col in column_pairs:\n",
    "    try:\n",
    "        mean_col = min_col.rsplit('_', 1)[0] + '_mean'\n",
    "        dataset_labs[mean_col] = dataset_labs[[min_col, max_col]].mean(axis=1)\n",
    "        dataset_labs.drop([min_col, max_col], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "dataset_labs_extended = pd.read_csv(DATA_PATH_labs_extended, sep=SEPARATOR)\n",
    "dataset_labs_extended = dataset_labs_extended.dropna(subset=['charttime']).dropna(subset=dataset_labs_extended.columns[4:], how='all')\n",
    "dataset_labs_extended['charttime'] = pd.to_datetime(dataset_labs_extended['charttime'])\n",
    "dataset_labs_extended = dataset_labs_extended.sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "column_pairs_extended = [('aniongap_min', 'aniongap_max'), ('albumin_min', 'albumin_max'), \n",
    "                ('bands_min', 'bands_max'), ('bicarbonate_min', 'bicarbonate_max'), \n",
    "                ('bilirubin_min', 'bilirubin_max'), ('creatinine_min', 'creatinine_max'), \n",
    "                ('chloride_min', 'chloride_max'), ('glucose_min', 'glucose_max'), \n",
    "                ('hematocrit_min', 'hematocrit_max'), ('hemoglobin_min', 'hemoglobin_max'), \n",
    "                ('lactate_min', 'lactate_max'), ('platelet_min', 'platelet_max'), \n",
    "                ('potassium_min', 'potassium_max'), ('ptt_min', 'ptt_max'), \n",
    "                ('inr_min', 'inr_max'), ('pt_min', 'pt_max'), ('sodium_min', 'sodium_max'), \n",
    "                ('bun_min', 'bun_max'), ('wbc_min', 'wbc_max'), \n",
    "                ('gfr_min', 'gfr_max'), ('phosphate_min', 'phosphate_max'),('uric_acid_min', 'uric_acid_max'), \n",
    "                ('calcium_min', 'calcium_max')]\n",
    "\n",
    "\n",
    "for min_col, max_col in column_pairs_extended:\n",
    "    mean_col = min_col.rsplit('_', 1)[0] + '_mean'\n",
    "    dataset_labs_extended[mean_col] = dataset_labs_extended[[min_col, max_col]].mean(axis=1)\n",
    "    dataset_labs_extended.drop([min_col, max_col], axis=1, inplace=True)\n",
    "dataset_labs_extended.drop(['gfr_mean'], axis=1, inplace=True)\n",
    "\n",
    "dataset_vitals = pd.read_csv(DATA_PATH_vitals, sep=SEPARATOR)\n",
    "dataset_vents = pd.read_csv(DATA_PATH_vents, sep=SEPARATOR)\n",
    "dataset_vitals.drop([\"heartrate_min\", \"heartrate_max\", \"sysbp_min\", \"sysbp_max\", \"diasbp_min\", \"diasbp_max\",\n",
    "                        'meanbp_min', 'meanbp_max', 'tempc_min', 'tempc_max', \"resprate_min\", \"resprate_max\", \n",
    "                        \"spo2_min\", \"spo2_max\", \"glucose_min\", \"glucose_max\"], axis=1, inplace=True)\n",
    "dataset_vitals['charttime'] = pd.to_datetime(dataset_vitals['charttime'])\n",
    "dataset_vents['charttime'] = pd.to_datetime(dataset_vents['charttime'])\n",
    "dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')\n",
    "dataset_vitals = dataset_vitals.sort_values(by=['icustay_id', 'charttime'])\n",
    "dataset_vents = dataset_vents.sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "dataset_heightweight = pd.read_csv(DATA_PATH_heightweight, sep=SEPARATOR)\n",
    "dataset_heightweight = dataset_heightweight.dropna(subset=['icustay_id', 'height_first', 'weight_first'], how='all')\n",
    "dataset_heightweight = dataset_heightweight.sort_values(by=['icustay_id'])\n",
    "\n",
    "dataset_calcium = pd.read_csv(DATA_PATH_calcium, sep=SEPARATOR)\n",
    "dataset_calcium.drop([\"hadm_id\"], axis=1, inplace=True)\n",
    "dataset_calcium['charttime'] = pd.to_datetime(dataset_calcium['charttime'])\n",
    "dataset_calcium = dataset_calcium.sort_values(by=['icustay_id', 'charttime'])\n",
    "dataset_calcium.drop([\"subject_id\"], axis=1, inplace=True)\n",
    "\n",
    "dataset_inr_max = pd.read_csv(DATA_PATH_inr_max, sep=SEPARATOR)\n",
    "dataset_inr_max.drop([\"hadm_id\", \"subject_id\"], axis=1, inplace=True)\n",
    "dataset_inr_max = dataset_inr_max.sort_values(by=['icustay_id'])\n",
    "\n",
    "\n",
    "\n",
    "# Merge datasets\n",
    "if MAX_FEATURE_SET:\n",
    "    \n",
    "    # Perform merge operations and then drop the 'subject_id' column\n",
    "    X_extended = X.merge(dataset_labs_extended, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "                   .merge(dataset_vitals, on=[\"icustay_id\", \"charttime\", \"subject_id\", \"hadm_id\"], how=\"outer\") \\\n",
    "                   .merge(dataset_vents, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "                   .merge(dataset_calcium, on=[\"icustay_id\", \"charttime\"], how=\"outer\")\n",
    "    X_extended.drop([\"subject_id\"], axis=1, inplace=True)\n",
    "    \n",
    "    X_original = X.merge(dataset_labs, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "                  .merge(dataset_vitals, on=[\"icustay_id\", \"charttime\", \"subject_id\", \"hadm_id\"], how=\"outer\") \\\n",
    "                  .merge(dataset_vents, on=[\"icustay_id\", \"charttime\"], how=\"outer\") \\\n",
    "                  .merge(dataset_calcium, on=[\"icustay_id\", \"charttime\"], how=\"outer\")\n",
    "    X_original.drop([\"subject_id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use subset of X for testing\n",
    "unique_ids_subset = X['icustay_id'].unique()[:100]  # Get the first 100 unique icustay_id values\n",
    "X = X[X['icustay_id'].isin(unique_ids_subset)]  # Filter rows in X\n",
    "X_extended = X_extended[X_extended['icustay_id'].isin(unique_ids_subset)]\n",
    "X_original = X_original[X_original['icustay_id'].isin(unique_ids_subset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering patients by age and length of stay...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1312113/1803750554.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  long_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) > MAX_DAYS)\n",
      "/tmp/ipykernel_1312113/1803750554.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  short_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) < (HOURS_AHEAD/24))\n",
      "/tmp/ipykernel_1312113/1803750554.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  long_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) > MAX_DAYS)\n",
      "/tmp/ipykernel_1312113/1803750554.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  short_stays = X.groupby(['icustay_id']).apply(lambda group: (group['charttime'].max() - group['charttime'].min()).total_seconds() / (24 * 60 * 60) < (HOURS_AHEAD/24))\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering patients by age and length of stay...\")\n",
    "# Filtering patients by age and length of stay\n",
    "dataset_detail = dataset_detail[dataset_detail['admission_age'] >= ADULTS_MIN_AGE]\n",
    "adults_icustay_id_list = dataset_detail['icustay_id'].unique()\n",
    "X = X[X.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id', 'charttime'])\n",
    "X_extended = X_extended[X_extended.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id', 'charttime'])\n",
    "X_original = X_original[X_original.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "# X = filter_by_length_of_stay(X)\n",
    "X_extended = filter_by_length_of_stay(X_extended)\n",
    "X_original = filter_by_length_of_stay(X_original)\n",
    "dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['aki_stage']\n",
    "skip = ['icustay_id', 'charttime', 'aki_stage']\n",
    "discrete_feat = ['sedative', 'vasopressor', 'vent', 'hadm_id']\n",
    "skip.extend(discrete_feat)    \n",
    "numeric_feat = list(X.columns.difference(skip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_original' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# save to csv\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_original\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/analysis/data_original_preprocessed_filtered_before_resampling.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_original' is not defined"
     ]
    }
   ],
   "source": [
    "# save to csv\n",
    "X_original.to_csv('data/analysis/data_original_preprocessed_filtered_before_resampling.csv', index=False)\n",
    "X_extended.to_csv('data/analysis/data_extended_preprocessed_filtered_before_resampling.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/analysis/data_original_preprocessed_filtered_before_resampling.csv')\n",
    "column_test = pd.read_csv('data\\preprocessed\\preprocessed_data_24H.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "print(len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['icustay_id', 'charttime', 'aniongap_mean', 'bicarbonate_mean',\n",
       "       'bun_mean', 'calcium', 'chloride_mean', 'creat', 'creatinine_mean',\n",
       "       'diasbp_mean', 'glucose_mean_x', 'glucose_mean_y', 'heartrate_mean',\n",
       "       'hematocrit_mean', 'hemoglobin_mean', 'meanbp_mean', 'potassium_mean',\n",
       "       'resprate_mean', 'sodium_mean', 'spo2_mean', 'sysbp_mean', 'tempc_mean',\n",
       "       'uo_rt_12hr', 'uo_rt_24hr', 'uo_rt_6hr', 'wbc_mean', 'sedative',\n",
       "       'vasopressor', 'vent', 'hadm_id', 'aki_stage', 'admission_age',\n",
       "       'gender_F', 'gender_M', 'weight_first', 'height_first', 'inr_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_test.drop(['icustay_id', 'charttime', 'aki_stage', 'hadm_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(column_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical features\n",
    "dataset_detail = pd.read_csv(DATA_PATH_detail, sep=SEPARATOR)\n",
    "# original data\n",
    "# out data\n",
    "dataset_detail.drop(['dod', 'admittime', 'dischtime', 'los_hospital', 'ethnicity', \n",
    "                     'hospital_expire_flag', 'hospstay_seq', 'first_hosp_stay', 'intime', \n",
    "                     'outtime', 'los_icu', 'icustay_seq', 'first_icu_stay', 'ethnicity_grouped'], axis=1, inplace=True)\n",
    "dataset_heightweight = pd.read_csv(DATA_PATH_heightweight, sep=SEPARATOR)\n",
    "dataset_heightweight = dataset_heightweight.dropna(subset=['icustay_id', 'height_first', 'weight_first'], how='all')\n",
    "dataset_heightweight = dataset_heightweight.sort_values(by=['icustay_id'])\n",
    "dataset_inr_max = pd.read_csv(DATA_PATH_inr_max, sep=SEPARATOR)\n",
    "dataset_inr_max.drop([\"hadm_id\", \"subject_id\"], axis=1, inplace=True)\n",
    "dataset_inr_max = dataset_inr_max.sort_values(by=['icustay_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging not time-dependent data\n",
    "dataset_detail_merging = dataset_detail[dataset_detail['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "# theirs\n",
    "# dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender', 'ethnicity_grouped'])\n",
    "#ours\n",
    "dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender'])\n",
    "# dataset_detail_merging.drop(['subject_id', 'hadm_id'], axis=1, inplace=True)\n",
    "\n",
    "X = X.merge(dataset_detail_merging, on='icustay_id')\n",
    "# X = X.merge(dataset_heightweight, on='icustay_id')\n",
    "# X = X.merge(dataset_inr_max, on='icustay_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(['subject_id', 'hadm_id_x', 'hadm_id_y', 'aki_stage'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['icustay_id', 'charttime', 'aniongap_mean', 'bicarbonate_mean',\n",
      "       'bun_mean', 'calcium', 'chloride_mean', 'creat', 'creatinine_mean',\n",
      "       'diasbp_mean', 'glucose_mean_x', 'glucose_mean_y', 'heartrate_mean',\n",
      "       'hematocrit_mean', 'hemoglobin_mean', 'meanbp_mean', 'potassium_mean',\n",
      "       'resprate_mean', 'sodium_mean', 'spo2_mean', 'sysbp_mean', 'tempc_mean',\n",
      "       'uo_rt_12hr', 'uo_rt_24hr', 'uo_rt_6hr', 'wbc_mean', 'sedative',\n",
      "       'vasopressor', 'vent', 'aki_stage', 'admission_age', 'gender_F',\n",
      "       'gender_M'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(column_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['icustay_id', 'charttime', 'creat', 'uo_rt_6hr', 'uo_rt_12hr',\n",
      "       'uo_rt_24hr', 'aniongap_mean', 'bicarbonate_mean', 'creatinine_mean',\n",
      "       'chloride_mean', 'glucose_mean_x', 'hematocrit_mean', 'hemoglobin_mean',\n",
      "       'potassium_mean', 'sodium_mean', 'bun_mean', 'wbc_mean',\n",
      "       'heartrate_mean', 'sysbp_mean', 'diasbp_mean', 'meanbp_mean',\n",
      "       'resprate_mean', 'tempc_mean', 'spo2_mean', 'glucose_mean_y', 'vent',\n",
      "       'vasopressor', 'sedative', 'calcium', 'admission_age', 'gender_F',\n",
      "       'gender_M'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# print difference between X.columns and column_test.columns\n",
    "print(set(X.columns) - set(column_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by icustay_id and save all rows of 10 random ids ommiting the icustay_id\n",
    "X_grouped = X.groupby('icustay_id')\n",
    "# pick 10 random ids\n",
    "ids = np.random.choice(X['icustay_id'].unique(), 10)\n",
    "os.makedirs('data/dummy_data_lab_values', exist_ok=True)\n",
    "for id in ids:\n",
    "    # remove icustay_id from the group\n",
    "    group = X_grouped.get_group(id).drop('icustay_id', axis=1)\n",
    "    # save to csv\n",
    "    group.to_csv(f'data/dummy_data_lab_values/{id}_original_preprocessed_filtered_before_resampling.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create unresampled shifted dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_extended.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11478\n"
     ]
    }
   ],
   "source": [
    "print(X['aki_stage'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option1: fill all labels since past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1312113/378773068.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  X_shifted = grouped.apply(shift_aki_stage)\n"
     ]
    }
   ],
   "source": [
    "# binarize aki_stage labels\n",
    "X['aki_stage'] = (X['aki_stage'] > 0).astype(int)\n",
    "# create X_shifted where, for each icuststay_id, an aki_stage > 0 is also applied to all timestamps in the 48 hours before\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X is your original dataframe\n",
    "# First, make sure X is sorted by icustay_id and charttime\n",
    "X = X.sort_values(['icustay_id', 'charttime'])\n",
    "\n",
    "# Create a copy of X to modify\n",
    "X_shifted = X.copy()\n",
    "\n",
    "# Convert charttime to datetime if it's not already\n",
    "X_shifted['charttime'] = pd.to_datetime(X_shifted['charttime'])\n",
    "\n",
    "# Group by icustay_id\n",
    "grouped = X_shifted.groupby('icustay_id')\n",
    "\n",
    "# Function to shift aki_stage\n",
    "def shift_aki_stage(group):\n",
    "    # Find where aki_stage > 0\n",
    "    aki_events = group[group['aki_stage'] > 0]\n",
    "    \n",
    "    for _, event in aki_events.iterrows():\n",
    "        # Calculate the time 48 hours before the event\n",
    "        time_before = event['charttime'] - pd.Timedelta(hours=48)\n",
    "        \n",
    "        # Set aki_stage to 1 for all rows in the 48-hour window before the event\n",
    "        mask = (group['charttime'] >= time_before) & (group['charttime'] <= event['charttime'])\n",
    "        group.loc[mask, 'aki_stage'] = 1\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Apply the function to each group\n",
    "X_shifted = grouped.apply(shift_aki_stage)\n",
    "\n",
    "# Reset index if needed\n",
    "X_shifted = X_shifted.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "7536107\n"
     ]
    }
   ],
   "source": [
    "# print number of rows in X where aki_stage is nan\n",
    "print(X_shifted['aki_stage'].isna().sum())\n",
    "print(X['aki_stage'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aki_stage\n",
      "0.0    79.691991\n",
      "2.0     9.808070\n",
      "3.0     5.596306\n",
      "1.0     4.903633\n",
      "Name: proportion, dtype: float64\n",
      "aki_stage\n",
      "1    51.789364\n",
      "0    48.210636\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X['aki_stage'].value_counts(normalize=True) * 100)\n",
    "print(X_shifted['aki_stage'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "X_shifted.to_csv('data/analysis/data_shifted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option2: fill only window in the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1312113/181169435.py:41: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  X_shifted_window = grouped.apply(shift_aki_stage)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X is your original dataframe\n",
    "# First, make sure X is sorted by icustay_id and charttime\n",
    "X = X.sort_values(['icustay_id', 'charttime'])\n",
    "\n",
    "# Create a copy of X to modify\n",
    "X_shifted_window = X.copy()\n",
    "\n",
    "# Convert charttime to datetime if it's not already\n",
    "X_shifted_window['charttime'] = pd.to_datetime(X_shifted_window['charttime'])\n",
    "\n",
    "# Group by icustay_id\n",
    "grouped = X_shifted_window.groupby('icustay_id')\n",
    "\n",
    "# Function to shift aki_stage\n",
    "def shift_aki_stage(group):\n",
    "    # Find where aki_stage > 0\n",
    "    aki_events = group[group['aki_stage'] > 0].copy()\n",
    "    \n",
    "    for idx, event in aki_events.iterrows():\n",
    "        # Calculate the time 48 and 40 hours before the event\n",
    "        time_before_48 = event['charttime'] - pd.Timedelta(hours=48)\n",
    "        time_before_40 = event['charttime'] - pd.Timedelta(hours=40)\n",
    "        \n",
    "        # Find rows in the 40-48 hour window before the event\n",
    "        mask = (group['charttime'] >= time_before_48) & (group['charttime'] <= time_before_40)\n",
    "        rows_to_update = group[mask]\n",
    "        \n",
    "        if not rows_to_update.empty:\n",
    "            # Update the aki_stage for these rows\n",
    "            group.loc[rows_to_update.index, 'aki_stage'] = 1\n",
    "            \n",
    "            # Remove the original event\n",
    "            group.loc[idx, 'aki_stage'] = 0\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Apply the function to each group\n",
    "X_shifted_window = grouped.apply(shift_aki_stage)\n",
    "\n",
    "# Reset index if needed\n",
    "X_shifted_window = X_shifted_window.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5970197\n",
      "7536107\n"
     ]
    }
   ],
   "source": [
    "# print number of rows in X where aki_stage is nan\n",
    "print(X_shifted_window['aki_stage'].isna().sum())\n",
    "print(X['aki_stage'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aki_stage\n",
      "0.0    79.691991\n",
      "2.0     9.808070\n",
      "3.0     5.596306\n",
      "1.0     4.903633\n",
      "Name: proportion, dtype: float64\n",
      "aki_stage\n",
      "0.0    51.128208\n",
      "1.0    47.391171\n",
      "2.0     1.198435\n",
      "3.0     0.282187\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X['aki_stage'].value_counts(normalize=True) * 100)\n",
    "print(X_shifted_window['aki_stage'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shifted_window.to_csv('data/analysis/data_shifted_window.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## looped dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_extended.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_INTERVALS = ['1H', '2H', '4H', '6H', '8H', '12H', '24H']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_detail = dataset_detail[dataset_detail['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "try:\n",
    "    # theirs\n",
    "    dataset_detail = pd.get_dummies(dataset_detail, columns=['gender', 'ethnicity_grouped'])\n",
    "except:\n",
    "    #ours\n",
    "    dataset_detail = pd.get_dummies(dataset_detail, columns=['gender'])\n",
    "dataset_detail.drop(['subject_id', 'hadm_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['icustay_id', 'admission_age', 'gender_F', 'gender_M'], dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_detail.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1312113/3901902011.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  X_resampled = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging sampled features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1312113/3901902011.py:7: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  X_resampled = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging sampled features\n"
     ]
    }
   ],
   "source": [
    "# saving every sampling interval\n",
    "original = True\n",
    "# Resampling\n",
    "for SAMPLING_INTERVAL in SAMPLING_INTERVALS:\n",
    "    if TIME_SAMPLING:\n",
    "        \n",
    "        # Set index and group by 'icustay_id' before resampling\n",
    "        X_resampled = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n",
    "        \n",
    "        # Resample and aggregate features\n",
    "        if MAX_FEATURE_SET:\n",
    "            X_discrete = X_resampled[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "        X_numeric = X_resampled[numeric_feat].mean()\n",
    "        X_label = X_resampled['aki_stage'].max()\n",
    "\n",
    "        print(\"Merging sampled features\")\n",
    "        try:\n",
    "            X_resampled = pd.concat([X_numeric, X_discrete, X_label], axis=1).reset_index()\n",
    "        except:\n",
    "            X_resampled = pd.concat([X_numeric, X_label], axis=1).reset_index()\n",
    "\n",
    "    # Forward fill again after resampling\n",
    "    X_resampled['aki_stage'] = X_resampled.groupby('icustay_id')['aki_stage'].ffill(limit=RESAMPLE_LIMIT).fillna(0)\n",
    "\n",
    "    # Ensure binary values (convert any positive number to 1)\n",
    "    X_resampled['aki_stage'] = (X_resampled['aki_stage'] > 0).astype(int)\n",
    "\n",
    "    # Shifting labels\n",
    "    shift_steps = HOURS_AHEAD // int(SAMPLING_INTERVAL[:-1])\n",
    "    X_resampled['aki_stage'] = X_resampled.groupby('icustay_id')['aki_stage'].shift(-shift_steps)\n",
    "    X_resampled = X_resampled.dropna(subset=['aki_stage'])\n",
    "    \n",
    "    # Merging not time-dependent data\n",
    "    dataset_detail_merging = dataset_detail[dataset_detail['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "    if original:\n",
    "        # theirs\n",
    "        dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender', 'ethnicity_grouped'])\n",
    "    else:\n",
    "        dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender'])\n",
    "        X_resampled = X_resampled.merge(dataset_heightweight, on='icustay_id')\n",
    "        X_resampled = X_resampled.merge(dataset_inr_max, on='icustay_id')\n",
    "\n",
    "    X_resampled = X_resampled.merge(dataset_detail_merging, on='icustay_id')\n",
    "\n",
    "\n",
    "    # If no imputation method selected or only impute each id, for the remaining nan impute direclty with FILL_VALUE\n",
    "    X_resampled = X_resampled.fillna(FILL_VALUE) \n",
    "\n",
    "\n",
    "    # Save preprocessed data\n",
    "    X_resampled.to_csv(f'data/preprocessed/preprocessed_data_extended_{SAMPLING_INTERVAL}.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normal ds creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_924951/2522398281.py:11: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging sampled features\n"
     ]
    }
   ],
   "source": [
    "# Resampling\n",
    "if TIME_SAMPLING:\n",
    "    \n",
    "    label = ['aki_stage']\n",
    "    skip = ['icustay_id', 'charttime', 'aki_stage']\n",
    "    discrete_feat = ['sedative', 'vasopressor', 'vent', 'hadm_id']\n",
    "    skip.extend(discrete_feat)    \n",
    "    numeric_feat = list(X.columns.difference(skip))\n",
    "    \n",
    "    # Set index and group by 'icustay_id' before resampling\n",
    "    X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n",
    "    \n",
    "    # Resample and aggregate features\n",
    "    if MAX_FEATURE_SET:\n",
    "        X_discrete = X[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "    X_numeric = X[numeric_feat].mean()\n",
    "    X_label = X['aki_stage'].max()\n",
    "\n",
    "    print(\"Merging sampled features\")\n",
    "    try:\n",
    "        X = pd.concat([X_numeric, X_discrete, X_label], axis=1).reset_index()\n",
    "    except:\n",
    "        X = pd.concat([X_numeric, X_label], axis=1).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# Forward fill again after resampling\n",
    "X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].ffill(limit=RESAMPLE_LIMIT).fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# Ensure binary values (convert any positive number to 1)\n",
    "X['aki_stage'] = (X['aki_stage'] > 0).astype(int)\n",
    "\n",
    "# Shifting labels\n",
    "shift_steps = HOURS_AHEAD // int(SAMPLING_INTERVAL[:-1])\n",
    "X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].shift(-shift_steps)\n",
    "X = X.dropna(subset=['aki_stage'])\n",
    "\n",
    "# Merging not time-dependent data\n",
    "dataset_detail_merging = dataset_detail[dataset_detail['icustay_id'].isin(X['icustay_id'].unique())].sort_values(by=['icustay_id'])\n",
    "# theirs\n",
    "# dataset_detail = pd.get_dummies(dataset_detail, columns=['gender', 'ethnicity_grouped'])\n",
    "#ours\n",
    "dataset_detail_merging = pd.get_dummies(dataset_detail_merging, columns=['gender'])\n",
    "dataset_detail_merging.drop(['subject_id', 'hadm_id'], axis=1, inplace=True)\n",
    "X = X.merge(dataset_detail_merging, on='icustay_id')\n",
    "X = X.merge(dataset_heightweight, on='icustay_id')\n",
    "X = X.merge(dataset_inr_max, on='icustay_id')\n",
    "\n",
    "# If no imputation method selected or only impute each id, for the remaining nan impute direclty with FILL_VALUE\n",
    "X = X.fillna(FILL_VALUE) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save preprocessed data\n",
    "X.to_csv('data/preprocessed/preprocessed_data_original.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
