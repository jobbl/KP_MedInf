{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Uni\\KP_MedInf\\continuous-aki-predict\\model_development\\notebooks\n",
      "d:\\Uni\\KP_MedInf\\continuous-aki-predict\\model_development\n"
     ]
    }
   ],
   "source": [
    "# only for taurus\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# os.chdir(\"/home/jori152b/DIR/horse/jori152b-medinf/KP_MedInf/model_development\")\n",
    "os.chdir(\"../\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-10 15:55:55.863873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "#data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "# NN\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score, roc_auc_score, auc, accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from captum.attr import IntegratedGradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Training on CPU') # On mac book GPU is not possible =() \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter as constant \n",
    "\n",
    "TESTING = False \n",
    "TEST_SIZE = 0.05\n",
    "\n",
    "SPLIT_SIZE = 0.2 \n",
    "\n",
    "NORMALIZATION = 'min-max' \n",
    "\n",
    "CAPPING_THRESHOLD_UPPER = 0.99\n",
    "CAPPING_THRESHOLD_LOWER = 0.01\n",
    "\n",
    "# How much time the prediction should occur (hours)\n",
    "HOURS_AHEAD = 48\n",
    "\n",
    "NORM_TYPE = 'min_max'\n",
    "\n",
    "RANDOM = 42\n",
    "\n",
    "# LSTM\n",
    "batch_size = 5\n",
    "\n",
    "# naming model and plot\n",
    "classifier_name = \"None vs. Any AKI\"    ###change every time #Moderate vs. Severe #None vs. Any #Others vs. Severe\n",
    "plot_name = \"adult_AnyAKI_LR\"    ###change every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions used later\n",
    "\n",
    "def cap_data(df):\n",
    "    print(\"Capping between the {} and {} quantile\".format(CAPPING_THRESHOLD_LOWER, CAPPING_THRESHOLD_UPPER))\n",
    "    cap_mask = df.columns.difference(['icustay_id', 'charttime', 'aki_stage'])\n",
    "    df[cap_mask] = df[cap_mask].clip(df[cap_mask].quantile(CAPPING_THRESHOLD_LOWER),\n",
    "                                     df[cap_mask].quantile(CAPPING_THRESHOLD_UPPER),\n",
    "                                     axis=1)\n",
    "\n",
    "    return df\n",
    " \n",
    "    \n",
    "def normalise_data(df, norm_mask):\n",
    "    print(\"Normalizing in [0,1] with {} normalization\".format(NORMALIZATION))\n",
    "    \n",
    "    min_values = df[norm_mask].min()\n",
    "    max_values = df[norm_mask].max()\n",
    "    \n",
    "    # Skip normalization for constant columns\n",
    "    for column in norm_mask:\n",
    "        if min_values[column] != max_values[column]:\n",
    "            df[column] = (df[column] - min_values[column]) / (max_values[column] - min_values[column])\n",
    "    \n",
    "    normalization_parameters = {column: {'min': min_values[column], 'max': max_values[column]} for column in norm_mask}\n",
    "    \n",
    "    return df, normalization_parameters\n",
    "\n",
    "\n",
    "# impute missing value in resampleing data with most common based on each id\n",
    "def fast_mode(df, key_cols, value_col):\n",
    "    \"\"\" Calculate a column mode, by group, ignoring null values. \n",
    "    \n",
    "    key_cols : list of str - Columns to groupby for calculation of mode.\n",
    "    value_col : str - Column for which to calculate the mode. \n",
    "\n",
    "    Return\n",
    "    pandas.DataFrame\n",
    "        One row for the mode of value_col per key_cols group. If ties, returns the one which is sorted first. \"\"\"\n",
    "    return (df.groupby(key_cols + [value_col]).size() \n",
    "              .to_frame('counts').reset_index() \n",
    "              .sort_values('counts', ascending=False) \n",
    "              .drop_duplicates(subset=key_cols)).drop('counts',axis=1)\n",
    "\n",
    "\n",
    "#get max shape of 3d array\n",
    "def get_dimensions(array, level=0):   \n",
    "    yield level, len(array)\n",
    "    try:\n",
    "        for row in array:\n",
    "            yield from get_dimensions(row, level + 1)\n",
    "    except TypeError: #not an iterable\n",
    "        pass\n",
    "\n",
    "def get_max_shape(array):\n",
    "    dimensions = defaultdict(int)\n",
    "    for level, length in get_dimensions(array):\n",
    "        dimensions[level] = max(dimensions[level], length)\n",
    "    return [value for _, value in sorted(dimensions.items())]\n",
    "\n",
    "#pad the ragged 3d array to rectangular shape based on max size\n",
    "def iterate_nested_array(array, index=()):\n",
    "    try:\n",
    "        for idx, row in enumerate(array):\n",
    "            yield from iterate_nested_array(row, (*index, idx)) \n",
    "    except TypeError: # final level            \n",
    "        yield (*index, slice(len(array))), array # think of the types\n",
    "\n",
    "def pad(array, fill_value):\n",
    "    dimensions = get_max_shape(array)\n",
    "    result = np.full(dimensions, fill_value, dtype = np.float64)  \n",
    "    for index, value in iterate_nested_array(array):\n",
    "        result[index] = value \n",
    "    return result\n",
    "\n",
    "def bin_total(y_true, y_prob, n_bins):\n",
    "    bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "    # In sklearn.calibration.calibration_curve,\n",
    "    # the last value in the array is always 0.\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "def missing_bin(bin_array):\n",
    "    midpoint = \" \"    \n",
    "    if bin_array[0]==0:\n",
    "        midpoint = \"5%, \"\n",
    "    if bin_array[1]==0:\n",
    "        midpoint = midpoint + \"15%, \"\n",
    "    if bin_array[2]==0:\n",
    "        midpoint = midpoint + \"25%, \"\n",
    "    if bin_array[3]==0:\n",
    "        midpoint = midpoint + \"35%, \" \n",
    "    if bin_array[4]==0:\n",
    "        midpoint = midpoint + \"45%, \"\n",
    "    if bin_array[5]==0:\n",
    "        midpoint = midpoint + \"55%, \"\n",
    "    if bin_array[6]==0:\n",
    "        midpoint = midpoint + \"65%, \"\n",
    "    if bin_array[7]==0:\n",
    "        midpoint = midpoint + \"75%, \"\n",
    "    if bin_array[8]==0:\n",
    "        midpoint = midpoint + \"85%, \"\n",
    "    if bin_array[9]==0:\n",
    "        midpoint = midpoint + \"95%, \"\n",
    "    return \"The missing bins have midpoint values of \"+ str(midpoint)\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    times = math.floor(data.shape[0]/batch_size)\n",
    "    remainder = data.shape[0]%times\n",
    "    a = 0\n",
    "    start = 0\n",
    "    end = start+batch_size\n",
    "    if remainder ==0:\n",
    "        a +=1\n",
    "    while a<times:\n",
    "        temp = pad(data[start:end,],0)\n",
    "        x = torch.from_numpy(temp[:,:,1:-1]).float() # without icustay_id and without aki_stage columns\n",
    "        y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "        X_batches.append(x)\n",
    "        y_batches.append(y)\n",
    "        start = end\n",
    "        end = start+batch_size\n",
    "        a +=1\n",
    "    temp = pad(data[start:data.shape[0]],0)\n",
    "    x = torch.from_numpy(temp[:,:,1:-1]).float()\n",
    "    y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "    X_batches.append(x)\n",
    "    y_batches.append(y)\n",
    "    if len(X_batches) != len(y_batches):\n",
    "        print(\"length error\")\n",
    "    return X_batches, y_batches # arrays\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size \n",
    "        self.output_size = output_size\n",
    "        self.number_layers = number_layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "        # in bidirectional encoder we have  forward and backward hidden states\n",
    "        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "        # Create affine layer to project to the classes \n",
    "        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "        #dropout layer for regularizetion of a sequence\n",
    "        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "        h = self.relu(self.combination_layer(h))\n",
    "        h = self.dropout_layer(h)\n",
    "        h = self.projection(h) \n",
    "        return h\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/preprocessed/preprocessed_data_extended_6H.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m X_original \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/preprocessed/preprocessed_data_original.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m X_extended \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/preprocessed/preprocessed_data_extended_6H.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# AND the unique ids so there are the same unique patients in both dataframes\u001b[39;00m\n\u001b[1;32m      5\u001b[0m common_id_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(X_original[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mintersection(X_extended[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/preprocessed/preprocessed_data_extended_6H.csv'"
     ]
    }
   ],
   "source": [
    "X_original = pd.read_csv(\"data/preprocessed/preprocessed_data_original.csv\")\n",
    "X_extended = pd.read_csv(\"data/preprocessed/preprocessed_data_extended_6H.csv\")  \n",
    "# AND the unique ids so there are the same unique patients in both dataframes\n",
    "\n",
    "common_id_list = list(set(X_original['icustay_id']).intersection(X_extended['icustay_id']))\n",
    "print(\"Number of common patients in both dataframes: \", len(common_id_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_extended = pd.read_csv(\"data/preprocessed/preprocessed_data_extended.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original = pd.read_csv(\"data/preprocessed/preprocessed_data_original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.str_('uo_rt_6hr'), np.str_('heartrate_mean'), np.str_('uo_rt_24hr'), np.str_('creat'), np.str_('resprate_mean'), np.str_('calcium'), np.str_('vasopressor'), np.str_('creatinine_mean'), np.str_('uo_rt_12hr'), np.str_('vent'), np.str_('weight_first'), np.str_('gender_F'), np.str_('height_first'), np.str_('admission_age'), np.str_('lactate_mean'), np.str_('inr_max'), np.str_('phosphate_mean'), np.str_('aniongap_mean'), np.str_('sedative'), np.str_('ptt_mean'), 'aki_stage', 'icustay_id', 'charttime']\n"
     ]
    }
   ],
   "source": [
    "# optional: load best features\n",
    "optimal_features = np.load(\"data/optimal_features.npy\", allow_pickle=True)\n",
    "# Extracting feature names (keys) from optimal_features\n",
    "optimal_feature_names = [feature[0] for feature in optimal_features]\n",
    "# include also aki_stage and icustay_id\n",
    "optimal_feature_names.extend(['aki_stage', 'icustay_id', 'charttime'])\n",
    "print(optimal_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3361833/647176284.py:82: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for preprocessed_data_1H.csv\n",
      "Train accuracy: 0.883.. Train ROC AUC: 0.89.. Train PR AUC: 0.64..\n",
      "Test accuracy: 0.834.. Test ROC AUC: 0.60.. Test PR AUC: 0.21..\n",
      "Validation accuracy: 0.829.. Validation ROC AUC: 0.59.. Validation PR AUC: 0.21..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:09:55] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3361833/647176284.py:82: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for preprocessed_data_2H.csv\n",
      "Train accuracy: 0.885.. Train ROC AUC: 0.89.. Train PR AUC: 0.65..\n",
      "Test accuracy: 0.836.. Test ROC AUC: 0.62.. Test PR AUC: 0.23..\n",
      "Validation accuracy: 0.832.. Validation ROC AUC: 0.62.. Validation PR AUC: 0.24..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:11:40] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3361833/647176284.py:82: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for preprocessed_data_4H.csv\n",
      "Train accuracy: 0.894.. Train ROC AUC: 0.90.. Train PR AUC: 0.70..\n",
      "Test accuracy: 0.845.. Test ROC AUC: 0.65.. Test PR AUC: 0.28..\n",
      "Validation accuracy: 0.841.. Validation ROC AUC: 0.65.. Validation PR AUC: 0.29..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:12:33] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3361833/647176284.py:82: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for preprocessed_data_6H.csv\n",
      "Train accuracy: 0.907.. Train ROC AUC: 0.91.. Train PR AUC: 0.76..\n",
      "Test accuracy: 0.850.. Test ROC AUC: 0.71.. Test PR AUC: 0.35..\n",
      "Validation accuracy: 0.848.. Validation ROC AUC: 0.71.. Validation PR AUC: 0.37..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:13:10] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3361833/647176284.py:82: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for preprocessed_data_8H.csv\n",
      "Train accuracy: 0.908.. Train ROC AUC: 0.92.. Train PR AUC: 0.79..\n",
      "Test accuracy: 0.844.. Test ROC AUC: 0.72.. Test PR AUC: 0.38..\n",
      "Validation accuracy: 0.843.. Validation ROC AUC: 0.72.. Validation PR AUC: 0.40..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:13:40] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3361833/647176284.py:82: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for preprocessed_data_12H.csv\n",
      "Train accuracy: 0.913.. Train ROC AUC: 0.93.. Train PR AUC: 0.84..\n",
      "Test accuracy: 0.831.. Test ROC AUC: 0.73.. Test PR AUC: 0.42..\n",
      "Validation accuracy: 0.834.. Validation ROC AUC: 0.73.. Validation PR AUC: 0.44..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:14:03] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3361833/647176284.py:82: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for preprocessed_data_24H.csv\n",
      "Train accuracy: 0.932.. Train ROC AUC: 0.97.. Train PR AUC: 0.93..\n",
      "Test accuracy: 0.806.. Test ROC AUC: 0.75.. Test PR AUC: 0.48..\n",
      "Validation accuracy: 0.807.. Validation ROC AUC: 0.75.. Validation PR AUC: 0.49..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:14:19] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3361833/647176284.py:82: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:83: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3361833/647176284.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for preprocessed_data.csv\n",
      "Train accuracy: 0.896.. Train ROC AUC: 0.90.. Train PR AUC: 0.72..\n",
      "Test accuracy: 0.850.. Test ROC AUC: 0.74.. Test PR AUC: 0.43..\n",
      "Validation accuracy: 0.851.. Validation ROC AUC: 0.74.. Validation PR AUC: 0.44..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [02:15:04] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# normal\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import os\n",
    "import gc\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "data_paths = [\n",
    "    \"data/preprocessed/preprocessed_data_1H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_2H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_4H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_6H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_8H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_12H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_24H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data.csv\",\n",
    "              ]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    \n",
    "    tail = data_path.split(\"/\")[-1]\n",
    "    X = pd.read_csv(data_path)\n",
    "    # take only head \n",
    "    # X = X.head(10000)\n",
    "\n",
    "    # For training a testing model, take only icu_stay_id, charttime,creatinine_mean,uo_rt_6hr,aki_stage\n",
    "    # X = X[['icustay_id', 'charttime', 'creatinine_mean', 'uo_rt_6hr', 'aki_stage']]\n",
    "\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage',)\n",
    "    numeric_feat.remove('icustay_id',)\n",
    "\n",
    "\n",
    "    # normalize data and cap features\n",
    "    # X = cap_data(X)\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "\n",
    "    # X = X.sort_values(by=['icustay_id', 'charttime'])\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    seq_lengths = X.groupby(['icustay_id'],as_index=False).size().sort_values(by = ['size'],ascending=False)\n",
    "    sequence_length = seq_lengths.max() # the longest sequence per icustay-id\n",
    "\n",
    "    #AL re-write as try except to make it work as hadm_id is not used if only one csv file is used and none are merged\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    # id_list = X['icustay_id'].unique()\n",
    "    # take the common id list defined earlier\n",
    "    id_list = common_id_list\n",
    "    \n",
    "    id_train, id_test_val = train_test_split(id_list, test_size = SPLIT_SIZE, random_state = 42) # train set is 80%)\n",
    "    # remaining 20% split in halves as test and validation 10% and 10%\n",
    "    id_valid, id_test = train_test_split(id_test_val, test_size = 0.5, random_state = 42) # test 10% valid 10%\n",
    "\n",
    "    # move (\"aki_stage\") to last column\n",
    "    X = X.reindex(columns = [col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "    test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index = True) \n",
    "    validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id']) \n",
    "\n",
    "    test = test.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "    train = train.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "    validation = validation.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "\n",
    "    train.drop(['charttime'], axis=1, inplace = True)  \n",
    "    test.drop(['charttime'], axis=1, inplace = True)\n",
    "    validation.drop(['charttime'], axis=1, inplace = True)\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "\n",
    "\n",
    "    # flatten the train, test and validation data\n",
    "    train_flat = np.concatenate(train, axis=0)\n",
    "    test_flat = np.concatenate(test, axis=0)\n",
    "    validation_flat = np.concatenate(validation, axis=0)\n",
    "\n",
    "    # get the labels\n",
    "    train_labels = np.array([x[-1] for x in train_flat])\n",
    "    test_labels = np.array([x[-1] for x in test_flat])\n",
    "    validation_labels = np.array([x[-1] for x in validation_flat])\n",
    "\n",
    "    # get the features\n",
    "    train_features = np.array([x[1:-1] for x in train_flat])\n",
    "    validation_features = np.array([x[1:-1] for x in validation_flat])\n",
    "    test_features = np.array([x[1:-1] for x in test_flat])\n",
    "\n",
    "    # create the XGBoost classifier\n",
    "    xgb = XGBClassifier(n_estimators=1000, use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM)\n",
    "\n",
    "    # train the classifier\n",
    "    xgb.fit(train_features, train_labels)\n",
    "\n",
    "    # get the predictions\n",
    "    train_predictions = xgb.predict(train_features)\n",
    "    test_predictions = xgb.predict(test_features)\n",
    "    validation_predictions = xgb.predict(validation_features)\n",
    "\n",
    "    # get the accuracy\n",
    "    train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "    test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    validation_accuracy = accuracy_score(validation_labels, validation_predictions)\n",
    "\n",
    "    # get the probabilities of the positive class\n",
    "    training_prob = xgb.predict_proba([x[1:-1] for x in train_flat])[:, 1]\n",
    "    test_prob = xgb.predict_proba([x[1:-1] for x in test_flat])[:, 1]\n",
    "    validation_prob = xgb.predict_proba([x[1:-1] for x in validation_flat])[:, 1]\n",
    "\n",
    "    # calculate ROC AUC and PR AUC for the training set\n",
    "    training_roc_auc = roc_auc_score(train_labels, training_prob)\n",
    "    training_pr_auc = average_precision_score(train_labels, training_prob)\n",
    "\n",
    "    # calculate ROC AUC and PR AUC for the test set\n",
    "    test_roc_auc = roc_auc_score(test_labels, test_prob)\n",
    "    test_pr_auc = average_precision_score(test_labels, test_prob)\n",
    "\n",
    "    # calculate ROC AUC and PR AUC for the validation set\n",
    "    validation_roc_auc = roc_auc_score(validation_labels, validation_prob)\n",
    "    validation_pr_auc = average_precision_score(validation_labels, validation_prob)\n",
    "    \n",
    "    print(f\"Results for {tail}\")\n",
    "    print(f\"Train accuracy: {train_accuracy:.3f}.. Train ROC AUC: {training_roc_auc:.2f}.. Train PR AUC: {training_pr_auc:.2f}..\")\n",
    "    print(f\"Test accuracy: {test_accuracy:.3f}.. Test ROC AUC: {test_roc_auc:.2f}.. Test PR AUC: {test_pr_auc:.2f}..\")\n",
    "    print(f\"Validation accuracy: {validation_accuracy:.3f}.. Validation ROC AUC: {validation_roc_auc:.2f}.. Validation PR AUC: {validation_pr_auc:.2f}..\")\n",
    "\n",
    "    now = datetime.now()\n",
    "    out_path = f'data/models/{tail}'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    # save the xgb model\n",
    "    xgb.save_model(f'{out_path}/xgb.model')\n",
    "    # save normalization parameters\n",
    "    try:\n",
    "        np.save(f'{out_path}/normalization_parameters.npy', normalization_parameters)\n",
    "    except:\n",
    "        pass\n",
    "    # save the train feature names\n",
    "    np.save(f'{out_path}/train_feature_names.npy', X.columns[2:-1])\n",
    "\n",
    "    results[tail] = {'train_accuracy': train_accuracy, 'test_accuracy': test_accuracy, 'validation_accuracy': validation_accuracy,\n",
    "                                'train_roc_auc': training_roc_auc, 'test_roc_auc': test_roc_auc, 'validation_roc_auc': validation_roc_auc,\n",
    "                                'train_pr_auc': training_pr_auc, 'test_pr_auc': test_pr_auc, 'validation_pr_auc': validation_pr_auc}\n",
    "\n",
    "    # save results dict\n",
    "    np.save(f'data/results.npy', results)\n",
    "    \n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "results = {}\n",
    "\n",
    "data_paths = [\n",
    "    \"../data/preprocessed/preprocessed_data_extended_1H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_2H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_3H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_4H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_5H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_6H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data.csv\",\n",
    "]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    tail = data_path.split(\"/\")[-1]\n",
    "    X = pd.read_csv(data_path)\n",
    "\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage')\n",
    "    numeric_feat.remove('icustay_id')\n",
    "\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # id_list = X['icustay_id'].unique()\n",
    "    id_list = common_id_list\n",
    "\n",
    "    common_id_list.sort()\n",
    "    print(common_id_list[:10])\n",
    "\n",
    "    print(len(id_list))\n",
    "\n",
    "    # Move \"aki_stage\" to last column\n",
    "    X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    X.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "    # Group by icustay_id and convert to numpy arrays\n",
    "    grouped_data = X.groupby('icustay_id').apply(lambda x: x.drop('icustay_id', axis=1).to_numpy())\n",
    "\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_results = []\n",
    "\n",
    "    print(len(X))\n",
    "\n",
    "    for fold, (id_train_idx, id_val_idx) in enumerate(kf.split(id_list), 1):\n",
    "        print(f\"Processing fold {fold}\")\n",
    "        id_train = [id_list[idx] for idx in id_train_idx]\n",
    "        id_val = [id_list[idx] for idx in id_val_idx]       \n",
    "\n",
    "        print(len(id_train), len(id_val))\n",
    "        print(id_train[:10], id_val[:10])\n",
    "        train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "        validation = X[X.icustay_id.isin(id_val)].sort_values(by=['icustay_id']) \n",
    "\n",
    "        print(len(train), len(validation))\n",
    "\n",
    "        train = train.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "        validation = validation.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "\n",
    "        try:\n",
    "            X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(len(train), len(validation))\n",
    "\n",
    "        train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "        validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "\n",
    "        print(len(train), len(validation))\n",
    "\n",
    "        # flatten the train, test and validation data\n",
    "        train_flat = np.concatenate(train, axis=0)\n",
    "        validation_flat = np.concatenate(validation, axis=0)\n",
    "\n",
    "        # get the labels\n",
    "        train_labels = np.array([x[-1] for x in train_flat])\n",
    "        val_labels = np.array([x[-1] for x in validation_flat])\n",
    "\n",
    "        # get the features\n",
    "        train_features = np.array([x[1:-1] for x in train_flat])\n",
    "        val_features = np.array([x[1:-1] for x in validation_flat])\n",
    "\n",
    "        # Create and train the XGBoost classifier\n",
    "        xgb = XGBClassifier(n_estimators=1000, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "        xgb.fit(train_features, train_labels)\n",
    "\n",
    "        # Make predictions\n",
    "        train_predictions = xgb.predict(train_features)\n",
    "        val_predictions = xgb.predict(val_features)\n",
    "\n",
    "        # unique values in the labels\n",
    "        unique_labels = np.unique(np.concatenate([train_labels, val_labels]))\n",
    "        print(f\"Unique labels: {unique_labels}\")\n",
    "        unique_labels_pred = np.unique(np.concatenate([train_predictions, val_predictions]))\n",
    "        print(f\"Unique predictions: {unique_labels_pred}\")\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "        val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        \n",
    "\n",
    "        # Calculate probabilities\n",
    "        train_prob = xgb.predict_proba(train_features)[:, 1]\n",
    "        val_prob = xgb.predict_proba(val_features)[:, 1]\n",
    "\n",
    "        # Calculate ROC AUC and PR AUC\n",
    "        train_roc_auc = roc_auc_score(train_labels, train_prob)\n",
    "        train_pr_auc = average_precision_score(train_labels, train_prob)\n",
    "        val_roc_auc = roc_auc_score(val_labels, val_prob)\n",
    "        val_pr_auc = average_precision_score(val_labels, val_prob)\n",
    "\n",
    "        fold_results.append({\n",
    "            'fold': fold,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'train_roc_auc': train_roc_auc,\n",
    "            'val_roc_auc': val_roc_auc,\n",
    "            'train_pr_auc': train_pr_auc,\n",
    "            'val_pr_auc': val_pr_auc,\n",
    "        })\n",
    "\n",
    "        print(f\"Fold {fold} results:\")\n",
    "        print(f\"Train accuracy: {train_accuracy:.3f}, ROC AUC: {train_roc_auc:.2f}, PR AUC: {train_pr_auc:.2f}\")\n",
    "        print(f\"Validation accuracy: {val_accuracy:.3f}, ROC AUC: {val_roc_auc:.2f}, PR AUC: {val_pr_auc:.2f}\")\n",
    "\n",
    "    # Calculate average scores across folds\n",
    "    avg_scores = {\n",
    "        'train_accuracy': np.mean([r['train_accuracy'] for r in fold_results]),\n",
    "        'val_accuracy': np.mean([r['val_accuracy'] for r in fold_results]),\n",
    "        'train_roc_auc': np.mean([r['train_roc_auc'] for r in fold_results]),\n",
    "        'val_roc_auc': np.mean([r['val_roc_auc'] for r in fold_results]),\n",
    "        'train_pr_auc': np.mean([r['train_pr_auc'] for r in fold_results]),\n",
    "        'val_pr_auc': np.mean([r['val_pr_auc'] for r in fold_results]),\n",
    "    }\n",
    "\n",
    "    print(\"\\nAverage scores across 5 folds:\")\n",
    "    for metric, value in avg_scores.items():\n",
    "        print(f\"{metric}: {value:.3f}\")\n",
    "\n",
    "    results[tail] = {\n",
    "        'fold_results': fold_results,\n",
    "        'average_scores': avg_scores\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    np.save(f'data/results_{tail}.npy', results[tail])\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "# Save overall results\n",
    "np.save('data/comparison_time_bins_results_cross_validated.npy', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and predict    \n",
    "loaded_model = XGBClassifier()\n",
    "loaded_model.load_model('data/models/simple_xgboost_model.model')\n",
    "test_predictions = loaded_model.predict(np.array([x[1:-1] for x in test_flat]))\n",
    "validation_predictions = loaded_model.predict(np.array([x[1:-1] for x in validation_flat]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Processing preprocessed_data_4H.csv\n",
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_13464\\3787019221.py:64: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_13464\\3787019221.py:65: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_13464\\3787019221.py:66: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.3823, Train Accuracy: 0.8524, Val Loss: 0.2545, Val AUC: 0.5006,Val Accuracy: 0.9299\n",
      "Epoch 2/50, Train Loss: 0.2160, Train Accuracy: 0.9286, Val Loss: 0.1926, Val AUC: 0.9041,Val Accuracy: 0.9299\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import collections\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "results = {}\n",
    "\n",
    "data_paths = [\n",
    "    \"data/preprocessed/preprocessed_data_4H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_6H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_8H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_extended_4H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_extended_6H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_extended_8H.csv\",\n",
    "]\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')\n",
    "\n",
    "for data_path in data_paths:\n",
    "    tail = data_path.split(\"/\")[-1]\n",
    "    print(f\"Processing {tail}\")\n",
    "\n",
    "    # ... (keep the data loading and preprocessing steps as they are)\n",
    "    X = pd.read_csv(data_path)\n",
    "\n",
    "    # Preprocessing steps (similar to XGBoost)\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage')\n",
    "    numeric_feat.remove('icustay_id')\n",
    "\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Split data (you may want to use the same splitting logic as in XGBoost)\n",
    "    id_list = X['icustay_id'].unique()\n",
    "    # id_list = common_id_list\n",
    "    id_train, id_test_val = train_test_split(id_list, test_size=SPLIT_SIZE, random_state=42)\n",
    "    id_valid, id_test = train_test_split(id_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "    X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "    test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index=True)\n",
    "    validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id'])\n",
    "\n",
    "    train.drop(['charttime'], axis=1, inplace=True)\n",
    "    test.drop(['charttime'], axis=1, inplace=True)\n",
    "    validation.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "    train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    # Prepare data for LSTM\n",
    "    # X_train, y_train = batch(train.to_numpy(), batch_size)\n",
    "    # X_test, y_test = batch(test.to_numpy(), test.shape[0])\n",
    "    # X_val, y_val = batch(validation.to_numpy(), validation.shape[0])\n",
    "    X_train, y_train = batch(train, batch_size)\n",
    "    X_test, y_test = batch(test, batch_size)\n",
    "    X_val, y_val = batch(validation, batch_size)\n",
    "\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(f'runs/{tail}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "\n",
    "    # LSTM parameters\n",
    "    input_size = X_train[0].shape[2]\n",
    "    output_size = 1\n",
    "    emb_size = round(input_size / 1)\n",
    "    number_layers = 3\n",
    "    dropout = 0\n",
    "    bi_directional = True\n",
    "\n",
    "    nn_model = Net(input_size, emb_size, output_size, bi_directional, number_layers, dropout).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "    n_epochs = 50\n",
    "    best_auc = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        nn_model.train()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "\n",
    "        for i, (X_batch, y_batch) in enumerate(zip(X_train, y_train)):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = nn_model(X_batch)\n",
    "            outputs = torch.flatten(outputs)\n",
    "            y_batch = y_batch.type_as(outputs)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            train_accuracy = accuracy_score(y_batch.cpu().numpy(), predicted.cpu().numpy())\n",
    "            running_accuracy += train_accuracy\n",
    "\n",
    "            # Log training metrics to TensorBoard\n",
    "            writer.add_scalar('Training/Loss', loss.item(), epoch * len(X_train) + i)\n",
    "            writer.add_scalar('Training/Accuracy', train_accuracy, epoch * len(X_train) + i)\n",
    "\n",
    "        # Validation\n",
    "        nn_model.eval()\n",
    "        total_v_loss = 0\n",
    "        all_y_val = []\n",
    "        all_val_prob = []\n",
    "        all_accuracy = 0\n",
    "\n",
    "        for X_val_batch, y_val_batch in zip(X_val, y_val):\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                v_out = nn_model(X_val_batch)\n",
    "                v_out = torch.flatten(v_out)\n",
    "                y_val_batch = y_val_batch.type_as(v_out)\n",
    "                v_loss = criterion(v_out, y_val_batch)\n",
    "                val_prob = torch.sigmoid(v_out)\n",
    "                total_v_loss += v_loss.item()\n",
    "                all_y_val.extend(y_val_batch.cpu().numpy())\n",
    "                all_val_prob.extend(val_prob.cpu().numpy())\n",
    "                \n",
    "                predicted = torch.sigmoid(v_out) > 0.5\n",
    "                val_accuracy = accuracy_score(y_val_batch.cpu().numpy(), predicted.cpu().numpy())\n",
    "                all_accuracy += val_accuracy\n",
    "\n",
    "        avg_v_loss = total_v_loss / len(X_val)\n",
    "        roc_auc = roc_auc_score(all_y_val, all_val_prob)\n",
    "        avg_accuracy = all_accuracy / len(X_val)\n",
    "        \n",
    "        # Log validation metrics to TensorBoard\n",
    "        writer.add_scalar('Validation/Loss', avg_v_loss, epoch)\n",
    "        writer.add_scalar('Validation/AUC', roc_auc, epoch)\n",
    "        writer.add_scalar('Validation/Accuracy', avg_accuracy, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, \"\n",
    "            f\"Train Loss: {running_loss/len(X_train):.4f}, \"\n",
    "            f\"Train Accuracy: {running_accuracy/len(X_train):.4f}, \"\n",
    "            f\"Val Loss: {avg_v_loss:.4f}, \"\n",
    "            f\"Val AUC: {roc_auc:.4f},\"\n",
    "            f\"Val Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "        if roc_auc > best_auc:\n",
    "            best_auc = roc_auc\n",
    "            torch.save(nn_model.state_dict(), f'data/models/{tail}/LSTM_best.pth')\n",
    "\n",
    "    # Close the TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing preprocessed_data_6H.csv\n",
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_13464\\4053935003.py:48: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_13464\\4053935003.py:49: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_13464\\4053935003.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1656, Test AUC: 0.9267, Test F1: 0.3431, Test PR AUC: 0.4332\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import collections\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "\n",
    "data_path = \"data/preprocessed/preprocessed_data_6H.csv\"\n",
    "tail = data_path.split(\"/\")[-1]\n",
    "print(f\"Processing {tail}\")\n",
    "\n",
    "X = pd.read_csv(data_path)\n",
    "\n",
    "# Preprocessing steps (similar to XGBoost)\n",
    "numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_feat.remove('aki_stage')\n",
    "numeric_feat.remove('icustay_id')\n",
    "\n",
    "X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "try:\n",
    "    X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Split data (you may want to use the same splitting logic as in XGBoost)\n",
    "id_list = X['icustay_id'].unique()\n",
    "# id_list = common_id_list\n",
    "id_train, id_test_val = train_test_split(id_list, test_size=SPLIT_SIZE, random_state=42)\n",
    "id_valid, id_test = train_test_split(id_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index=True)\n",
    "validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id'])\n",
    "\n",
    "train.drop(['charttime'], axis=1, inplace=True)\n",
    "test.drop(['charttime'], axis=1, inplace=True)\n",
    "validation.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "# Prepare data for LSTM\n",
    "# X_train, y_train = batch(train.to_numpy(), batch_size)\n",
    "# X_test, y_test = batch(test.to_numpy(), test.shape[0])\n",
    "# X_val, y_val = batch(validation.to_numpy(), validation.shape[0])\n",
    "X_train, y_train = batch(train, batch_size)\n",
    "X_test, y_test = batch(test, batch_size)\n",
    "X_val, y_val = batch(validation, batch_size)\n",
    "\n",
    "# LSTM parameters\n",
    "input_size = X_train[0].shape[2]  # Subtract 2 for icustay_id and aki_stage\n",
    "output_size = 1\n",
    "emb_size = round(input_size / 1)\n",
    "number_layers = 3\n",
    "dropout = 0\n",
    "bi_directional = True\n",
    "\n",
    "\n",
    "# Assuming Net is defined elsewhere\n",
    "# Assuming X_train, y_train, X_val, y_val, X_test, y_test are defined and split into batches if necessary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "nn_model = Net(input_size, emb_size, output_size, bi_directional, number_layers, dropout).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Test evaluation with F1 score\n",
    "nn_model.load_state_dict(torch.load(f'data/models/{tail}/LSTM_best.pth'))\n",
    "nn_model.eval()\n",
    "total_test_loss = 0\n",
    "all_y_test = []\n",
    "all_test_prob = []\n",
    "all_test_f1 = 0\n",
    "\n",
    "for X_test_batch, y_test_batch in zip(X_test, y_test):\n",
    "    X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        t_out = nn_model(X_test_batch)\n",
    "        t_out = torch.flatten(t_out)\n",
    "        y_test_batch = y_test_batch.type_as(t_out)\n",
    "        test_loss = criterion(t_out, y_test_batch)\n",
    "        test_prob = torch.sigmoid(t_out)\n",
    "        total_test_loss += test_loss.item()\n",
    "        all_y_test.extend(y_test_batch.cpu().numpy())\n",
    "        all_test_prob.extend(test_prob.cpu().numpy())\n",
    "        \n",
    "        predicted = torch.sigmoid(t_out) > 0.08\n",
    "        test_f1 = f1_score(y_test_batch.cpu().numpy(), predicted.cpu().numpy(), zero_division=1)\n",
    "        all_test_f1 += test_f1\n",
    "        \n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(all_y_test, all_test_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "os.makedirs('data/plots', exist_ok=True)\n",
    "plt.savefig(f'data/plots/ROC_{tail}.png')  # Save ROC curve\n",
    "plt.close()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(all_y_test, all_test_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='PR curve (area = %0.2f)' % pr_auc)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "os.makedirs('data/plots', exist_ok=True)\n",
    "plt.savefig(f'data/plots/PR_{tail}.png')  # Save PR curve\n",
    "plt.close()\n",
    "\n",
    "print(f\"Test Loss: {total_test_loss / len(X_test):.4f}, \"\n",
    "    f\"Test AUC: {roc_auc:.4f}, \"\n",
    "    f\"Test F1: {all_test_f1 / len(X_test):.4f}, \"\n",
    "    f\"Test PR AUC: {pr_auc:.4f}\")\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.0816330760717392\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import numpy as np\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(all_y_test, all_test_prob)\n",
    "# Find the optimal threshold\n",
    "optimal_idx = np.argmin(np.sqrt(np.square(1-tpr) + np.square(fpr)))\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.17803381383419037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(all_y_test, all_test_prob)\n",
    "# Add a last threshold corresponding to recall = 0.\n",
    "thresholds = np.append(thresholds, 1)\n",
    "\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "# Find the optimal threshold\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.18\n",
      "Best F1 Score: 0.47297940834714364\n"
     ]
    }
   ],
   "source": [
    "# Convert all_y_test and all_test_prob to numpy arrays for easier manipulation\n",
    "all_y_test = np.array(all_y_test)\n",
    "all_test_prob = np.array(all_test_prob)\n",
    "\n",
    "# Initialize variables to store the best threshold and its corresponding F1 score\n",
    "best_threshold = 0.0\n",
    "best_f1 = 0.0\n",
    "\n",
    "# Iterate over a range of possible threshold values (e.g., 0 to 1, step 0.01)\n",
    "for threshold in np.arange(0.0, 1.01, 0.01):\n",
    "    # Convert probabilities to binary predictions based on the current threshold\n",
    "    predictions = (all_test_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate F1 score for the current threshold\n",
    "    f1 = f1_score(all_y_test, predictions, zero_division=1)\n",
    "    \n",
    "    # Update best threshold and F1 score if the current F1 score is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "# Print the best threshold and its corresponding F1 score\n",
    "print(f\"Best Threshold: {best_threshold}\")\n",
    "print(f\"Best F1 Score: {best_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Training on CPU') # On mac book GPU is not possible =() \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(np.array(train)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    times = math.floor(data.shape[0]/batch_size)\n",
    "    remainder = data.shape[0]%times\n",
    "    a = 0\n",
    "    start = 0\n",
    "    end = start+batch_size\n",
    "    if remainder ==0:\n",
    "        a +=1\n",
    "    while a<times:\n",
    "        temp = pad(data[start:end,],0)\n",
    "        x = torch.from_numpy(temp[:,:,1:-1]).float() # without icustay_id and without aki_stage columns\n",
    "        y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "        X_batches.append(x)\n",
    "        y_batches.append(y)\n",
    "        start = end\n",
    "        end = start+batch_size\n",
    "        a +=1\n",
    "    temp = pad(data[start:data.shape[0]],0)\n",
    "    x = torch.from_numpy(temp[:,:,1:-1]).float()\n",
    "    y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "    X_batches.append(x)\n",
    "    y_batches.append(y)\n",
    "    if len(X_batches) != len(y_batches):\n",
    "        print(\"length error\")\n",
    "    return X_batches, y_batches # arrays\n",
    "\n",
    "# batching\n",
    "X_train, y_train = batch(train, batch_size) # to count weights\n",
    "\n",
    "# counting balance of the classes\n",
    "y = []\n",
    "for i in y_train:\n",
    "    for element in i:\n",
    "        y.append(element.item())\n",
    "\n",
    "#  weights\n",
    "counter=collections.Counter(y)\n",
    "print(counter)\n",
    "zeroes = counter[0]\n",
    "ones = counter[1]\n",
    "\n",
    "X_test, y_test = batch(test, test.shape[0]) \n",
    "X_val, y_val = batch(validation, validation.shape[0])\n",
    "X_val = X_val[0]\n",
    "y_val = y_val[0]\n",
    "X_test = X_test[0]\n",
    "y_test = y_test[0]\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# setup\n",
    "\n",
    "bi_directional = True\n",
    "n_epochs = 50\n",
    "lr = 0.001\n",
    "features = len(X_train[0][0][0])\n",
    "print(features)\n",
    "# features = \n",
    "emb_size = round(features/1)\n",
    "number_layers = 3\n",
    "dropout = 0 # dropout\n",
    "\n",
    "##########################\n",
    "input_size = features\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size \n",
    "        self.output_size = output_size\n",
    "        self.number_layers = number_layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "        # in bidirectional encoder we have  forward and backward hidden states\n",
    "        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "        # Create affine layer to project to the classes \n",
    "        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "        #dropout layer for regularizetion of a sequence\n",
    "        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "        h = self.relu(self.combination_layer(h))\n",
    "        h = self.dropout_layer(h)\n",
    "        h = self.projection(h) \n",
    "        return h\n",
    "\n",
    "#create a network \n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "#print(nn_model)\n",
    "#print(list(nn_model.parameters()))\n",
    "\n",
    "\n",
    "# BCE Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss() # class imbalance\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(round(zeroes/ones,0))) # class imbalance\n",
    "#print(round(zeroes/ones,0))\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=lr) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count unique values\n",
    "unique, counts = np.unique(y_val, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "# Count NaN values\n",
    "nan_count = np.isnan(y_val).sum()\n",
    "print(f\"Number of NaN values: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all nans in X_val with 0\n",
    "X_val[torch.isnan(X_val)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_original = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_original = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_extended = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training loop (full data 3.5 hours)\n",
    "\n",
    "epochs = n_epochs\n",
    "starttime = datetime.now() # datetime object containing current date and time\n",
    "train_losses, validation_losses = [], []\n",
    "best = 0\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    print (\"\\n Epoch [%d] out of %d\" % (epoch + 1, epochs))\n",
    "    running_loss = 0.0\n",
    "    validation_loss = 0.0\n",
    "    roc_auc = 0.0\n",
    "    pr_auc = 0.0\n",
    "    m = 0\n",
    "    \n",
    "    #train\n",
    "    #print(list(nn_model.parameters())[0])\n",
    "    # pbar = tqdm(X_train, desc=f\"Epoch {epoch+1}\")\n",
    "    # for i in pbar:\n",
    "    #     # zero the parameter gradients\n",
    "    #     optimizer.zero_grad() # zero the gradient buffers not to consider gradients of previous iterations\n",
    "    #     X_batch = X_train[m]\n",
    "    #     y_batch = y_train[m]\n",
    "    #     # print(X_batch.shape)\n",
    "    #     # forward + backward + optimize\n",
    "    #     outputs = nn_model(X_batch)\n",
    "    #     outputs = torch.flatten(outputs)\n",
    "    #     y_batch = y_batch.type_as(outputs)\n",
    "    #     loss = criterion(outputs, y_batch)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step() # Does the update\n",
    "    #     running_loss += loss.item()\n",
    "    #     m +=1\n",
    "    #     pbar.set_postfix({\"Training Loss\": running_loss/len(X_train)})\n",
    "        \n",
    "   \n",
    "    #validation \n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        v_out = nn_model(X_val) \n",
    "        v_out = torch.flatten(v_out) \n",
    "        y_val = y_val.type_as(v_out)\n",
    "        v_loss = criterion(v_out, y_val)\n",
    "        validation_loss = v_loss.item()\n",
    "        # auc and pr auc\n",
    "        val_prob = torch.nn.Sigmoid() (v_out)\n",
    "        print(type(v_out))\n",
    "        print(v_out)\n",
    "        print(val_prob)\n",
    "        print(y_val)\n",
    "        roc_auc = roc_auc_score(y_val,val_prob) \n",
    "        \n",
    "    validation_losses.append(validation_loss) \n",
    "    train_losses.append(running_loss/len(X_train)) \n",
    "    print(f\"Training loss: {running_loss/len(X_train):.3f}.. \" f\"Validation loss: {validation_loss:.3f}.. \")\n",
    "    print(f\"AUC: {roc_auc:.2f}\")  \n",
    "    nn_model.train()\n",
    "    \n",
    "    \n",
    "    if roc_auc > best:\n",
    "        best = roc_auc\n",
    "        PATH = './LSTMbest.pth' \n",
    "        torch.save(nn_model.state_dict(), PATH) # save the model\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "       \n",
    "print('Finished Training')\n",
    "print(\"starttime =\", starttime)\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "PATH = './LSTM.pth' \n",
    "torch.save(nn_model.state_dict(), PATH) # save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "PATH = './LSTM.pth'\n",
    "nn_model.load_state_dict(torch.load(PATH))\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    t_out = nn_model(X_test)\n",
    "    t_out = torch.flatten(t_out)\n",
    "    y_test = y_test.type_as(t_out)\n",
    "    t_loss = criterion(t_out, y_test)\n",
    "    test_loss = t_loss.item()\n",
    "    # auc and pr auc\n",
    "    test_prob = torch.nn.Sigmoid() (t_out)\n",
    "    roc_auc = roc_auc_score(y_test,test_prob) \n",
    "    pr_auc = average_precision_score(y_test,test_prob)\n",
    "    # convert output probabilities to class labels\n",
    "    test_pred = (test_prob > 0.5).float()\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = accuracy_score(y_test.cpu().numpy(), test_pred.cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}.. \" f\"ROC AUC: {roc_auc:.2f}.. \" f\"PR AUC: {pr_auc:.2f}.. \")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a freshly initialized model on test\n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "# nn_model.load_state_dict(torch.load(PATH))\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    t_out = nn_model(X_test)\n",
    "    t_out = torch.flatten(t_out)\n",
    "    y_test = y_test.type_as(t_out)\n",
    "    t_loss = criterion(t_out, y_test)\n",
    "    test_loss = t_loss.item()\n",
    "    # auc and pr auc\n",
    "    test_prob = torch.nn.Sigmoid() (t_out)\n",
    "    roc_auc = roc_auc_score(y_test,test_prob) \n",
    "    pr_auc = average_precision_score(y_test,test_prob)\n",
    "    print(f\"Test loss: {test_loss:.3f}.. \" f\"ROC AUC: {roc_auc:.2f}.. \" f\"PR AUC: {pr_auc:.2f}.. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './i-Bidir_3_lr_0.001_nodropbest.pth'\n",
    "\n",
    "# save the model\n",
    "#torch.save(nn_model.state_dict(), PATH)\n",
    "\n",
    "# code to load saved model\n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "nn_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test) # single batch with zero padding to the max shape 635208"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = nn_model(X_test)\n",
    "pred = torch.nn.Sigmoid() (logits)\n",
    "pred = pred.detach().numpy()\n",
    "pred = pred.reshape(-1,1)\n",
    "print(\"Performance on full X_test where it has no batching: is padded to max dimentions. \\n\")\n",
    "print (\"Area Under ROC Curve: %0.2f\" % roc_auc_score(y_test, pred, average = 'micro')  )\n",
    "brier = round(metrics.brier_score_loss(y_test, pred, sample_weight=None, pos_label=None),3)\n",
    "print(\"Brier score : {:.3f}\".format(brier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('padded_lstm.npy', 'wb') as f:\n",
    "    np.save(f, y_test)\n",
    "    np.save(f, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = X_test.shape[1] #133\n",
    "icustays = X_test.shape[0]\n",
    "times = []\n",
    "auc_s = []\n",
    "t = 0\n",
    "\n",
    "while t < timestamps:\n",
    "    times.append(t+1)\n",
    "    row = t\n",
    "    i = 0\n",
    "    prob_t = []\n",
    "    y_t = []\n",
    "    while i < icustays:\n",
    "        prob_t.append(pred[row])\n",
    "        y_t.append(y_test[row])\n",
    "        row += timestamps\n",
    "        i +=1\n",
    "    prob_t = np.array(prob_t).reshape(-1,1)\n",
    "    y_t = np.array(y_t).reshape(-1,1)\n",
    "    auc_s.append(roc_auc_score(y_t, prob_t, average = 'micro'))\n",
    "    t +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.DataFrame(auc_s, columns = ['AUC'])\n",
    "df['Timestamps'] = times\n",
    "#df[120:133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "sns.lineplot(x=\"Timestamps\", y=\"AUC\", color = 'g',\n",
    "             data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing to LogR, XGB, RF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = batch(test, test.shape[0]) \n",
    "X_test = X_test[0]\n",
    "y_test = y_test[0]\n",
    "\n",
    "\n",
    "def to_one_label (model, label_list,X_test,index_list):\n",
    "    # evaluate on a test set\n",
    "    labels = np.array(label_list)\n",
    "    labels = labels.reshape(-1,1)\n",
    "    labels = labels.astype(int)\n",
    "    logits = model(X_test)\n",
    "    pred = torch.nn.Sigmoid() (logits)\n",
    "    max_rows = pred.shape[1]\n",
    "    predictions = pred.detach().numpy()\n",
    "    predictions = predictions.reshape(-1,1) \n",
    "    # select 1 per icu stay id by index\n",
    "    prob_1_label = []\n",
    "    row = 0\n",
    "    prev = 0\n",
    "    for i in index_list:\n",
    "        prob_1_label.append(predictions[row+i-prev])\n",
    "        row += pred.shape[1]\n",
    "        prev = i\n",
    "    prob_1_label = np.array(prob_1_label).reshape(-1,1)\n",
    "    \n",
    "    return labels, prob_1_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance (y_test, pred_probabilities):\n",
    "    # performance\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    # compute roc auc\n",
    "    roc_auc = roc_auc_score(y_test, pred_probabilities, average = 'micro')\n",
    "    # compute Precision_Recall curves\n",
    "    precision, recall, _ = precision_recall_curve(y_test, pred_probabilities)\n",
    "    # compute PR_AUC\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "       \n",
    "    # I add confusion matrix\n",
    "    optimal_cut_off = round(thresholds[np.argmax(tpr - fpr)],4)\n",
    "    a = np.where(pred_probabilities > optimal_cut_off, 1, 0)\n",
    "    brier = round(metrics.brier_score_loss(y_test, pred_probabilities, sample_weight=None, pos_label=None),3)\n",
    "    predictions = np.where(pred_probabilities > optimal_cut_off, 1, 0)  \n",
    "    \n",
    "    print (\"Area Under ROC Curve: %0.2f\" % roc_auc  )\n",
    "    #print (\"Area Under PR Curve(AP): %0.2f\" % pr_auc  ) \n",
    "    print(\"Brier score : {:.3f}\".format(brier))\n",
    "    #print('Accuracy for Classifier : {:.2f}'.format(accuracy_score(y_test, predictions)))\n",
    "    #print('Cut off: ' + str(optimal_cut_off))\n",
    "    matrix = metrics.confusion_matrix(y_test, a, labels=None, normalize=None)\n",
    "    #print(str(matrix))\n",
    "    \n",
    "    #f.write(\"\\n Area Under ROC Curve: \" +str(roc_auc))\n",
    "    #f.write(\"\\n Area Under PR Curve(AP): \" + str(pr_auc))\n",
    "    #f.write(\"\\n Brier score: \" +str(brier))\n",
    "    #f.write('\\n Accuracy for Classifier '+str(round((accuracy_score(labels, predictions)),3)))\n",
    "    #f.write(\"\\n Cut off: \" +str(optimal_cut_off))\n",
    "    #f.write(str(matrix))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels, prob_1_label = to_one_label (nn_model, label_list,X_test,index_list)\n",
    "performance(labels,prob_1_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save labels, prob_1_label\n",
    "\n",
    "with open('test.npy', 'wb') as f:\n",
    "    #np.save(f, labels)\n",
    "    np.save(f, prob_1_label)\n",
    "with open('test.npy', 'rb') as f:\n",
    "    #lstm_labels = np.load(f)\n",
    "    lstm_prob = np.load(f)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply integrated gradients, we first create an IntegratedGradients object, providing the model object.\n",
    "ig = IntegratedGradients(nn_model)\n",
    "# To compute the integrated gradients, we use the attribute method of the IntegratedGradients object. The method takes\n",
    "# tensor(s) of input examples (matching the forward function of the model), and returns the input attributions for the\n",
    "# given examples. A target index, defining the index of the output for which gradients are computed is 1, \n",
    "# corresponding to AKI (1/0).\n",
    "\n",
    "#The input tensor provided should require grad, so we call requires_grad_ on the tensor. The attribute method also \n",
    "# takes a baseline, which is the starting point from which gradients are integrated. The default value is just the \n",
    "# 0 tensor, which is a reasonable baseline / default for this task.\n",
    "\n",
    "#The returned values of the attribute method are the attributions, which match the size of the given inputs, and delta,\n",
    "# which approximates the error between the approximated integral and true integral.\n",
    "print(datetime.now())\n",
    "X_test.requires_grad_()\n",
    "attr, delta = ig.attribute(X_test,target=1, return_convergence_delta=True)\n",
    "attr = attr.detach().numpy()\n",
    "attr= np.reshape(attr,(-1,35))\n",
    "importances = np.mean(attr, axis=0)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[:,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[:,4].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_importances(feature_names, importances, title=\"LSTM Average Feature Importances\", axis_title=\"Features\"):\n",
    "    print(title)\n",
    "    i = 0\n",
    "    while i < features:\n",
    "        print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "        i +=1\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    \n",
    "visualize_feature_importances(feature_names, importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df =  pd.DataFrame(importances, columns = ['Feature Importance'])\n",
    "lstm_df['Features'] = feature_names\n",
    "lstm_df = lstm_df.sort_values(by = ['Feature Importance'], ascending = False, ignore_index = True)\n",
    "#lstm_df[\"Feature Importance\"] =  lstm_df[\"Feature Importance\"]\n",
    "#lstm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df[\"Feature Importance\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df)\n",
    "ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df, color = 'grey')\n",
    "ax.set_xlabel('Feature Importance', fontsize = 15)\n",
    "ax.set_ylabel(\"Features\",fontsize=15)\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 6)\n",
    "plt.title('LSTM feature Importances')\n",
    "plt.savefig('LSTM_feature_importance_grey.png', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df['abs'] = abs(lstm_df['Feature Importance'])\n",
    "lstm_df = lstm_df.sort_values(by = ['abs'], ascending = False, ignore_index = True)\n",
    "lstm_df_10 = lstm_df.head(10)\n",
    "#lstm_df_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df_10, palette=\"mako\")\n",
    "\n",
    "ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df_10, color = 'darkgreen')\n",
    "ax.set_xlabel('Feature Importance', fontsize = 15)\n",
    "ax.set_ylabel(\"Features\",fontsize=15)\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 10)\n",
    "plt.title('LSTM top 10 features by feature importance')\n",
    "plt.savefig('LSTM_top10_feature_importance_darkgreen.png', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graphs (y_test,pred_probabilities, classifier_name, plot_name, algorithm):\n",
    "    \n",
    "    def bin_total(y_true, y_prob, n_bins):\n",
    "        bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "        # In sklearn.calibration.calibration_curve, the last value in the array is always 0.\n",
    "        binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "        return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "    def missing_bin(bin_array):\n",
    "        midpoint = \" \"    \n",
    "        if bin_array[0]==0:\n",
    "            midpoint = \"5%, \"\n",
    "        if bin_array[1]==0:\n",
    "            midpoint = midpoint + \"15%, \"\n",
    "        if bin_array[2]==0:\n",
    "            midpoint = midpoint + \"25%, \"\n",
    "        if bin_array[3]==0:\n",
    "            midpoint = midpoint + \"35%, \" \n",
    "        if bin_array[4]==0:\n",
    "            midpoint = midpoint + \"45%, \"\n",
    "        if bin_array[5]==0:\n",
    "            midpoint = midpoint + \"55%, \"\n",
    "        if bin_array[6]==0:\n",
    "            midpoint = midpoint + \"65%, \"\n",
    "        if bin_array[7]==0:\n",
    "            midpoint = midpoint + \"75%, \"\n",
    "        if bin_array[8]==0:\n",
    "            midpoint = midpoint + \"85%, \"\n",
    "        if bin_array[9]==0:\n",
    "            midpoint = midpoint + \"95%, \"\n",
    "        return \"The missing bins have midpoint values of \"+ str(midpoint)\n",
    "    \n",
    "    # performance\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    # compute roc auc\n",
    "    roc_auc = roc_auc_score(y_test, pred_probabilities, average = 'micro')\n",
    "    # compute Precision_Recall curves\n",
    "    precision, recall, _ = precision_recall_curve(y_test, pred_probabilities)\n",
    "    # compute PR_AUC\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "\n",
    "    # compute calibration curve\n",
    "    LR_y, LR_x = calibration_curve(y_test, pred_probabilities, n_bins=10)\n",
    "    #find out which one are the missing bins\n",
    "    bin_array = bin_total(y_test, pred_probabilities , n_bins=10)\n",
    "    print(missing_bin(bin_array))\n",
    "\n",
    "    print(\"plot curves and save in one png file\")\n",
    "    #save three plots in one png file\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, figsize=(7, 24))\n",
    "    fig.subplots_adjust(wspace=0.3, hspace= 0.3)\n",
    "    fig.suptitle('Evaluation of '+ plot_name)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    \n",
    "    # plot roc curve\n",
    "    ax1.plot(fpr, tpr,'C2', label=algorithm+\" \"+\"Classifier \" + str(classifier_name) + \", auc=\" +str(round(roc_auc,2)))\n",
    "    ax1.title.set_text('ROC AUC')\n",
    "    ax1.set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot PR curve\n",
    "    ax2.plot(recall, precision,'C2', label=algorithm+\" \"+\"Classifier \" + str(classifier_name) + \", auc=\"+str(round(pr_auc,2)))\n",
    "    ax2.title.set_text('PR AUC')\n",
    "    ax2.set(xlabel='Recall', ylabel='Precision')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot calibration curve\n",
    "    ax3.plot(LR_x, LR_y, 'C2',marker='o', linewidth=1, label='LR')\n",
    "    line = mlines.Line2D([0, 1], [0, 1], color='black')\n",
    "    transform = ax3.transAxes\n",
    "    line.set_transform(transform)\n",
    "    ax3.add_line(line)\n",
    "    ax3.title.set_text('Calibration plot for '+str(plot_name))\n",
    "    ax3.set(xlabel= 'Predicted probability', ylabel= 'True probability in each bin')\n",
    "    ax3.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.savefig(plot_name+\".png\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(pred_probabilities, y_test, dist_name):\n",
    "    #probabilities distributions graphs\n",
    "    true_1 = pd.DataFrame(pred_probabilities, columns=['Predicted probabilities'])\n",
    "    true_1['labels'] = y_test.tolist()\n",
    "    true_0 = true_1.copy(deep = True) \n",
    "    indexNames = true_1[true_1['labels'] == 0].index\n",
    "    true_1.drop(indexNames , inplace=True)\n",
    "    indexNames = true_0[ true_0['labels'] == 1 ].index\n",
    "    true_0.drop(indexNames , inplace=True)\n",
    "    true_1.drop(columns=['labels'], inplace = True)\n",
    "    true_0.drop(columns=['labels'], inplace = True)\n",
    "    \n",
    "    sns.distplot(true_1['Predicted probabilities'], hist = False, kde = True,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3,\"color\": \"g\"}, label = 'Class 1')\n",
    "    plt.ylabel('Density')\n",
    "    sns.distplot(true_0['Predicted probabilities'], hist = False, kde = True,\n",
    "                     kde_kws = {'shade': True, 'linewidth': 3}, label = 'Class 0')\n",
    "    plt.title('Density Plot'+ dist_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distribution(prob_1_label, labels.flatten(), \" Bidirectional LSTM no imputation \")\n",
    "plt.savefig('dist_LSTM_bi_NOimp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"None vs. Any AKI\"    ###change every time #Moderate vs. Severe #None vs. Any #Others vs. Severe\n",
    "plot_name = \"LSTM NO imputation\"\n",
    "build_graphs(labels.flatten(), prob_1_label.flatten(), classifier_name, plot_name, \"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(labels, prob_1_label)\n",
    "fpr, tpr, thresholds = roc_curve(labels, prob_1_label)\n",
    "optimal_cut_off = round(thresholds[np.argmax(tpr - fpr)],2)\n",
    "prediction = np.where(prob_1_label > optimal_cut_off, 1, 0)\n",
    "f1 = f1_score(labels,prediction)\n",
    "prauc =auc(recall, precision)\n",
    "print('F1 = %.3f, PR auc =%.3f' % (f1,prauc))\n",
    "\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(labels[labels==1]) / len(labels)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall,precision, marker='.', label='LSTM')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search grid \n",
    "layers = [1,2,3]\n",
    "l_rate = [0.001, 0.0001]\n",
    "drop = [0,0.2]\n",
    "bidirectionality = [True,False]\n",
    "#loops count\n",
    "hypercount = 0\n",
    "# static parameters\n",
    "n_epochs = 80\n",
    "emb_size = round(features/1)\n",
    "input_size = features\n",
    "output_size = 1\n",
    "###############################\n",
    "\n",
    "f = open('lstm_no_imp_uni.txt', 'w+') #change with or without imp\n",
    "\n",
    "for q1 in bidirectionality:\n",
    "    for q2 in layers:\n",
    "        for q3 in drop:\n",
    "            for q4 in l_rate:\n",
    "                hypercount +=1\n",
    "                name = \"i-Bidir_\" if q1 else \"i-Onedir_\"\n",
    "                name = name+str(q2) + \"_lr_\"+str(q4)\n",
    "                name = name+\"_drop\"+str(q3) if q3 == 0.2 else name+\"_nodrop\"\n",
    "                #set parameters\n",
    "                bi_directional = q1\n",
    "                lr = q4\n",
    "                number_layers = q2\n",
    "                dropout = q3 # dropout\n",
    "                print('hypercount: %d' % hypercount)\n",
    "                print('\\n')\n",
    "                print(name)\n",
    "                f.write('\\n\\n' + str(name)+ '\\n\\n')\n",
    "                    \n",
    "                # create the NN\n",
    "                class Net(nn.Module):\n",
    "                    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "                        super(Net, self).__init__()\n",
    "                        self.input_size = input_size\n",
    "                        self.emb_size = emb_size \n",
    "                        self.output_size = output_size\n",
    "                        self.number_layers = number_layers\n",
    "                        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "                        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "                        # in bidirectional encoder we have  forward and backward hidden states\n",
    "                        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "                        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "                        # Create affine layer to project to the classes \n",
    "                        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "                        #dropout layer for regularizetion of a sequence\n",
    "                        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "                        self.relu = nn.ReLU()\n",
    "\n",
    "                    def forward(self, x):\n",
    "                        h = self.relu(self.fc1(x))\n",
    "                        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "                        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "                        h = self.relu(self.combination_layer(h))\n",
    "                        h = self.dropout_layer(h)\n",
    "                        h = self.projection(h) \n",
    "                        return h\n",
    "\n",
    "                #create a network \n",
    "                nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "                print(nn_model)\n",
    "                #print(list(nn_model.parameters()))\n",
    "                \n",
    "                # BCE Loss and optimizer\n",
    "                criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(round(zeroes/ones,0))) # class imbalance\n",
    "                #print(round(zeroes/ones,0))\n",
    "                optimizer = optim.Adam(nn_model.parameters(), lr=lr) \n",
    "    \n",
    "    \n",
    "                # TRAINING LOOP \n",
    "                epochs = n_epochs\n",
    "                starttime = datetime.now() # datetime object containing current date and time\n",
    "                train_losses, validation_losses = [], []\n",
    "                best = 0\n",
    "                patience = 0\n",
    "                old_auc = 0\n",
    "                old_pr = 0\n",
    "\n",
    "                for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "                    print (\"\\n Epoch [%d] out of %d\" % (epoch + 1, epochs))\n",
    "                    running_loss = 0.0\n",
    "                    validation_loss = 0.0\n",
    "                    roc_auc = 0.0\n",
    "                    pr_auc = 0.0\n",
    "                    m = 0\n",
    "                    \n",
    "                    #train\n",
    "                    #print(list(nn_model.parameters())[0])\n",
    "                    for i in X_train:\n",
    "                        # zero the parameter gradients\n",
    "                        optimizer.zero_grad() # zero the gradient buffers not to consider gradients of previous iterations\n",
    "                        X_batch = X_train[m]\n",
    "                        y_batch = y_train[m]\n",
    "                        # forward + backward + optimize\n",
    "                        outputs = nn_model(X_batch)\n",
    "                        outputs = torch.flatten(outputs)\n",
    "                        y_batch = y_batch.type_as(outputs)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                        loss.backward()\n",
    "                        optimizer.step() # Does the update\n",
    "                        running_loss += loss.item()\n",
    "                        m +=1\n",
    "                    #validation \n",
    "                    nn_model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        v_out = nn_model(X_val) \n",
    "                        v_out = torch.flatten(v_out) \n",
    "                        y_val = y_val.type_as(v_out)\n",
    "                        v_loss = criterion(v_out, y_val)\n",
    "                        validation_loss = v_loss.item()\n",
    "                        # auc and pr auc\n",
    "                        val_prob = torch.nn.Sigmoid() (v_out)\n",
    "                        precision, recall, thresholds = precision_recall_curve(y_val, val_prob)\n",
    "                        pr_auc = auc(recall, precision)\n",
    "                        roc_auc = roc_auc_score(y_val,val_prob) \n",
    "\n",
    "                    validation_losses.append(validation_loss) \n",
    "                    train_losses.append(running_loss/len(X_train)) \n",
    "                    print(f\"Training loss: {running_loss/len(X_train):.3f}.. \" f\"Validation loss: {validation_loss:.3f}.. \")\n",
    "                    print(f\"AUC: {roc_auc:.2f} \" f\"PR AUC: {pr_auc:.2f} \")  \n",
    "                    nn_model.train()\n",
    "\n",
    "                    \n",
    "                    if roc_auc > best:\n",
    "                        best = roc_auc\n",
    "                        PATH1 = './'+str(name)+'best.pth' \n",
    "                        torch.save(nn_model.state_dict(), PATH1) # save the model\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    if roc_auc == old_auc and pr_auc==old_pr:\n",
    "                        patience +=1\n",
    "                    old_auc = roc_auc\n",
    "                    old_pr = pr_auc\n",
    "                    if patience ==10:\n",
    "                        print(\"out of patience\")\n",
    "                        break\n",
    "\n",
    "                print('\\n Finished Training')\n",
    "                print(\"starttime =\", starttime)\n",
    "                now = datetime.now()\n",
    "                print(\"endtime =\", now)\n",
    "                # end of training loop\n",
    "                \n",
    "                PATH2 = './'+str(name)+'last.pth' \n",
    "                torch.save(nn_model.state_dict(), PATH2) # save the model\n",
    "                print('\\n Last model \\n')\n",
    "                labels, probs = to_one_label(nn_model,label_list,X_test,index_list)\n",
    "                performance (nn_model, labels, probs)\n",
    "                \n",
    "                #load the best model\n",
    "                best_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "                best_model.load_state_dict(torch.load(PATH1))\n",
    "                print('\\n Best model \\n')\n",
    "                labels, probs = to_one_label(best_model,label_list,X_test,index_list)\n",
    "                performance (best_model, labels, probs)\n",
    "f.close() \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
