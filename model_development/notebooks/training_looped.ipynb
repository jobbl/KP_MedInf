{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Uni\\KP_MedInf\\continuous-aki-predict\\model_development\\notebooks\n",
      "d:\\Uni\\KP_MedInf\\continuous-aki-predict\\model_development\n"
     ]
    }
   ],
   "source": [
    "# only for taurus\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# os.chdir(\"/home/jori152b/DIR/horse/jori152b-medinf/KP_MedInf/model_development\")\n",
    "os.chdir(\"../\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-16 14:50:17.615121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "#data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "# NN\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score, roc_auc_score, auc, accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from captum.attr import IntegratedGradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Training on CPU') # On mac book GPU is not possible =() \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter as constant \n",
    "\n",
    "TESTING = False \n",
    "TEST_SIZE = 0.05\n",
    "\n",
    "SPLIT_SIZE = 0.2 \n",
    "\n",
    "NORMALIZATION = 'min-max' \n",
    "\n",
    "CAPPING_THRESHOLD_UPPER = 0.99\n",
    "CAPPING_THRESHOLD_LOWER = 0.01\n",
    "\n",
    "# How much time the prediction should occur (hours)\n",
    "HOURS_AHEAD = 48\n",
    "\n",
    "NORM_TYPE = 'min_max'\n",
    "\n",
    "RANDOM = 42\n",
    "\n",
    "# LSTM\n",
    "batch_size = 5\n",
    "\n",
    "# naming model and plot\n",
    "classifier_name = \"None vs. Any AKI\"    ###change every time #Moderate vs. Severe #None vs. Any #Others vs. Severe\n",
    "plot_name = \"adult_AnyAKI_LR\"    ###change every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions used later\n",
    "\n",
    "def cap_data(df):\n",
    "    print(\"Capping between the {} and {} quantile\".format(CAPPING_THRESHOLD_LOWER, CAPPING_THRESHOLD_UPPER))\n",
    "    cap_mask = df.columns.difference(['icustay_id', 'charttime', 'aki_stage'])\n",
    "    df[cap_mask] = df[cap_mask].clip(df[cap_mask].quantile(CAPPING_THRESHOLD_LOWER),\n",
    "                                     df[cap_mask].quantile(CAPPING_THRESHOLD_UPPER),\n",
    "                                     axis=1)\n",
    "\n",
    "    return df\n",
    " \n",
    "    \n",
    "def normalise_data(df, norm_mask):\n",
    "    print(\"Normalizing in [0,1] with {} normalization\".format(NORMALIZATION))\n",
    "    \n",
    "    min_values = df[norm_mask].min()\n",
    "    max_values = df[norm_mask].max()\n",
    "    \n",
    "    # Skip normalization for constant columns\n",
    "    for column in norm_mask:\n",
    "        if min_values[column] != max_values[column]:\n",
    "            df[column] = (df[column] - min_values[column]) / (max_values[column] - min_values[column])\n",
    "    \n",
    "    normalization_parameters = {column: {'min': min_values[column], 'max': max_values[column]} for column in norm_mask}\n",
    "    \n",
    "    return df, normalization_parameters\n",
    "\n",
    "\n",
    "# impute missing value in resampleing data with most common based on each id\n",
    "def fast_mode(df, key_cols, value_col):\n",
    "    \"\"\" Calculate a column mode, by group, ignoring null values. \n",
    "    \n",
    "    key_cols : list of str - Columns to groupby for calculation of mode.\n",
    "    value_col : str - Column for which to calculate the mode. \n",
    "\n",
    "    Return\n",
    "    pandas.DataFrame\n",
    "        One row for the mode of value_col per key_cols group. If ties, returns the one which is sorted first. \"\"\"\n",
    "    return (df.groupby(key_cols + [value_col]).size() \n",
    "              .to_frame('counts').reset_index() \n",
    "              .sort_values('counts', ascending=False) \n",
    "              .drop_duplicates(subset=key_cols)).drop('counts',axis=1)\n",
    "\n",
    "\n",
    "#get max shape of 3d array\n",
    "def get_dimensions(array, level=0):   \n",
    "    yield level, len(array)\n",
    "    try:\n",
    "        for row in array:\n",
    "            yield from get_dimensions(row, level + 1)\n",
    "    except TypeError: #not an iterable\n",
    "        pass\n",
    "\n",
    "def get_max_shape(array):\n",
    "    dimensions = defaultdict(int)\n",
    "    for level, length in get_dimensions(array):\n",
    "        dimensions[level] = max(dimensions[level], length)\n",
    "    return [value for _, value in sorted(dimensions.items())]\n",
    "\n",
    "#pad the ragged 3d array to rectangular shape based on max size\n",
    "def iterate_nested_array(array, index=()):\n",
    "    try:\n",
    "        for idx, row in enumerate(array):\n",
    "            yield from iterate_nested_array(row, (*index, idx)) \n",
    "    except TypeError: # final level            \n",
    "        yield (*index, slice(len(array))), array # think of the types\n",
    "\n",
    "def pad(array, fill_value):\n",
    "    dimensions = get_max_shape(array)\n",
    "    result = np.full(dimensions, fill_value, dtype = np.float64)  \n",
    "    for index, value in iterate_nested_array(array):\n",
    "        result[index] = value \n",
    "    return result\n",
    "\n",
    "def bin_total(y_true, y_prob, n_bins):\n",
    "    bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "    # In sklearn.calibration.calibration_curve,\n",
    "    # the last value in the array is always 0.\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "def missing_bin(bin_array):\n",
    "    midpoint = \" \"    \n",
    "    if bin_array[0]==0:\n",
    "        midpoint = \"5%, \"\n",
    "    if bin_array[1]==0:\n",
    "        midpoint = midpoint + \"15%, \"\n",
    "    if bin_array[2]==0:\n",
    "        midpoint = midpoint + \"25%, \"\n",
    "    if bin_array[3]==0:\n",
    "        midpoint = midpoint + \"35%, \" \n",
    "    if bin_array[4]==0:\n",
    "        midpoint = midpoint + \"45%, \"\n",
    "    if bin_array[5]==0:\n",
    "        midpoint = midpoint + \"55%, \"\n",
    "    if bin_array[6]==0:\n",
    "        midpoint = midpoint + \"65%, \"\n",
    "    if bin_array[7]==0:\n",
    "        midpoint = midpoint + \"75%, \"\n",
    "    if bin_array[8]==0:\n",
    "        midpoint = midpoint + \"85%, \"\n",
    "    if bin_array[9]==0:\n",
    "        midpoint = midpoint + \"95%, \"\n",
    "    return \"The missing bins have midpoint values of \"+ str(midpoint)\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    times = math.floor(data.shape[0]/batch_size)\n",
    "    remainder = data.shape[0]%times\n",
    "    a = 0\n",
    "    start = 0\n",
    "    end = start+batch_size\n",
    "    if remainder ==0:\n",
    "        a +=1\n",
    "    while a<times:\n",
    "        temp = pad(data[start:end,],0)\n",
    "        x = torch.from_numpy(temp[:,:,1:-1]).float() # without icustay_id and without aki_stage columns\n",
    "        y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "        X_batches.append(x)\n",
    "        y_batches.append(y)\n",
    "        start = end\n",
    "        end = start+batch_size\n",
    "        a +=1\n",
    "    temp = pad(data[start:data.shape[0]],0)\n",
    "    x = torch.from_numpy(temp[:,:,1:-1]).float()\n",
    "    y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "    X_batches.append(x)\n",
    "    y_batches.append(y)\n",
    "    if len(X_batches) != len(y_batches):\n",
    "        print(\"length error\")\n",
    "    return X_batches, y_batches # arrays\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size \n",
    "        self.output_size = output_size\n",
    "        self.number_layers = number_layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "        # in bidirectional encoder we have  forward and backward hidden states\n",
    "        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "        # Create affine layer to project to the classes \n",
    "        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "        #dropout layer for regularizetion of a sequence\n",
    "        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "        h = self.relu(self.combination_layer(h))\n",
    "        h = self.dropout_layer(h)\n",
    "        h = self.projection(h) \n",
    "        return h\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uo_rt_6hr', 'vasopressor', 'meanbp_mean', 'creat', 'uo_rt_12hr', 'creatinine_mean', 'sedative', 'aniongap_mean', 'uo_rt_24hr', 'calcium', 'glucose_mean_y', 'vent', 'sysbp_mean', 'hadm_id', 'heartrate_mean', 'tempc_mean', 'spo2_mean', 'potassium_mean', 'diasbp_mean', 'platelet_mean', 'wbc_mean', 'resprate_mean', 'ptt_mean', 'sodium_mean', 'hemoglobin_mean', 'aki_stage', 'icustay_id', 'charttime']\n"
     ]
    }
   ],
   "source": [
    "# optional: load best features\n",
    "optimal_features = np.load(\"data/optimal_features.npy\", allow_pickle=True)\n",
    "# Extracting feature names (keys) from optimal_features\n",
    "optimal_feature_names = [feature[0] for feature in optimal_features]\n",
    "# include also aki_stage and icustay_id\n",
    "optimal_feature_names.extend(['aki_stage', 'icustay_id', 'charttime'])\n",
    "print(optimal_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original = pd.read_csv(\"data/preprocessed/preprocessed_data_6H.csv\")\n",
    "extended = pd.read_csv(\"data/preprocessed/preprocessed_data_extended_1H.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# print(len(original.columns))\n",
    "print(len(extended.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing in [0,1] with min-max normalization\n",
      "33\n",
      "Index(['icustay_id', 'charttime', 'aniongap_mean', 'bicarbonate_mean',\n",
      "       'bun_mean', 'calcium', 'chloride_mean', 'creat', 'creatinine_mean',\n",
      "       'diasbp_mean', 'glucose_mean_x', 'glucose_mean_y', 'heartrate_mean',\n",
      "       'hematocrit_mean', 'hemoglobin_mean', 'meanbp_mean', 'potassium_mean',\n",
      "       'resprate_mean', 'sodium_mean', 'spo2_mean', 'sysbp_mean', 'tempc_mean',\n",
      "       'uo_rt_12hr', 'uo_rt_24hr', 'uo_rt_6hr', 'wbc_mean', 'sedative',\n",
      "       'vasopressor', 'vent', 'aki_stage', 'admission_age', 'gender_F',\n",
      "       'gender_M'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_25944\\3108228712.py:85: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_25944\\3108228712.py:86: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_25944\\3108228712.py:87: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for preprocessed_data_6H.csv\n",
      "Train accuracy: 0.866.. Train ROC AUC: 0.76.. Train PR AUC: 0.46..\n",
      "Test accuracy: 0.857.. Test ROC AUC: 0.74.. Test PR AUC: 0.41..\n",
      "Validation accuracy: 0.862.. Validation ROC AUC: 0.74.. Validation PR AUC: 0.42..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derqu\\miniconda3\\envs\\medinf\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [19:46:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# normal\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import os\n",
    "import gc\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "data_paths = [\n",
    "    # \"data/preprocessed/preprocessed_data_1H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_2H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_4H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_6H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_8H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_12H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_24H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data.csv\",\n",
    "              ]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    \n",
    "    tail = data_path.split(\"/\")[-1]\n",
    "    X = pd.read_csv(data_path)\n",
    "    # take only head \n",
    "    # X = X.head(10000)\n",
    "\n",
    "    # For training a testing model, take only icu_stay_id, charttime,creatinine_mean,uo_rt_6hr,aki_stage\n",
    "    # X = X[['icustay_id', 'charttime', 'creatinine_mean', 'uo_rt_6hr', 'aki_stage']]\n",
    "\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage',)\n",
    "    numeric_feat.remove('icustay_id',)\n",
    "\n",
    "\n",
    "    # normalize data and cap features\n",
    "    # X = cap_data(X)\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    \n",
    "    print(len(X.columns))\n",
    "    print(X.columns)\n",
    "\n",
    "    # X = X.sort_values(by=['icustay_id', 'charttime'])\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    seq_lengths = X.groupby(['icustay_id'],as_index=False).size().sort_values(by = ['size'],ascending=False)\n",
    "    sequence_length = seq_lengths.max() # the longest sequence per icustay-id\n",
    "\n",
    "    #AL re-write as try except to make it work as hadm_id is not used if only one csv file is used and none are merged\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    id_list = X['icustay_id'].unique()\n",
    "    # take the common id list defined earlier\n",
    "    # id_list = common_id_list\n",
    "    \n",
    "    id_train, id_test_val = train_test_split(id_list, test_size = SPLIT_SIZE, random_state = 42) # train set is 80%)\n",
    "    # remaining 20% split in halves as test and validation 10% and 10%\n",
    "    id_valid, id_test = train_test_split(id_test_val, test_size = 0.5, random_state = 42) # test 10% valid 10%\n",
    "\n",
    "    # move (\"aki_stage\") to last column\n",
    "    X = X.reindex(columns = [col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "    test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index = True) \n",
    "    validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id']) \n",
    "\n",
    "    test = test.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "    train = train.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "    validation = validation.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "\n",
    "    train.drop(['charttime'], axis=1, inplace = True)  \n",
    "    test.drop(['charttime'], axis=1, inplace = True)\n",
    "    validation.drop(['charttime'], axis=1, inplace = True)\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "\n",
    "\n",
    "    # flatten the train, test and validation data\n",
    "    train_flat = np.concatenate(train, axis=0)\n",
    "    test_flat = np.concatenate(test, axis=0)\n",
    "    validation_flat = np.concatenate(validation, axis=0)\n",
    "\n",
    "    # get the labels\n",
    "    train_labels = np.array([x[-1] for x in train_flat])\n",
    "    test_labels = np.array([x[-1] for x in test_flat])\n",
    "    validation_labels = np.array([x[-1] for x in validation_flat])\n",
    "\n",
    "    # get the features\n",
    "    train_features = np.array([x[1:-1] for x in train_flat])\n",
    "    validation_features = np.array([x[1:-1] for x in validation_flat])\n",
    "    test_features = np.array([x[1:-1] for x in test_flat])\n",
    "\n",
    "    # create the XGBoost classifier\n",
    "    xgb = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM)\n",
    "\n",
    "    # train the classifier\n",
    "    xgb.fit(train_features, train_labels)\n",
    "\n",
    "    # get the predictions\n",
    "    train_predictions = xgb.predict(train_features)\n",
    "    test_predictions = xgb.predict(test_features)\n",
    "    validation_predictions = xgb.predict(validation_features)\n",
    "\n",
    "    # get the accuracy\n",
    "    train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "    test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    validation_accuracy = accuracy_score(validation_labels, validation_predictions)\n",
    "\n",
    "    # get the probabilities of the positive class\n",
    "    training_prob = xgb.predict_proba([x[1:-1] for x in train_flat])[:, 1]\n",
    "    test_prob = xgb.predict_proba([x[1:-1] for x in test_flat])[:, 1]\n",
    "    validation_prob = xgb.predict_proba([x[1:-1] for x in validation_flat])[:, 1]\n",
    "\n",
    "    # calculate ROC AUC and PR AUC for the training set\n",
    "    training_roc_auc = roc_auc_score(train_labels, training_prob)\n",
    "    training_pr_auc = average_precision_score(train_labels, training_prob)\n",
    "\n",
    "    # calculate ROC AUC and PR AUC for the test set\n",
    "    test_roc_auc = roc_auc_score(test_labels, test_prob)\n",
    "    test_pr_auc = average_precision_score(test_labels, test_prob)\n",
    "\n",
    "    # calculate ROC AUC and PR AUC for the validation set\n",
    "    validation_roc_auc = roc_auc_score(validation_labels, validation_prob)\n",
    "    validation_pr_auc = average_precision_score(validation_labels, validation_prob)\n",
    "    \n",
    "    print(f\"Results for {tail}\")\n",
    "    print(f\"Train accuracy: {train_accuracy:.3f}.. Train ROC AUC: {training_roc_auc:.2f}.. Train PR AUC: {training_pr_auc:.2f}..\")\n",
    "    print(f\"Test accuracy: {test_accuracy:.3f}.. Test ROC AUC: {test_roc_auc:.2f}.. Test PR AUC: {test_pr_auc:.2f}..\")\n",
    "    print(f\"Validation accuracy: {validation_accuracy:.3f}.. Validation ROC AUC: {validation_roc_auc:.2f}.. Validation PR AUC: {validation_pr_auc:.2f}..\")\n",
    "\n",
    "    now = datetime.now()\n",
    "    out_path = f'data/models/{tail}'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    # save the xgb model\n",
    "    xgb.save_model(f'{out_path}/xgb.model')\n",
    "    # save normalization parameters\n",
    "    try:\n",
    "        np.save(f'{out_path}/normalization_parameters.npy', normalization_parameters)\n",
    "    except:\n",
    "        pass\n",
    "    # save the train feature names\n",
    "    np.save(f'{out_path}/train_feature_names.npy', X.columns[2:-1])\n",
    "\n",
    "    results[tail] = {'train_accuracy': train_accuracy, 'test_accuracy': test_accuracy, 'validation_accuracy': validation_accuracy,\n",
    "                                'train_roc_auc': training_roc_auc, 'test_roc_auc': test_roc_auc, 'validation_roc_auc': validation_roc_auc,\n",
    "                                'train_pr_auc': training_pr_auc, 'test_pr_auc': test_pr_auc, 'validation_pr_auc': validation_pr_auc}\n",
    "\n",
    "    # save results dict\n",
    "    np.save(f'{out_path}/results.npy', results)\n",
    "    \n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = [\n",
    "    \"data/preprocessed/preprocessed_data_1H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_2H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_4H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_6H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_8H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_12H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_24H.csv\",\n",
    "]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    X = pd.read_csv(data_path)\n",
    "    X.drop(['height_first', 'hadm_id', 'weight_first', 'inr_max'], axis=1, inplace = True)\n",
    "    # write back to the same file\n",
    "    X.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "results = {}\n",
    "\n",
    "data_paths = [\n",
    "    \"../data/preprocessed/preprocessed_data_extended_1H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_2H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_3H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_4H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_5H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_6H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data.csv\",\n",
    "]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    tail = data_path.split(\"/\")[-1]\n",
    "    X = pd.read_csv(data_path)\n",
    "\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage')\n",
    "    numeric_feat.remove('icustay_id')\n",
    "\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # id_list = X['icustay_id'].unique()\n",
    "    id_list = common_id_list\n",
    "\n",
    "    common_id_list.sort()\n",
    "    print(common_id_list[:10])\n",
    "\n",
    "    print(len(id_list))\n",
    "\n",
    "    # Move \"aki_stage\" to last column\n",
    "    X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    X.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "    # Group by icustay_id and convert to numpy arrays\n",
    "    grouped_data = X.groupby('icustay_id').apply(lambda x: x.drop('icustay_id', axis=1).to_numpy())\n",
    "\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_results = []\n",
    "\n",
    "    print(len(X))\n",
    "\n",
    "    for fold, (id_train_idx, id_val_idx) in enumerate(kf.split(id_list), 1):\n",
    "        print(f\"Processing fold {fold}\")\n",
    "        id_train = [id_list[idx] for idx in id_train_idx]\n",
    "        id_val = [id_list[idx] for idx in id_val_idx]       \n",
    "\n",
    "        print(len(id_train), len(id_val))\n",
    "        print(id_train[:10], id_val[:10])\n",
    "        train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "        validation = X[X.icustay_id.isin(id_val)].sort_values(by=['icustay_id']) \n",
    "\n",
    "        print(len(train), len(validation))\n",
    "\n",
    "        train = train.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "        validation = validation.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "\n",
    "        try:\n",
    "            X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(len(train), len(validation))\n",
    "\n",
    "        train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "        validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "\n",
    "        print(len(train), len(validation))\n",
    "\n",
    "        # flatten the train, test and validation data\n",
    "        train_flat = np.concatenate(train, axis=0)\n",
    "        validation_flat = np.concatenate(validation, axis=0)\n",
    "\n",
    "        # get the labels\n",
    "        train_labels = np.array([x[-1] for x in train_flat])\n",
    "        val_labels = np.array([x[-1] for x in validation_flat])\n",
    "\n",
    "        # get the features\n",
    "        train_features = np.array([x[1:-1] for x in train_flat])\n",
    "        val_features = np.array([x[1:-1] for x in validation_flat])\n",
    "\n",
    "        # Create and train the XGBoost classifier\n",
    "        xgb = XGBClassifier(n_estimators=1000, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "        xgb.fit(train_features, train_labels)\n",
    "\n",
    "        # Make predictions\n",
    "        train_predictions = xgb.predict(train_features)\n",
    "        val_predictions = xgb.predict(val_features)\n",
    "\n",
    "        # unique values in the labels\n",
    "        unique_labels = np.unique(np.concatenate([train_labels, val_labels]))\n",
    "        print(f\"Unique labels: {unique_labels}\")\n",
    "        unique_labels_pred = np.unique(np.concatenate([train_predictions, val_predictions]))\n",
    "        print(f\"Unique predictions: {unique_labels_pred}\")\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "        val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        \n",
    "\n",
    "        # Calculate probabilities\n",
    "        train_prob = xgb.predict_proba(train_features)[:, 1]\n",
    "        val_prob = xgb.predict_proba(val_features)[:, 1]\n",
    "\n",
    "        # Calculate ROC AUC and PR AUC\n",
    "        train_roc_auc = roc_auc_score(train_labels, train_prob)\n",
    "        train_pr_auc = average_precision_score(train_labels, train_prob)\n",
    "        val_roc_auc = roc_auc_score(val_labels, val_prob)\n",
    "        val_pr_auc = average_precision_score(val_labels, val_prob)\n",
    "\n",
    "        fold_results.append({\n",
    "            'fold': fold,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'train_roc_auc': train_roc_auc,\n",
    "            'val_roc_auc': val_roc_auc,\n",
    "            'train_pr_auc': train_pr_auc,\n",
    "            'val_pr_auc': val_pr_auc,\n",
    "        })\n",
    "\n",
    "        print(f\"Fold {fold} results:\")\n",
    "        print(f\"Train accuracy: {train_accuracy:.3f}, ROC AUC: {train_roc_auc:.2f}, PR AUC: {train_pr_auc:.2f}\")\n",
    "        print(f\"Validation accuracy: {val_accuracy:.3f}, ROC AUC: {val_roc_auc:.2f}, PR AUC: {val_pr_auc:.2f}\")\n",
    "\n",
    "    # Calculate average scores across folds\n",
    "    avg_scores = {\n",
    "        'train_accuracy': np.mean([r['train_accuracy'] for r in fold_results]),\n",
    "        'val_accuracy': np.mean([r['val_accuracy'] for r in fold_results]),\n",
    "        'train_roc_auc': np.mean([r['train_roc_auc'] for r in fold_results]),\n",
    "        'val_roc_auc': np.mean([r['val_roc_auc'] for r in fold_results]),\n",
    "        'train_pr_auc': np.mean([r['train_pr_auc'] for r in fold_results]),\n",
    "        'val_pr_auc': np.mean([r['val_pr_auc'] for r in fold_results]),\n",
    "    }\n",
    "\n",
    "    print(\"\\nAverage scores across 5 folds:\")\n",
    "    for metric, value in avg_scores.items():\n",
    "        print(f\"{metric}: {value:.3f}\")\n",
    "\n",
    "    results[tail] = {\n",
    "        'fold_results': fold_results,\n",
    "        'average_scores': avg_scores\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    np.save(f'data/results_{tail}.npy', results[tail])\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "# Save overall results\n",
    "np.save('data/comparison_time_bins_results_cross_validated.npy', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and predict    \n",
    "loaded_model = XGBClassifier()\n",
    "loaded_model.load_model('data/models/simple_xgboost_model.model')\n",
    "test_predictions = loaded_model.predict(np.array([x[1:-1] for x in test_flat]))\n",
    "validation_predictions = loaded_model.predict(np.array([x[1:-1] for x in validation_flat]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n",
      "Processing preprocessed_data_6H.csv\n",
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_25440\\3894920638.py:102: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = np.array(sorted(list(train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)),key=len, reverse=True))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (210,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 102\u001b[0m\n\u001b[0;32m     96\u001b[0m validation\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharttime\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(train\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m'\u001b[39m],as_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mto_numpy)),key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    104\u001b[0m test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mlist\u001b[39m(test\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124micustay_id\u001b[39m\u001b[38;5;124m'\u001b[39m],as_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mto_numpy)))\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (210,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import collections\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "results = {}\n",
    "\n",
    "data_paths = [\n",
    "    # \"data/preprocessed/preprocessed_data_4H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_6H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_8H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_4H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_6H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_8H.csv\",\n",
    "]\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    times = math.floor(data.shape[0]/batch_size)\n",
    "    remainder = data.shape[0]%times\n",
    "    a = 0\n",
    "    start = 0\n",
    "    end = start+batch_size\n",
    "    if remainder ==0:\n",
    "        a +=1\n",
    "    while a<times:\n",
    "        temp = pad(data[start:end,],0)\n",
    "        x = torch.from_numpy(temp[:,:,1:-1]).float() # without icustay_id and without aki_stage columns\n",
    "        y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "        X_batches.append(x)\n",
    "        y_batches.append(y)\n",
    "        start = end\n",
    "        end = start+batch_size\n",
    "        a +=1\n",
    "    temp = pad(data[start:data.shape[0]],0)\n",
    "    x = torch.from_numpy(temp[:,:,1:-1]).float()\n",
    "    y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "    X_batches.append(x)\n",
    "    y_batches.append(y)\n",
    "    if len(X_batches) != len(y_batches):\n",
    "        print(\"length error\")\n",
    "    return X_batches, y_batches # arrays\n",
    "\n",
    "for data_path in data_paths:\n",
    "    tail = data_path.split(\"/\")[-1]\n",
    "    print(f\"Processing {tail}\")\n",
    "\n",
    "    # ... (keep the data loading and preprocessing steps as they are)\n",
    "    X = pd.read_csv(data_path)\n",
    "    # only take the first 10000 rows\n",
    "    X = X.head(10000) \n",
    "\n",
    "    # Preprocessing steps (similar to XGBoost)\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage')\n",
    "    numeric_feat.remove('icustay_id')\n",
    "\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Split data (you may want to use the same splitting logic as in XGBoost)\n",
    "    id_list = X['icustay_id'].unique()\n",
    "    # id_list = common_id_list\n",
    "    id_train, id_test_val = train_test_split(id_list, test_size=SPLIT_SIZE, random_state=42)\n",
    "    id_valid, id_test = train_test_split(id_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "    X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "    test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index=True)\n",
    "    validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id'])\n",
    "    \n",
    "    test = test.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "    train = train.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "    validation = validation.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "\n",
    "    train.drop(['charttime'], axis=1, inplace=True)\n",
    "    test.drop(['charttime'], axis=1, inplace=True)\n",
    "    validation.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "    # train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    # test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    # validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    \n",
    "    train = np.array(sorted(list(train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)),key=len, reverse=True))\n",
    "    print(\"train shape\", train.shape)\n",
    "    test = np.array(list(test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)))\n",
    "    validation = np.array(list(validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)))\n",
    "    \n",
    "    # Prepare data for LSTM\n",
    "    # X_train, y_train = batch(train.to_numpy(), batch_size)\n",
    "    # X_test, y_test = batch(test.to_numpy(), test.shape[0])\n",
    "    # X_val, y_val = batch(validation.to_numpy(), validation.shape[0])\n",
    "    X_train, y_train = batch(train, batch_size)\n",
    "    X_test, y_test = batch(test, batch_size)\n",
    "    X_val, y_val = batch(validation, batch_size)\n",
    "    \n",
    "    print(train.shape[0])\n",
    "\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(f'runs/{tail}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "\n",
    "    # LSTM parameters\n",
    "    input_size = X_train[0].shape[2]\n",
    "    output_size = 1\n",
    "    emb_size = round(input_size / 1)\n",
    "    number_layers = 3\n",
    "    dropout = 0\n",
    "    bi_directional = True\n",
    "\n",
    "    nn_model = Net(input_size, emb_size, output_size, bi_directional, number_layers, dropout).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "    \n",
    "    use_pretrained = False\n",
    "    \n",
    "    best_auc = 0\n",
    "    start_epoch = 0\n",
    "    \n",
    "    if use_pretrained:\n",
    "        model_path = f'data/models/{tail}/LSTM_best.pth'\n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path)\n",
    "            nn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            best_auc = checkpoint.get('best_auc', 0)  # Load best AUC or default to 0 if not found\n",
    "            start_epoch = checkpoint.get('epoch', 0)  # Load last epoch or default to 0 if not found\n",
    "            print(f\"Loaded pretrained model from {model_path} with AUC {best_auc} at epoch {start_epoch}.\")\n",
    "        else:\n",
    "            print(f\"No pretrained model found, starting training from scratch.\")\n",
    "\n",
    "    n_epochs = 200\n",
    "    best_auc = 0\n",
    "\n",
    "    for epoch in range(start_epoch + 1, n_epochs):\n",
    "        nn_model.train()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "\n",
    "        for i, (X_batch, y_batch) in enumerate(zip(X_train, y_train)):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            print(\"Shape X batch, y batch\", X_batch.shape, y_batch.shape)\n",
    "            print(X_batch[1][1][0].shape)\n",
    "            print(X_batch[1][1][0])\n",
    "            outputs = nn_model(X_batch)\n",
    "            print(\"Shape outputs\", outputs.shape)\n",
    "            # print(outputs)\n",
    "            outputs = torch.flatten(outputs)\n",
    "            y_batch = y_batch.type_as(outputs)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            train_accuracy = accuracy_score(y_batch.cpu().numpy(), predicted.cpu().numpy())\n",
    "            running_accuracy += train_accuracy\n",
    "\n",
    "            # Log training metrics to TensorBoard\n",
    "            writer.add_scalar('Training/Loss', loss.item(), epoch * len(X_train) + i)\n",
    "            writer.add_scalar('Training/Accuracy', train_accuracy, epoch * len(X_train) + i)\n",
    "\n",
    "        # Validation\n",
    "        nn_model.eval()\n",
    "        total_v_loss = 0\n",
    "        all_y_val = []\n",
    "        all_val_prob = []\n",
    "        all_accuracy = 0\n",
    "\n",
    "        for X_val_batch, y_val_batch in zip(X_val, y_val):\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                v_out = nn_model(X_val_batch)\n",
    "                v_out = torch.flatten(v_out)\n",
    "                y_val_batch = y_val_batch.type_as(v_out)\n",
    "                v_loss = criterion(v_out, y_val_batch)\n",
    "                val_prob = torch.sigmoid(v_out)\n",
    "                total_v_loss += v_loss.item()\n",
    "                all_y_val.extend(y_val_batch.cpu().numpy())\n",
    "                all_val_prob.extend(val_prob.cpu().numpy())\n",
    "                \n",
    "                predicted = torch.sigmoid(v_out) > 0.5\n",
    "                val_accuracy = accuracy_score(y_val_batch.cpu().numpy(), predicted.cpu().numpy())\n",
    "                all_accuracy += val_accuracy\n",
    "\n",
    "        avg_v_loss = total_v_loss / len(X_val)\n",
    "        roc_auc = roc_auc_score(all_y_val, all_val_prob)\n",
    "        avg_accuracy = all_accuracy / len(X_val)\n",
    "        \n",
    "        # Log validation metrics to TensorBoard\n",
    "        writer.add_scalar('Validation/Loss', avg_v_loss, epoch)\n",
    "        writer.add_scalar('Validation/AUC', roc_auc, epoch)\n",
    "        writer.add_scalar('Validation/Accuracy', avg_accuracy, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, \"\n",
    "            f\"Train Loss: {running_loss/len(X_train):.4f}, \"\n",
    "            f\"Train Accuracy: {running_accuracy/len(X_train):.4f}, \"\n",
    "            f\"Val Loss: {avg_v_loss:.4f}, \"\n",
    "            f\"Val AUC: {roc_auc:.4f},\"\n",
    "            f\"Val Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "        if roc_auc > best_auc:\n",
    "            best_auc = roc_auc\n",
    "            save_path = f'data/models/{tail}/LSTM_best.pth'\n",
    "            torch.save({\n",
    "                'model_state_dict': nn_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_auc': best_auc,\n",
    "                'epoch': epoch\n",
    "            }, save_path)\n",
    "\n",
    "    # Close the TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n",
      "Processing preprocessed_data_6H.csv\n",
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_25440\\1969946254.py:99: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = list(train.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences in train: 30782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_25440\\1969946254.py:102: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = list(test.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_25440\\1969946254.py:103: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = list(validation.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train: 962\n",
      "Epoch 2/200, Train Loss: 0.4182, Train Accuracy: 0.8285, Val Loss: 0.7201, Val AUC: 0.4049,Val Accuracy: 0.8283\n",
      "Epoch 3/200, Train Loss: 0.4107, Train Accuracy: 0.8289, Val Loss: 0.4539, Val AUC: 0.5860,Val Accuracy: 0.8283\n",
      "Epoch 4/200, Train Loss: 0.3928, Train Accuracy: 0.8307, Val Loss: 0.4969, Val AUC: 0.5414,Val Accuracy: 0.8283\n",
      "Epoch 5/200, Train Loss: 0.3881, Train Accuracy: 0.8323, Val Loss: 0.4840, Val AUC: 0.4944,Val Accuracy: 0.8275\n",
      "Epoch 6/200, Train Loss: 0.3812, Train Accuracy: 0.8351, Val Loss: 0.4589, Val AUC: 0.6373,Val Accuracy: 0.8277\n",
      "Epoch 7/200, Train Loss: 0.3782, Train Accuracy: 0.8372, Val Loss: 0.4591, Val AUC: 0.5621,Val Accuracy: 0.8280\n",
      "Epoch 8/200, Train Loss: 0.3751, Train Accuracy: 0.8390, Val Loss: 0.4617, Val AUC: 0.5594,Val Accuracy: 0.8259\n",
      "Epoch 9/200, Train Loss: 0.3733, Train Accuracy: 0.8383, Val Loss: 0.4408, Val AUC: 0.6452,Val Accuracy: 0.8288\n",
      "Epoch 10/200, Train Loss: 0.3726, Train Accuracy: 0.8395, Val Loss: 0.4229, Val AUC: 0.6845,Val Accuracy: 0.8314\n",
      "Epoch 11/200, Train Loss: 0.3741, Train Accuracy: 0.8387, Val Loss: 0.4443, Val AUC: 0.6610,Val Accuracy: 0.8285\n",
      "Epoch 12/200, Train Loss: 0.3692, Train Accuracy: 0.8411, Val Loss: 0.4261, Val AUC: 0.7050,Val Accuracy: 0.8285\n",
      "Epoch 13/200, Train Loss: 0.3683, Train Accuracy: 0.8427, Val Loss: 0.4343, Val AUC: 0.6824,Val Accuracy: 0.8285\n",
      "Epoch 14/200, Train Loss: 0.3668, Train Accuracy: 0.8432, Val Loss: 0.4306, Val AUC: 0.7041,Val Accuracy: 0.8283\n",
      "Epoch 15/200, Train Loss: 0.3728, Train Accuracy: 0.8408, Val Loss: 0.4675, Val AUC: 0.7231,Val Accuracy: 0.8283\n",
      "Epoch 16/200, Train Loss: 0.3675, Train Accuracy: 0.8429, Val Loss: 0.4534, Val AUC: 0.7156,Val Accuracy: 0.8280\n",
      "Epoch 17/200, Train Loss: 0.3660, Train Accuracy: 0.8436, Val Loss: 0.4484, Val AUC: 0.7263,Val Accuracy: 0.8283\n",
      "Epoch 18/200, Train Loss: 0.3643, Train Accuracy: 0.8447, Val Loss: 0.4635, Val AUC: 0.7171,Val Accuracy: 0.8290\n",
      "Epoch 19/200, Train Loss: 0.3634, Train Accuracy: 0.8447, Val Loss: 0.4649, Val AUC: 0.7228,Val Accuracy: 0.8290\n",
      "Epoch 20/200, Train Loss: 0.3621, Train Accuracy: 0.8458, Val Loss: 0.4790, Val AUC: 0.7172,Val Accuracy: 0.8288\n",
      "Epoch 21/200, Train Loss: 0.3609, Train Accuracy: 0.8473, Val Loss: 0.4679, Val AUC: 0.7170,Val Accuracy: 0.8290\n",
      "Epoch 22/200, Train Loss: 0.3599, Train Accuracy: 0.8475, Val Loss: 0.4671, Val AUC: 0.7240,Val Accuracy: 0.8290\n",
      "Epoch 23/200, Train Loss: 0.3585, Train Accuracy: 0.8478, Val Loss: 0.4457, Val AUC: 0.7266,Val Accuracy: 0.8288\n",
      "Epoch 24/200, Train Loss: 0.3562, Train Accuracy: 0.8487, Val Loss: 0.4622, Val AUC: 0.7232,Val Accuracy: 0.8290\n",
      "Epoch 25/200, Train Loss: 0.3551, Train Accuracy: 0.8499, Val Loss: 0.4659, Val AUC: 0.6996,Val Accuracy: 0.8295\n",
      "Epoch 26/200, Train Loss: 0.3533, Train Accuracy: 0.8498, Val Loss: 0.4409, Val AUC: 0.7219,Val Accuracy: 0.8303\n",
      "Epoch 27/200, Train Loss: 0.3512, Train Accuracy: 0.8504, Val Loss: 0.4377, Val AUC: 0.7181,Val Accuracy: 0.8324\n",
      "Epoch 28/200, Train Loss: 0.3501, Train Accuracy: 0.8517, Val Loss: 0.4479, Val AUC: 0.7140,Val Accuracy: 0.8308\n",
      "Epoch 29/200, Train Loss: 0.3485, Train Accuracy: 0.8526, Val Loss: 0.4345, Val AUC: 0.7319,Val Accuracy: 0.8329\n",
      "Epoch 30/200, Train Loss: 0.3480, Train Accuracy: 0.8534, Val Loss: 0.4203, Val AUC: 0.7194,Val Accuracy: 0.8329\n",
      "Epoch 31/200, Train Loss: 0.3456, Train Accuracy: 0.8541, Val Loss: 0.4442, Val AUC: 0.7330,Val Accuracy: 0.8339\n",
      "Epoch 32/200, Train Loss: 0.3440, Train Accuracy: 0.8546, Val Loss: 0.4446, Val AUC: 0.7364,Val Accuracy: 0.8355\n",
      "Epoch 33/200, Train Loss: 0.3402, Train Accuracy: 0.8570, Val Loss: 0.4533, Val AUC: 0.7275,Val Accuracy: 0.8360\n",
      "Epoch 34/200, Train Loss: 0.3370, Train Accuracy: 0.8580, Val Loss: 0.4401, Val AUC: 0.7321,Val Accuracy: 0.8342\n",
      "Epoch 35/200, Train Loss: 0.3362, Train Accuracy: 0.8591, Val Loss: 0.4508, Val AUC: 0.7318,Val Accuracy: 0.8376\n",
      "Epoch 36/200, Train Loss: 0.3313, Train Accuracy: 0.8602, Val Loss: 0.4544, Val AUC: 0.7291,Val Accuracy: 0.8352\n",
      "Epoch 37/200, Train Loss: 0.3300, Train Accuracy: 0.8605, Val Loss: 0.4704, Val AUC: 0.7169,Val Accuracy: 0.8352\n",
      "Epoch 38/200, Train Loss: 0.3280, Train Accuracy: 0.8629, Val Loss: 0.4749, Val AUC: 0.7182,Val Accuracy: 0.8376\n",
      "Epoch 39/200, Train Loss: 0.3237, Train Accuracy: 0.8644, Val Loss: 0.4665, Val AUC: 0.7237,Val Accuracy: 0.8342\n",
      "Epoch 40/200, Train Loss: 0.3224, Train Accuracy: 0.8657, Val Loss: 0.4872, Val AUC: 0.7143,Val Accuracy: 0.8306\n",
      "Epoch 41/200, Train Loss: 0.3210, Train Accuracy: 0.8655, Val Loss: 0.4744, Val AUC: 0.7140,Val Accuracy: 0.8298\n",
      "Epoch 42/200, Train Loss: 0.3172, Train Accuracy: 0.8676, Val Loss: 0.5049, Val AUC: 0.7104,Val Accuracy: 0.8306\n",
      "Epoch 43/200, Train Loss: 0.3165, Train Accuracy: 0.8674, Val Loss: 0.5183, Val AUC: 0.7026,Val Accuracy: 0.8285\n",
      "Epoch 44/200, Train Loss: 0.3098, Train Accuracy: 0.8696, Val Loss: 0.4958, Val AUC: 0.7093,Val Accuracy: 0.8339\n",
      "Epoch 45/200, Train Loss: 0.3079, Train Accuracy: 0.8720, Val Loss: 0.5235, Val AUC: 0.7048,Val Accuracy: 0.8319\n",
      "Epoch 46/200, Train Loss: 0.3008, Train Accuracy: 0.8750, Val Loss: 0.5445, Val AUC: 0.6953,Val Accuracy: 0.8290\n",
      "Epoch 47/200, Train Loss: 0.3002, Train Accuracy: 0.8751, Val Loss: 0.5600, Val AUC: 0.6987,Val Accuracy: 0.8324\n",
      "Epoch 48/200, Train Loss: 0.2970, Train Accuracy: 0.8768, Val Loss: 0.5669, Val AUC: 0.6940,Val Accuracy: 0.8301\n",
      "Epoch 49/200, Train Loss: 0.2913, Train Accuracy: 0.8786, Val Loss: 0.5856, Val AUC: 0.6941,Val Accuracy: 0.8293\n",
      "Epoch 50/200, Train Loss: 0.2966, Train Accuracy: 0.8747, Val Loss: 0.5597, Val AUC: 0.6970,Val Accuracy: 0.8314\n",
      "Epoch 51/200, Train Loss: 0.2871, Train Accuracy: 0.8811, Val Loss: 0.5465, Val AUC: 0.6997,Val Accuracy: 0.8290\n",
      "Epoch 52/200, Train Loss: 0.2839, Train Accuracy: 0.8812, Val Loss: 0.5373, Val AUC: 0.6943,Val Accuracy: 0.8365\n",
      "Epoch 53/200, Train Loss: 0.2773, Train Accuracy: 0.8843, Val Loss: 0.5533, Val AUC: 0.7055,Val Accuracy: 0.8355\n",
      "Epoch 54/200, Train Loss: 0.2704, Train Accuracy: 0.8882, Val Loss: 0.5322, Val AUC: 0.7121,Val Accuracy: 0.8337\n",
      "Epoch 55/200, Train Loss: 0.2711, Train Accuracy: 0.8873, Val Loss: 0.6009, Val AUC: 0.6974,Val Accuracy: 0.8308\n",
      "Epoch 56/200, Train Loss: 0.2663, Train Accuracy: 0.8901, Val Loss: 0.6434, Val AUC: 0.6894,Val Accuracy: 0.8267\n",
      "Epoch 57/200, Train Loss: 0.2613, Train Accuracy: 0.8918, Val Loss: 0.5842, Val AUC: 0.7109,Val Accuracy: 0.8303\n",
      "Epoch 58/200, Train Loss: 0.2564, Train Accuracy: 0.8945, Val Loss: 0.6690, Val AUC: 0.6919,Val Accuracy: 0.8293\n",
      "Epoch 59/200, Train Loss: 0.2591, Train Accuracy: 0.8940, Val Loss: 0.6925, Val AUC: 0.6794,Val Accuracy: 0.8298\n",
      "Epoch 60/200, Train Loss: 0.2480, Train Accuracy: 0.8985, Val Loss: 0.6321, Val AUC: 0.6957,Val Accuracy: 0.8272\n",
      "Epoch 61/200, Train Loss: 0.2440, Train Accuracy: 0.9003, Val Loss: 0.6793, Val AUC: 0.6902,Val Accuracy: 0.8249\n",
      "Epoch 62/200, Train Loss: 0.2567, Train Accuracy: 0.8946, Val Loss: 0.6573, Val AUC: 0.6914,Val Accuracy: 0.8326\n",
      "Epoch 63/200, Train Loss: 0.2402, Train Accuracy: 0.9022, Val Loss: 0.6963, Val AUC: 0.6892,Val Accuracy: 0.8277\n",
      "Epoch 64/200, Train Loss: 0.2342, Train Accuracy: 0.9043, Val Loss: 0.7182, Val AUC: 0.6871,Val Accuracy: 0.8272\n",
      "Epoch 65/200, Train Loss: 0.2266, Train Accuracy: 0.9085, Val Loss: 0.7077, Val AUC: 0.6935,Val Accuracy: 0.8283\n",
      "Epoch 66/200, Train Loss: 0.2288, Train Accuracy: 0.9063, Val Loss: 0.6673, Val AUC: 0.7078,Val Accuracy: 0.8298\n",
      "Epoch 67/200, Train Loss: 0.2237, Train Accuracy: 0.9100, Val Loss: 0.7131, Val AUC: 0.6942,Val Accuracy: 0.8314\n",
      "Epoch 68/200, Train Loss: 0.2139, Train Accuracy: 0.9136, Val Loss: 0.7756, Val AUC: 0.6882,Val Accuracy: 0.8262\n",
      "Epoch 69/200, Train Loss: 0.2108, Train Accuracy: 0.9152, Val Loss: 0.7177, Val AUC: 0.6956,Val Accuracy: 0.8233\n",
      "Epoch 70/200, Train Loss: 0.2096, Train Accuracy: 0.9162, Val Loss: 0.7832, Val AUC: 0.6863,Val Accuracy: 0.8210\n",
      "Epoch 71/200, Train Loss: 0.1964, Train Accuracy: 0.9201, Val Loss: 0.8116, Val AUC: 0.6890,Val Accuracy: 0.8174\n",
      "Epoch 72/200, Train Loss: 0.2038, Train Accuracy: 0.9158, Val Loss: 0.8231, Val AUC: 0.6912,Val Accuracy: 0.8295\n",
      "Epoch 73/200, Train Loss: 0.1988, Train Accuracy: 0.9192, Val Loss: 0.8291, Val AUC: 0.6855,Val Accuracy: 0.8140\n",
      "Epoch 74/200, Train Loss: 0.1899, Train Accuracy: 0.9244, Val Loss: 0.7043, Val AUC: 0.7003,Val Accuracy: 0.8171\n",
      "Epoch 75/200, Train Loss: 0.1837, Train Accuracy: 0.9267, Val Loss: 0.7661, Val AUC: 0.6986,Val Accuracy: 0.8197\n",
      "Epoch 76/200, Train Loss: 0.1761, Train Accuracy: 0.9288, Val Loss: 0.7459, Val AUC: 0.6956,Val Accuracy: 0.8252\n",
      "Epoch 77/200, Train Loss: 0.1838, Train Accuracy: 0.9259, Val Loss: 0.7754, Val AUC: 0.7067,Val Accuracy: 0.8037\n",
      "Epoch 78/200, Train Loss: 0.1816, Train Accuracy: 0.9275, Val Loss: 0.7615, Val AUC: 0.7073,Val Accuracy: 0.8060\n",
      "Epoch 79/200, Train Loss: 0.1783, Train Accuracy: 0.9280, Val Loss: 0.7533, Val AUC: 0.7098,Val Accuracy: 0.8169\n",
      "Epoch 80/200, Train Loss: 0.1661, Train Accuracy: 0.9335, Val Loss: 0.8416, Val AUC: 0.7068,Val Accuracy: 0.8110\n",
      "Epoch 81/200, Train Loss: 0.1786, Train Accuracy: 0.9276, Val Loss: 0.8573, Val AUC: 0.7035,Val Accuracy: 0.8001\n",
      "Epoch 82/200, Train Loss: 0.1625, Train Accuracy: 0.9364, Val Loss: 0.8868, Val AUC: 0.7073,Val Accuracy: 0.8035\n",
      "Epoch 83/200, Train Loss: 0.1624, Train Accuracy: 0.9351, Val Loss: 0.9130, Val AUC: 0.7001,Val Accuracy: 0.8006\n",
      "Epoch 84/200, Train Loss: 0.1526, Train Accuracy: 0.9405, Val Loss: 0.9400, Val AUC: 0.7019,Val Accuracy: 0.8104\n",
      "Epoch 85/200, Train Loss: 0.1714, Train Accuracy: 0.9307, Val Loss: 0.9287, Val AUC: 0.6971,Val Accuracy: 0.8050\n",
      "Epoch 86/200, Train Loss: 0.1513, Train Accuracy: 0.9416, Val Loss: 1.0082, Val AUC: 0.7003,Val Accuracy: 0.8032\n",
      "Epoch 87/200, Train Loss: 0.1494, Train Accuracy: 0.9413, Val Loss: 0.9292, Val AUC: 0.7001,Val Accuracy: 0.8027\n",
      "Epoch 88/200, Train Loss: 0.1456, Train Accuracy: 0.9423, Val Loss: 0.9447, Val AUC: 0.7002,Val Accuracy: 0.8110\n",
      "Epoch 89/200, Train Loss: 0.1522, Train Accuracy: 0.9401, Val Loss: 0.9831, Val AUC: 0.7008,Val Accuracy: 0.7895\n",
      "Epoch 90/200, Train Loss: 0.1417, Train Accuracy: 0.9438, Val Loss: 0.9913, Val AUC: 0.7046,Val Accuracy: 0.8040\n",
      "Epoch 91/200, Train Loss: 0.1574, Train Accuracy: 0.9392, Val Loss: 0.9695, Val AUC: 0.7110,Val Accuracy: 0.8133\n",
      "Epoch 92/200, Train Loss: 0.1536, Train Accuracy: 0.9393, Val Loss: 0.9137, Val AUC: 0.7086,Val Accuracy: 0.8037\n",
      "Epoch 93/200, Train Loss: 0.1300, Train Accuracy: 0.9491, Val Loss: 1.0613, Val AUC: 0.6992,Val Accuracy: 0.8099\n",
      "Epoch 94/200, Train Loss: 0.1330, Train Accuracy: 0.9478, Val Loss: 1.0742, Val AUC: 0.6994,Val Accuracy: 0.8068\n",
      "Epoch 95/200, Train Loss: 0.1336, Train Accuracy: 0.9485, Val Loss: 1.0395, Val AUC: 0.7055,Val Accuracy: 0.8135\n",
      "Epoch 96/200, Train Loss: 0.1307, Train Accuracy: 0.9496, Val Loss: 1.0729, Val AUC: 0.6954,Val Accuracy: 0.8084\n",
      "Epoch 97/200, Train Loss: 0.1216, Train Accuracy: 0.9523, Val Loss: 1.0676, Val AUC: 0.6907,Val Accuracy: 0.8029\n",
      "Epoch 98/200, Train Loss: 0.1235, Train Accuracy: 0.9528, Val Loss: 1.1558, Val AUC: 0.6908,Val Accuracy: 0.8130\n",
      "Epoch 99/200, Train Loss: 0.1184, Train Accuracy: 0.9541, Val Loss: 1.0523, Val AUC: 0.7010,Val Accuracy: 0.8156\n",
      "Epoch 100/200, Train Loss: 0.1321, Train Accuracy: 0.9498, Val Loss: 0.9657, Val AUC: 0.6939,Val Accuracy: 0.8022\n",
      "Epoch 101/200, Train Loss: 0.1148, Train Accuracy: 0.9569, Val Loss: 1.1444, Val AUC: 0.6962,Val Accuracy: 0.8091\n",
      "Epoch 102/200, Train Loss: 0.1136, Train Accuracy: 0.9564, Val Loss: 1.2005, Val AUC: 0.6904,Val Accuracy: 0.8004\n",
      "Epoch 103/200, Train Loss: 0.1226, Train Accuracy: 0.9538, Val Loss: 1.1432, Val AUC: 0.6951,Val Accuracy: 0.8081\n",
      "Epoch 104/200, Train Loss: 0.1109, Train Accuracy: 0.9582, Val Loss: 1.1968, Val AUC: 0.6908,Val Accuracy: 0.8079\n",
      "Epoch 105/200, Train Loss: 0.1037, Train Accuracy: 0.9612, Val Loss: 1.1585, Val AUC: 0.6978,Val Accuracy: 0.8019\n",
      "Epoch 106/200, Train Loss: 0.1086, Train Accuracy: 0.9591, Val Loss: 1.2026, Val AUC: 0.6913,Val Accuracy: 0.8068\n",
      "Epoch 107/200, Train Loss: 0.1078, Train Accuracy: 0.9595, Val Loss: 1.1096, Val AUC: 0.6861,Val Accuracy: 0.7947\n",
      "Epoch 108/200, Train Loss: 0.1069, Train Accuracy: 0.9604, Val Loss: 1.1289, Val AUC: 0.6967,Val Accuracy: 0.7843\n",
      "Epoch 109/200, Train Loss: 0.1193, Train Accuracy: 0.9570, Val Loss: 1.1129, Val AUC: 0.6916,Val Accuracy: 0.8081\n",
      "Epoch 110/200, Train Loss: 0.0953, Train Accuracy: 0.9657, Val Loss: 1.1316, Val AUC: 0.6941,Val Accuracy: 0.7986\n",
      "Epoch 111/200, Train Loss: 0.1012, Train Accuracy: 0.9626, Val Loss: 1.1967, Val AUC: 0.6921,Val Accuracy: 0.7967\n",
      "Epoch 112/200, Train Loss: 0.1005, Train Accuracy: 0.9628, Val Loss: 1.1331, Val AUC: 0.6905,Val Accuracy: 0.7952\n",
      "Epoch 113/200, Train Loss: 0.1066, Train Accuracy: 0.9605, Val Loss: 1.1041, Val AUC: 0.7011,Val Accuracy: 0.7872\n",
      "Epoch 114/200, Train Loss: 0.1027, Train Accuracy: 0.9613, Val Loss: 1.3075, Val AUC: 0.6978,Val Accuracy: 0.8182\n",
      "Epoch 115/200, Train Loss: 0.1005, Train Accuracy: 0.9614, Val Loss: 1.1896, Val AUC: 0.6982,Val Accuracy: 0.7998\n",
      "Epoch 116/200, Train Loss: 0.0946, Train Accuracy: 0.9646, Val Loss: 1.3093, Val AUC: 0.6988,Val Accuracy: 0.8045\n",
      "Epoch 117/200, Train Loss: 0.0998, Train Accuracy: 0.9642, Val Loss: 1.2751, Val AUC: 0.6954,Val Accuracy: 0.8125\n",
      "Epoch 118/200, Train Loss: 0.0819, Train Accuracy: 0.9708, Val Loss: 1.2811, Val AUC: 0.6992,Val Accuracy: 0.7957\n",
      "Epoch 119/200, Train Loss: 0.0915, Train Accuracy: 0.9664, Val Loss: 1.2408, Val AUC: 0.6962,Val Accuracy: 0.7916\n",
      "Epoch 120/200, Train Loss: 0.1038, Train Accuracy: 0.9617, Val Loss: 1.3260, Val AUC: 0.6984,Val Accuracy: 0.8133\n",
      "Epoch 121/200, Train Loss: 0.0818, Train Accuracy: 0.9710, Val Loss: 1.4042, Val AUC: 0.6950,Val Accuracy: 0.8089\n",
      "Epoch 122/200, Train Loss: 0.0895, Train Accuracy: 0.9673, Val Loss: 1.4572, Val AUC: 0.6953,Val Accuracy: 0.8156\n",
      "Epoch 123/200, Train Loss: 0.1112, Train Accuracy: 0.9602, Val Loss: 1.2579, Val AUC: 0.6803,Val Accuracy: 0.8066\n",
      "Epoch 124/200, Train Loss: 0.0868, Train Accuracy: 0.9687, Val Loss: 1.4125, Val AUC: 0.6869,Val Accuracy: 0.8215\n",
      "Epoch 125/200, Train Loss: 0.0861, Train Accuracy: 0.9689, Val Loss: 1.4589, Val AUC: 0.6945,Val Accuracy: 0.8159\n",
      "Epoch 126/200, Train Loss: 0.0829, Train Accuracy: 0.9703, Val Loss: 1.3519, Val AUC: 0.6817,Val Accuracy: 0.8110\n",
      "Epoch 127/200, Train Loss: 0.0786, Train Accuracy: 0.9719, Val Loss: 1.3242, Val AUC: 0.6885,Val Accuracy: 0.8045\n",
      "Epoch 128/200, Train Loss: 0.1007, Train Accuracy: 0.9643, Val Loss: 1.4994, Val AUC: 0.6922,Val Accuracy: 0.8257\n",
      "Epoch 129/200, Train Loss: 0.0800, Train Accuracy: 0.9720, Val Loss: 1.5665, Val AUC: 0.6800,Val Accuracy: 0.8177\n",
      "Epoch 130/200, Train Loss: 0.0702, Train Accuracy: 0.9754, Val Loss: 1.6207, Val AUC: 0.6907,Val Accuracy: 0.8184\n",
      "Epoch 131/200, Train Loss: 0.0894, Train Accuracy: 0.9684, Val Loss: 1.3269, Val AUC: 0.6916,Val Accuracy: 0.8169\n",
      "Epoch 132/200, Train Loss: 0.0850, Train Accuracy: 0.9695, Val Loss: 1.5029, Val AUC: 0.6857,Val Accuracy: 0.8032\n",
      "Epoch 133/200, Train Loss: 0.0804, Train Accuracy: 0.9714, Val Loss: 1.4964, Val AUC: 0.6865,Val Accuracy: 0.8094\n",
      "Epoch 134/200, Train Loss: 0.0776, Train Accuracy: 0.9729, Val Loss: 1.4882, Val AUC: 0.6937,Val Accuracy: 0.8179\n",
      "Epoch 135/200, Train Loss: 0.0682, Train Accuracy: 0.9760, Val Loss: 1.5468, Val AUC: 0.6896,Val Accuracy: 0.8042\n",
      "Epoch 136/200, Train Loss: 0.0733, Train Accuracy: 0.9747, Val Loss: 1.6833, Val AUC: 0.6762,Val Accuracy: 0.8187\n",
      "Epoch 137/200, Train Loss: 0.0898, Train Accuracy: 0.9680, Val Loss: 1.6077, Val AUC: 0.6843,Val Accuracy: 0.8156\n",
      "Epoch 138/200, Train Loss: 0.0791, Train Accuracy: 0.9724, Val Loss: 1.5591, Val AUC: 0.6872,Val Accuracy: 0.8146\n",
      "Epoch 139/200, Train Loss: 0.1269, Train Accuracy: 0.9569, Val Loss: 1.5411, Val AUC: 0.6891,Val Accuracy: 0.8143\n",
      "Epoch 140/200, Train Loss: 0.0858, Train Accuracy: 0.9699, Val Loss: 1.5473, Val AUC: 0.6924,Val Accuracy: 0.8148\n",
      "Epoch 141/200, Train Loss: 0.0659, Train Accuracy: 0.9779, Val Loss: 1.6493, Val AUC: 0.6872,Val Accuracy: 0.8210\n",
      "Epoch 142/200, Train Loss: 0.0589, Train Accuracy: 0.9796, Val Loss: 1.6014, Val AUC: 0.6904,Val Accuracy: 0.8161\n",
      "Epoch 143/200, Train Loss: 0.0761, Train Accuracy: 0.9731, Val Loss: 1.6079, Val AUC: 0.6874,Val Accuracy: 0.8079\n",
      "Epoch 144/200, Train Loss: 0.1071, Train Accuracy: 0.9619, Val Loss: 1.5083, Val AUC: 0.6934,Val Accuracy: 0.8089\n",
      "Epoch 145/200, Train Loss: 0.0809, Train Accuracy: 0.9722, Val Loss: 1.6340, Val AUC: 0.6863,Val Accuracy: 0.8161\n",
      "Epoch 146/200, Train Loss: 0.0741, Train Accuracy: 0.9746, Val Loss: 1.4581, Val AUC: 0.6901,Val Accuracy: 0.8143\n",
      "Epoch 147/200, Train Loss: 0.0630, Train Accuracy: 0.9780, Val Loss: 1.4557, Val AUC: 0.6869,Val Accuracy: 0.7967\n",
      "Epoch 148/200, Train Loss: 0.0708, Train Accuracy: 0.9761, Val Loss: 1.4903, Val AUC: 0.6912,Val Accuracy: 0.8053\n",
      "Epoch 149/200, Train Loss: 0.0686, Train Accuracy: 0.9756, Val Loss: 1.7567, Val AUC: 0.6902,Val Accuracy: 0.8223\n",
      "Epoch 150/200, Train Loss: 0.0884, Train Accuracy: 0.9695, Val Loss: 1.5691, Val AUC: 0.6931,Val Accuracy: 0.8244\n",
      "Epoch 151/200, Train Loss: 0.0722, Train Accuracy: 0.9750, Val Loss: 1.5519, Val AUC: 0.6832,Val Accuracy: 0.8068\n",
      "Epoch 152/200, Train Loss: 0.0556, Train Accuracy: 0.9814, Val Loss: 1.6033, Val AUC: 0.6900,Val Accuracy: 0.8110\n",
      "Epoch 153/200, Train Loss: 0.0704, Train Accuracy: 0.9758, Val Loss: 1.5484, Val AUC: 0.6968,Val Accuracy: 0.8063\n",
      "Epoch 154/200, Train Loss: 0.0586, Train Accuracy: 0.9801, Val Loss: 1.5621, Val AUC: 0.6918,Val Accuracy: 0.8011\n",
      "Epoch 155/200, Train Loss: 0.0658, Train Accuracy: 0.9768, Val Loss: 1.6324, Val AUC: 0.6905,Val Accuracy: 0.8099\n",
      "Epoch 156/200, Train Loss: 0.0752, Train Accuracy: 0.9744, Val Loss: 1.4898, Val AUC: 0.6811,Val Accuracy: 0.7753\n",
      "Epoch 157/200, Train Loss: 0.0740, Train Accuracy: 0.9750, Val Loss: 1.5354, Val AUC: 0.6943,Val Accuracy: 0.8035\n",
      "Epoch 158/200, Train Loss: 0.0650, Train Accuracy: 0.9771, Val Loss: 1.5624, Val AUC: 0.6779,Val Accuracy: 0.8112\n",
      "Epoch 159/200, Train Loss: 0.0526, Train Accuracy: 0.9820, Val Loss: 1.5346, Val AUC: 0.6931,Val Accuracy: 0.8040\n",
      "Epoch 160/200, Train Loss: 0.0686, Train Accuracy: 0.9767, Val Loss: 1.5939, Val AUC: 0.6880,Val Accuracy: 0.7929\n",
      "Epoch 161/200, Train Loss: 0.0625, Train Accuracy: 0.9791, Val Loss: 1.8161, Val AUC: 0.6753,Val Accuracy: 0.8055\n",
      "Epoch 162/200, Train Loss: 0.0531, Train Accuracy: 0.9808, Val Loss: 1.7288, Val AUC: 0.6803,Val Accuracy: 0.8004\n",
      "Epoch 163/200, Train Loss: 0.0624, Train Accuracy: 0.9782, Val Loss: 1.7375, Val AUC: 0.6838,Val Accuracy: 0.8032\n",
      "Epoch 164/200, Train Loss: 0.0494, Train Accuracy: 0.9826, Val Loss: 1.7188, Val AUC: 0.6896,Val Accuracy: 0.8081\n",
      "Epoch 165/200, Train Loss: 0.0693, Train Accuracy: 0.9765, Val Loss: 1.6751, Val AUC: 0.6923,Val Accuracy: 0.8192\n",
      "Epoch 166/200, Train Loss: 0.0560, Train Accuracy: 0.9811, Val Loss: 1.6209, Val AUC: 0.6981,Val Accuracy: 0.8125\n",
      "Epoch 167/200, Train Loss: 0.0758, Train Accuracy: 0.9742, Val Loss: 1.7579, Val AUC: 0.6881,Val Accuracy: 0.8213\n",
      "Epoch 168/200, Train Loss: 0.0628, Train Accuracy: 0.9788, Val Loss: 1.7278, Val AUC: 0.6805,Val Accuracy: 0.8148\n",
      "Epoch 169/200, Train Loss: 0.0634, Train Accuracy: 0.9785, Val Loss: 1.6005, Val AUC: 0.6919,Val Accuracy: 0.7991\n",
      "Epoch 170/200, Train Loss: 0.0419, Train Accuracy: 0.9861, Val Loss: 1.7044, Val AUC: 0.6848,Val Accuracy: 0.8001\n",
      "Epoch 171/200, Train Loss: 0.0538, Train Accuracy: 0.9816, Val Loss: 1.7167, Val AUC: 0.6839,Val Accuracy: 0.8045\n",
      "Epoch 172/200, Train Loss: 0.0739, Train Accuracy: 0.9743, Val Loss: 1.7174, Val AUC: 0.6871,Val Accuracy: 0.8239\n",
      "Epoch 173/200, Train Loss: 0.0572, Train Accuracy: 0.9802, Val Loss: 1.6832, Val AUC: 0.6831,Val Accuracy: 0.8009\n",
      "Epoch 174/200, Train Loss: 0.0572, Train Accuracy: 0.9800, Val Loss: 1.6531, Val AUC: 0.6903,Val Accuracy: 0.8045\n",
      "Epoch 175/200, Train Loss: 0.0431, Train Accuracy: 0.9849, Val Loss: 1.6921, Val AUC: 0.6901,Val Accuracy: 0.8140\n",
      "Epoch 176/200, Train Loss: 0.0673, Train Accuracy: 0.9769, Val Loss: 1.6350, Val AUC: 0.6965,Val Accuracy: 0.7859\n",
      "Epoch 177/200, Train Loss: 0.0519, Train Accuracy: 0.9814, Val Loss: 1.5927, Val AUC: 0.6914,Val Accuracy: 0.8208\n",
      "Epoch 178/200, Train Loss: 0.0533, Train Accuracy: 0.9813, Val Loss: 1.5999, Val AUC: 0.6948,Val Accuracy: 0.8135\n",
      "Epoch 179/200, Train Loss: 0.0681, Train Accuracy: 0.9776, Val Loss: 1.6259, Val AUC: 0.6957,Val Accuracy: 0.7947\n",
      "Epoch 180/200, Train Loss: 0.0528, Train Accuracy: 0.9821, Val Loss: 1.5677, Val AUC: 0.7027,Val Accuracy: 0.8228\n",
      "Epoch 181/200, Train Loss: 0.0547, Train Accuracy: 0.9810, Val Loss: 1.6107, Val AUC: 0.6989,Val Accuracy: 0.8089\n",
      "Epoch 182/200, Train Loss: 0.0399, Train Accuracy: 0.9867, Val Loss: 1.6480, Val AUC: 0.6912,Val Accuracy: 0.8120\n",
      "Epoch 183/200, Train Loss: 0.0522, Train Accuracy: 0.9819, Val Loss: 1.6342, Val AUC: 0.6910,Val Accuracy: 0.8014\n",
      "Epoch 184/200, Train Loss: 0.0530, Train Accuracy: 0.9815, Val Loss: 1.5702, Val AUC: 0.6981,Val Accuracy: 0.8159\n",
      "Epoch 185/200, Train Loss: 0.0594, Train Accuracy: 0.9783, Val Loss: 1.7065, Val AUC: 0.6966,Val Accuracy: 0.8019\n",
      "Epoch 186/200, Train Loss: 0.0475, Train Accuracy: 0.9844, Val Loss: 1.6126, Val AUC: 0.6963,Val Accuracy: 0.8037\n",
      "Epoch 187/200, Train Loss: 0.0471, Train Accuracy: 0.9847, Val Loss: 1.7476, Val AUC: 0.6978,Val Accuracy: 0.8153\n",
      "Epoch 188/200, Train Loss: 0.0444, Train Accuracy: 0.9844, Val Loss: 1.6739, Val AUC: 0.7017,Val Accuracy: 0.7908\n",
      "Epoch 189/200, Train Loss: 0.0684, Train Accuracy: 0.9771, Val Loss: 1.5965, Val AUC: 0.6898,Val Accuracy: 0.7952\n",
      "Epoch 190/200, Train Loss: 0.0402, Train Accuracy: 0.9869, Val Loss: 1.5639, Val AUC: 0.6924,Val Accuracy: 0.7986\n",
      "Epoch 191/200, Train Loss: 0.0367, Train Accuracy: 0.9875, Val Loss: 1.5475, Val AUC: 0.6927,Val Accuracy: 0.8017\n",
      "Epoch 192/200, Train Loss: 0.0640, Train Accuracy: 0.9787, Val Loss: 1.5907, Val AUC: 0.6953,Val Accuracy: 0.8146\n",
      "Epoch 193/200, Train Loss: 0.0651, Train Accuracy: 0.9782, Val Loss: 1.6277, Val AUC: 0.6837,Val Accuracy: 0.8138\n",
      "Epoch 194/200, Train Loss: 0.0424, Train Accuracy: 0.9856, Val Loss: 1.5606, Val AUC: 0.6968,Val Accuracy: 0.8076\n",
      "Epoch 195/200, Train Loss: 0.0556, Train Accuracy: 0.9813, Val Loss: 1.6341, Val AUC: 0.6852,Val Accuracy: 0.7929\n",
      "Epoch 196/200, Train Loss: 0.0452, Train Accuracy: 0.9849, Val Loss: 1.6701, Val AUC: 0.6884,Val Accuracy: 0.7998\n",
      "Epoch 197/200, Train Loss: 0.0557, Train Accuracy: 0.9809, Val Loss: 1.6628, Val AUC: 0.6796,Val Accuracy: 0.7949\n",
      "Epoch 198/200, Train Loss: 0.0634, Train Accuracy: 0.9796, Val Loss: 1.6254, Val AUC: 0.6937,Val Accuracy: 0.8110\n",
      "Epoch 199/200, Train Loss: 0.0392, Train Accuracy: 0.9869, Val Loss: 1.6492, Val AUC: 0.6909,Val Accuracy: 0.8063\n",
      "Epoch 200/200, Train Loss: 0.0437, Train Accuracy: 0.9855, Val Loss: 1.6167, Val AUC: 0.6905,Val Accuracy: 0.8055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lstm adjusted\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "results = {}\n",
    "\n",
    "data_paths = [\n",
    "    \"data/preprocessed/preprocessed_data_6H.csv\",\n",
    "]\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch_data = data[i:i+batch_size]\n",
    "        \n",
    "        # Pad sequences to the same length within the batch\n",
    "        max_seq_length = max(len(seq) for seq in batch_data)\n",
    "        \n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        for seq in batch_data:\n",
    "            padded_seq = np.pad(seq, ((0, max_seq_length - len(seq)), (0, 0)), mode='constant')\n",
    "            X_batch.append(padded_seq[:, 1:-1])  # Exclude icustay_id and aki_stage\n",
    "            y_batch.append(padded_seq[0, -1])  # Take the aki_stage of the first row\n",
    "        \n",
    "        X_batches.append(torch.FloatTensor(X_batch))\n",
    "        y_batches.append(torch.LongTensor(y_batch))\n",
    "    \n",
    "    return X_batches, y_batches\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, emb_size, num_layers=number_layers, \n",
    "                            bidirectional=bi_directional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(emb_size * (2 if bi_directional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        \n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "for data_path in data_paths:\n",
    "    tail = data_path.split(\"/\")[-1]\n",
    "    print(f\"Processing {tail}\")\n",
    "\n",
    "    X = pd.read_csv(data_path)\n",
    "    # X = X.head(10000)  # only take the first 10000 rows\n",
    "\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage')\n",
    "    numeric_feat.remove('icustay_id')\n",
    "\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    id_list = X['icustay_id'].unique()\n",
    "    id_train, id_test_val = train_test_split(id_list, test_size=SPLIT_SIZE, random_state=42)\n",
    "    id_valid, id_test = train_test_split(id_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "    X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id', 'charttime'])\n",
    "    test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id', 'charttime'])\n",
    "    validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "    train.drop(['charttime'], axis=1, inplace=True)\n",
    "    test.drop(['charttime'], axis=1, inplace=True)\n",
    "    validation.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "    train = list(train.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
    "    train.sort(key=len, reverse=True)  # Sort by sequence length in descending order\n",
    "    print(\"Number of sequences in train:\", len(train))\n",
    "    test = list(test.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
    "    validation = list(validation.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
    "\n",
    "    batch_size = 32  # You may need to adjust this\n",
    "    X_train, y_train = batch(train, batch_size)\n",
    "    X_test, y_test = batch(test, batch_size)\n",
    "    X_val, y_val = batch(validation, batch_size)\n",
    "\n",
    "    print(f\"Number of batches in train: {len(X_train)}\")\n",
    "\n",
    "    writer = SummaryWriter(f'runs/{tail}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "\n",
    "    input_size = X_train[0].shape[2]\n",
    "    output_size = 1\n",
    "    emb_size = round(input_size / 1)\n",
    "    number_layers = 3\n",
    "    dropout = 0\n",
    "    bi_directional = True\n",
    "\n",
    "    nn_model = Net(input_size, emb_size, output_size, bi_directional, number_layers, dropout).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "    use_pretrained = False\n",
    "    best_auc = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    if use_pretrained:\n",
    "        model_path = f'data/models/{tail}/LSTM_best.pth'\n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path)\n",
    "            nn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            best_auc = checkpoint.get('best_auc', 0)\n",
    "            start_epoch = checkpoint.get('epoch', 0)\n",
    "            print(f\"Loaded pretrained model from {model_path} with AUC {best_auc} at epoch {start_epoch}.\")\n",
    "        else:\n",
    "            print(f\"No pretrained model found, starting training from scratch.\")\n",
    "\n",
    "    n_epochs = 200\n",
    "\n",
    "    for epoch in range(start_epoch + 1, n_epochs):\n",
    "        nn_model.train()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "\n",
    "        for i, (X_batch, y_batch) in enumerate(zip(X_train, y_train)):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = nn_model(X_batch)\n",
    "            loss = criterion(outputs, y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            train_accuracy = accuracy_score(y_batch.cpu().numpy(), predicted.cpu().numpy())\n",
    "            running_accuracy += train_accuracy\n",
    "\n",
    "            writer.add_scalar('Training/Loss', loss.item(), epoch * len(X_train) + i)\n",
    "            writer.add_scalar('Training/Accuracy', train_accuracy, epoch * len(X_train) + i)\n",
    "\n",
    "        nn_model.eval()\n",
    "        total_v_loss = 0\n",
    "        all_y_val = []\n",
    "        all_val_prob = []\n",
    "        all_accuracy = 0\n",
    "\n",
    "        for X_val_batch, y_val_batch in zip(X_val, y_val):\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                v_out = nn_model(X_val_batch)\n",
    "                v_loss = criterion(v_out, y_val_batch.float())\n",
    "                val_prob = torch.sigmoid(v_out)\n",
    "                total_v_loss += v_loss.item()\n",
    "                all_y_val.extend(y_val_batch.cpu().numpy())\n",
    "                all_val_prob.extend(val_prob.cpu().numpy())\n",
    "                \n",
    "                predicted = val_prob > 0.5\n",
    "                val_accuracy = accuracy_score(y_val_batch.cpu().numpy(), predicted.cpu().numpy())\n",
    "                all_accuracy += val_accuracy\n",
    "\n",
    "        avg_v_loss = total_v_loss / len(X_val)\n",
    "        roc_auc = roc_auc_score(all_y_val, all_val_prob)\n",
    "        avg_accuracy = all_accuracy / len(X_val)\n",
    "        \n",
    "        writer.add_scalar('Validation/Loss', avg_v_loss, epoch)\n",
    "        writer.add_scalar('Validation/AUC', roc_auc, epoch)\n",
    "        writer.add_scalar('Validation/Accuracy', avg_accuracy, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, \"\n",
    "              f\"Train Loss: {running_loss/len(X_train):.4f}, \"\n",
    "              f\"Train Accuracy: {running_accuracy/len(X_train):.4f}, \"\n",
    "              f\"Val Loss: {avg_v_loss:.4f}, \"\n",
    "              f\"Val AUC: {roc_auc:.4f},\"\n",
    "              f\"Val Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "        if roc_auc > best_auc:\n",
    "            best_auc = roc_auc\n",
    "            save_path = f'data/models/{tail}/LSTM_best.pth'\n",
    "            torch.save({\n",
    "                'model_state_dict': nn_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_auc': best_auc,\n",
    "                'epoch': epoch\n",
    "            }, save_path)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing preprocessed_data_6H.csv\n",
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_25440\\4084146763.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_25440\\4084146763.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "C:\\Users\\derqu\\AppData\\Local\\Temp\\ipykernel_25440\\4084146763.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5193, Test AUC: 0.6666, Test F1: 0.3546, Test PR AUC: 0.3450\n"
     ]
    }
   ],
   "source": [
    "# evaluate pretrained lstm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import collections\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "\n",
    "data_path = \"data/preprocessed/preprocessed_data_6H.csv\"\n",
    "tail = data_path.split(\"/\")[-1]\n",
    "print(f\"Processing {tail}\")\n",
    "\n",
    "X = pd.read_csv(data_path)\n",
    "\n",
    "# Preprocessing steps (similar to XGBoost)\n",
    "numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_feat.remove('aki_stage')\n",
    "numeric_feat.remove('icustay_id')\n",
    "\n",
    "X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "try:\n",
    "    X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Split data (you may want to use the same splitting logic as in XGBoost)\n",
    "id_list = X['icustay_id'].unique()\n",
    "# id_list = common_id_list\n",
    "id_train, id_test_val = train_test_split(id_list, test_size=SPLIT_SIZE, random_state=42)\n",
    "id_valid, id_test = train_test_split(id_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index=True)\n",
    "validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id'])\n",
    "\n",
    "train.drop(['charttime'], axis=1, inplace=True)\n",
    "test.drop(['charttime'], axis=1, inplace=True)\n",
    "validation.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "# Prepare data for LSTM\n",
    "# X_train, y_train = batch(train.to_numpy(), batch_size)\n",
    "# X_test, y_test = batch(test.to_numpy(), test.shape[0])\n",
    "# X_val, y_val = batch(validation.to_numpy(), validation.shape[0])\n",
    "X_train, y_train = batch(train, batch_size)\n",
    "X_test, y_test = batch(test, batch_size)\n",
    "X_val, y_val = batch(validation, batch_size)\n",
    "\n",
    "# LSTM parameters\n",
    "input_size = X_train[0].shape[2]  # Subtract 2 for icustay_id and aki_stage\n",
    "output_size = 1\n",
    "emb_size = round(input_size / 1)\n",
    "number_layers = 3\n",
    "dropout = 0\n",
    "bi_directional = True\n",
    "\n",
    "\n",
    "# Assuming Net is defined elsewhere\n",
    "# Assuming X_train, y_train, X_val, y_val, X_test, y_test are defined and split into batches if necessary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "nn_model = Net(input_size, emb_size, output_size, bi_directional, number_layers, dropout).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Test evaluation with F1 score\n",
    "nn_model.load_state_dict(torch.load(f'data/models/{tail}/LSTM_best.pth')['model_state_dict'])\n",
    "nn_model.eval()\n",
    "total_test_loss = 0\n",
    "all_y_test = []\n",
    "all_test_prob = []\n",
    "all_test_f1 = 0\n",
    "\n",
    "for X_test_batch, y_test_batch in zip(X_test, y_test):\n",
    "    X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        t_out = nn_model(X_test_batch)\n",
    "        t_out = torch.flatten(t_out)\n",
    "        y_test_batch = y_test_batch.type_as(t_out)\n",
    "        test_loss = criterion(t_out, y_test_batch)\n",
    "        test_prob = torch.sigmoid(t_out)\n",
    "        total_test_loss += test_loss.item()\n",
    "        all_y_test.extend(y_test_batch.cpu().numpy())\n",
    "        all_test_prob.extend(test_prob.cpu().numpy())\n",
    "        \n",
    "        predicted = torch.sigmoid(t_out) > 0.08\n",
    "        test_f1 = f1_score(y_test_batch.cpu().numpy(), predicted.cpu().numpy(), zero_division=1)\n",
    "        all_test_f1 += test_f1\n",
    "        \n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(all_y_test, all_test_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "os.makedirs('data/plots', exist_ok=True)\n",
    "plt.savefig(f'data/plots/ROC_{tail}.png')  # Save ROC curve\n",
    "plt.close()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(all_y_test, all_test_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='PR curve (area = %0.2f)' % pr_auc)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "os.makedirs('data/plots', exist_ok=True)\n",
    "plt.savefig(f'data/plots/PR_{tail}.png')  # Save PR curve\n",
    "plt.close()\n",
    "\n",
    "print(f\"Test Loss: {total_test_loss / len(X_test):.4f}, \"\n",
    "    f\"Test AUC: {roc_auc:.4f}, \"\n",
    "    f\"Test F1: {all_test_f1 / len(X_test):.4f}, \"\n",
    "    f\"Test PR AUC: {pr_auc:.4f}\")\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[-6.2844e-01, -3.4840e-01,  5.1099e-01, -9.4520e-01, -2.9931e-01,\n",
      "         -1.9681e+01, -4.7650e+00,  2.0987e-01,  1.0024e+00, -1.2117e+01,\n",
      "         -7.8178e-01,  5.6486e-01,  1.2308e+00,  1.4534e+00,  4.9985e-01,\n",
      "         -4.3694e-01, -5.4323e-01, -1.4126e+00, -3.6823e-02,  2.7138e-01,\n",
      "          6.0871e-01,  6.2375e-01,  7.4021e-01, -7.2923e+00, -1.4893e+00,\n",
      "          8.1274e-01,  3.5702e-02,  1.2902e-01,  5.8829e-01,  1.3141e-01],\n",
      "        [ 2.3295e-03, -7.9618e-03, -1.9757e-01,  1.5201e-01,  8.5466e-02,\n",
      "          2.9914e-02,  3.9310e-02, -1.8665e-01,  3.2036e-02,  6.3996e-02,\n",
      "         -6.1319e-02, -6.5049e-02, -1.4711e-02, -7.4032e-03,  8.1882e-02,\n",
      "         -5.8111e-02, -6.4294e-02, -1.5904e-01, -1.2179e-01,  1.0915e-01,\n",
      "         -3.5988e-02, -5.6990e-02, -1.9008e-01,  3.4828e-02, -1.7858e-01,\n",
      "         -1.4834e-01, -1.0436e-01,  9.6695e-02, -1.1385e-01, -2.2227e-01],\n",
      "        [-5.5578e-01,  3.3070e-01, -4.0488e-01, -1.5083e+01,  3.8799e-01,\n",
      "         -9.5876e+00,  2.2099e+00, -2.3215e-01, -1.9323e+00, -2.7901e+00,\n",
      "         -5.2577e-01,  5.9785e-01,  2.2955e-01,  1.7498e+00, -1.2657e+00,\n",
      "         -3.9688e-01,  9.3862e-01,  4.0498e-02,  6.0123e-01,  1.3599e+00,\n",
      "          1.2116e+00,  9.7679e-01,  1.5382e+00,  1.6435e+00, -3.9546e-01,\n",
      "         -1.9655e-01, -6.3161e-01, -2.8138e+00, -5.9622e-01, -4.1068e-01],\n",
      "        [ 1.5330e+00,  1.2195e-01,  4.6677e-01,  1.9961e+00, -1.0950e+00,\n",
      "          3.1617e+00,  6.7643e+00, -9.1643e-02,  1.5458e+00, -5.7973e-01,\n",
      "         -1.6915e+00, -6.7800e-01,  6.2540e-02,  1.2208e+00, -1.8755e+00,\n",
      "          1.2937e-01,  7.6763e-01, -2.0063e-01,  1.2685e+00,  6.0064e-04,\n",
      "          1.6750e+00,  1.6231e+00,  1.7686e+00,  1.6081e+00,  1.1192e+00,\n",
      "          1.1681e+00, -1.6727e+00, -4.2283e-01,  1.6945e-01, -3.2511e-01],\n",
      "        [ 3.6567e-01, -1.0535e+00, -1.9633e+00,  1.9543e+01,  2.0288e-01,\n",
      "          2.1655e+01,  2.8408e-01,  7.0675e-01,  1.2752e+00,  6.4746e+00,\n",
      "          3.7265e-01,  8.3691e-01,  2.4345e-01, -1.0338e+00,  5.8818e-01,\n",
      "          5.7358e-02, -1.2217e-04,  6.5469e-01, -8.4307e-02, -3.6879e-01,\n",
      "         -3.8643e+00, -3.6689e+00, -4.4250e+00, -2.1002e+00,  2.0074e-01,\n",
      "         -1.9805e-01, -3.6330e-01, -7.8364e-01,  3.4209e-01,  3.3979e-01],\n",
      "        [ 3.2338e-02, -1.2214e-01, -1.3246e+00, -5.1238e+00, -1.3068e-01,\n",
      "         -2.5858e+01,  5.2316e+00,  1.0552e+00, -1.1036e+00, -1.7323e+01,\n",
      "          1.7366e-01,  5.2922e-01, -2.3668e-01,  1.4178e+00,  6.6954e-02,\n",
      "         -1.4617e+00, -1.1938e-01, -2.4463e-01,  5.5001e-01, -2.0717e+00,\n",
      "          3.4434e-01,  2.7202e-01,  2.2816e-01, -1.6172e+00, -3.3264e-01,\n",
      "         -1.4940e+00, -2.3969e-01, -1.2156e-01,  4.6454e-01,  4.4568e-01],\n",
      "        [-7.0404e+00,  2.5362e-01, -4.2304e+00,  1.7323e+01,  1.8194e-01,\n",
      "          1.1203e+01, -2.8942e-01,  2.8305e-01, -9.3455e-01,  1.1402e+00,\n",
      "         -5.4567e-01,  7.3834e-02,  1.7987e-01, -1.9604e-01,  1.1094e+00,\n",
      "          2.4676e-01,  6.1508e-01, -4.6308e-01, -6.6428e-02,  4.2090e-01,\n",
      "         -7.5936e-01, -3.0838e-01, -5.9409e-01, -6.3179e-01, -1.5485e-01,\n",
      "         -2.5167e-01,  4.7908e-01,  1.9976e-01, -1.3139e-02, -3.0100e-01],\n",
      "        [-6.7501e+00, -5.7526e+00, -2.8419e+00, -2.3773e+01,  6.1068e-01,\n",
      "         -7.5904e+00, -3.9648e+00,  4.4052e-02, -4.1338e+00, -1.0307e+01,\n",
      "          1.0204e+00, -4.9100e-01, -5.9459e-01,  1.7185e+00,  2.9463e+00,\n",
      "          6.8240e-01,  8.8811e-01, -2.6917e-01,  8.1737e-01,  4.4915e-02,\n",
      "          3.5633e+00,  3.4585e+00,  3.7952e+00,  4.9522e+00,  1.4406e-01,\n",
      "         -6.1278e-01,  1.9486e-01, -2.4621e+00, -7.0963e-01, -5.8039e-01],\n",
      "        [ 1.7362e+00,  2.5302e-01, -1.5143e+00, -1.5470e+01, -7.2494e-02,\n",
      "          1.4904e+01,  6.3751e+00,  3.0862e-01, -1.5686e+00,  3.9528e-01,\n",
      "         -5.5028e-01,  4.9640e-01, -2.4891e-01, -9.6162e-01, -2.6056e-01,\n",
      "          1.3988e+00, -1.8304e-01, -5.2321e-01, -9.2840e-01,  1.9271e-01,\n",
      "         -1.7915e+00, -1.8429e+00, -2.0721e+00,  3.8105e+00, -8.2022e-01,\n",
      "         -5.3617e-02,  1.0039e+00, -2.5110e-01,  1.8777e-01,  1.9653e-01],\n",
      "        [ 2.3635e-01, -1.1695e+00,  1.2724e+00, -2.2921e+00,  1.8928e+00,\n",
      "         -8.9293e+00,  4.0232e+00, -4.4787e+00, -1.5754e-01, -5.3587e+00,\n",
      "          8.7788e-01,  6.5850e-01,  3.4465e-02, -3.4663e+00, -8.2893e-01,\n",
      "          6.0549e-02,  7.8597e-01, -2.5076e-01, -4.1464e+00,  7.7798e-01,\n",
      "         -3.9765e-01, -2.6757e-01, -2.4919e-01,  6.9160e-01, -1.5196e-01,\n",
      "         -7.4643e-01,  1.6607e-01, -8.1879e-02, -8.8609e-02,  3.8931e-02],\n",
      "        [-2.0361e+00,  1.4477e+00,  2.9677e+00, -3.8689e-01, -1.9877e-01,\n",
      "         -2.4787e-01, -9.8837e-01,  1.4504e+00,  1.9697e-01,  1.9707e-01,\n",
      "         -7.7616e-01, -1.1987e-01, -8.9650e-01,  1.1036e+00, -1.3481e+00,\n",
      "          5.8891e-01,  6.9484e-01, -1.9361e+00,  1.2843e+00,  4.8908e-01,\n",
      "          7.5605e-01,  7.7963e-01,  8.2005e-01, -8.1903e+00,  1.9534e-01,\n",
      "          7.8510e-01, -9.5870e-01,  7.8632e-01, -3.0483e+00, -1.5825e-01],\n",
      "        [ 3.5318e+00, -2.1040e+00,  3.0486e-01, -1.2570e+01, -1.4151e+00,\n",
      "         -3.3845e+00,  1.3423e+00, -2.7478e-01,  5.1646e-01, -2.1824e+00,\n",
      "          9.7563e-01, -4.2071e-01,  2.5808e-01, -5.8828e-01,  5.8204e-02,\n",
      "          2.0085e+00, -4.8713e-01,  3.0307e-03, -1.4497e-01, -2.0571e-01,\n",
      "         -1.8653e+00, -1.9125e+00, -2.2576e+00,  2.9335e+00,  2.8101e-01,\n",
      "          2.3725e-01,  4.1179e-01, -2.4523e+00,  5.1159e-01,  5.1673e-01],\n",
      "        [-1.9889e+00, -2.8924e+00,  1.4425e+00,  1.1868e+01,  2.3920e+00,\n",
      "          5.3650e+00, -6.0111e+00,  1.1041e+00,  2.7985e+00,  7.0504e+00,\n",
      "         -1.1369e+00,  1.3667e-01,  3.1491e-01, -8.2805e-01, -1.2668e+00,\n",
      "         -5.3614e-01,  5.8231e-01, -8.9491e-03,  1.1123e-01,  1.0062e+00,\n",
      "         -2.2060e-01, -6.0163e-02, -1.6746e-01, -3.3592e+00,  1.0627e-02,\n",
      "         -4.8399e-01, -9.8257e-02,  6.1397e-02, -1.3908e-01,  1.5769e-01],\n",
      "        [-8.1765e-01,  4.9723e-01,  2.8106e+00, -6.8877e+00,  5.3640e-01,\n",
      "         -3.4935e+00, -9.1233e+00,  1.3837e-01,  1.1071e+00,  2.7791e+00,\n",
      "         -7.7414e-01,  2.8319e-01, -2.1248e-01, -1.1719e+00, -7.5222e-01,\n",
      "         -1.1055e+00,  8.5223e-01,  3.3355e-01,  2.8650e-01,  3.5526e-01,\n",
      "          1.7634e+00,  1.4152e+00,  1.8673e+00,  3.0955e-01,  1.7542e-01,\n",
      "         -5.9886e-01, -5.2492e-01, -3.1140e-02,  4.7592e-02, -1.8701e-01],\n",
      "        [ 9.4349e-01, -1.4635e-01, -8.8848e-01, -7.5850e+00,  3.0031e-01,\n",
      "         -1.8770e+01,  3.1512e+00, -9.0942e-02, -2.1956e+00, -1.7173e+01,\n",
      "         -1.3099e+00,  7.6776e-01, -1.1705e-02,  2.0262e+00,  2.6952e-02,\n",
      "         -8.2950e-02,  2.6184e-01, -1.3891e+00, -5.3193e-02, -1.7928e+00,\n",
      "         -4.4546e-01, -2.8939e-01, -3.3623e-01, -1.5539e+00, -7.1811e-02,\n",
      "          2.0238e+00,  1.7928e-01, -7.8029e-01,  1.2634e-02,  3.6919e-01],\n",
      "        [ 2.2456e+00,  1.4623e+00, -3.9748e+00, -7.4775e+00, -1.1082e+00,\n",
      "          7.2496e+00,  5.8886e+00, -6.8033e-01, -4.4436e+00, -6.9703e-02,\n",
      "         -5.0422e-01,  1.0663e+00, -1.3589e-01, -6.5403e-01,  1.5110e+00,\n",
      "         -6.1394e-01, -2.6844e-01,  7.8839e-01,  7.0783e-01, -3.6630e-01,\n",
      "         -1.6513e+00, -1.4894e+00, -1.6008e+00, -7.5532e+00,  8.5559e-01,\n",
      "         -5.9518e-01, -2.2427e-01,  6.8261e-01, -1.0168e-01,  1.9571e-01],\n",
      "        [-2.1390e+00, -1.7898e+00, -3.0191e+00, -5.8412e+00,  1.5200e-01,\n",
      "         -7.1959e+00, -1.4309e+01, -5.1876e-01, -4.2269e+00, -2.3639e+00,\n",
      "          4.9509e-01, -1.9036e-01,  1.8935e+00,  8.0958e-01, -4.6010e-01,\n",
      "          3.0752e-01, -3.1765e-01,  2.1395e-01,  7.4585e-02,  1.4927e+00,\n",
      "          3.5569e+00,  3.1404e+00,  4.0844e+00,  7.6673e+00, -3.0028e-02,\n",
      "          6.3274e-03,  3.5515e-01, -1.1604e+00,  1.8968e-01,  2.7455e-01],\n",
      "        [-2.0478e+00,  2.3074e+00,  6.6038e+00,  3.9851e-01, -1.4934e-01,\n",
      "          3.8239e+00, -1.4033e-01, -1.5757e+00, -1.0412e+00,  9.1950e-01,\n",
      "          1.1869e+00,  2.6111e-01,  3.7034e-01, -7.6940e-01, -1.7561e+00,\n",
      "          2.2155e+00,  4.2884e-01, -7.4504e-01,  3.9313e-01,  1.0063e+00,\n",
      "          1.0730e+00,  1.1448e+00,  1.5729e+00, -3.0851e+00, -6.6106e-01,\n",
      "         -8.9542e-01,  4.6699e-01,  1.3432e+00, -1.4941e+00, -1.2922e+00],\n",
      "        [-1.7560e-01, -3.7635e-02,  1.3527e-01,  5.2830e-02, -1.5618e-02,\n",
      "         -1.8168e-01, -2.4936e-02, -8.5939e-02,  9.8538e-02, -3.1050e-02,\n",
      "         -7.4104e-02, -1.5964e-02,  2.2995e-02,  1.1757e-01, -5.7381e-02,\n",
      "         -1.3288e-01, -9.1634e-02, -2.2427e-02, -1.2971e-01, -9.2904e-02,\n",
      "          4.4274e-02, -1.3389e-01, -7.0214e-02,  4.8901e-03, -1.0948e-01,\n",
      "          5.5614e-02,  6.9216e-02, -1.2663e-01,  7.2842e-03,  5.1826e-02],\n",
      "        [-1.4484e+00,  6.8033e-01,  4.0330e-01,  4.6227e+00,  8.3843e-01,\n",
      "          3.9992e+00, -1.4984e+01, -2.9862e-01, -2.2933e+00,  2.0160e+00,\n",
      "         -1.5482e-01, -8.6327e-01, -1.9906e-01,  9.2371e-01, -7.7240e-01,\n",
      "          5.4910e-01, -1.5341e-01,  5.0161e-01,  7.6518e-01,  1.6218e-01,\n",
      "          3.3019e+00,  3.1321e+00,  3.8660e+00,  1.8959e+00, -6.7883e-01,\n",
      "          1.5481e-02,  4.6197e-01, -6.2406e-02, -4.8972e-01, -3.3790e-01],\n",
      "        [ 7.9675e-02, -1.9542e-01,  5.7587e-02, -8.6757e-02,  6.2769e-02,\n",
      "         -1.4140e-01, -1.7719e-01, -1.8156e-01, -1.9201e-01, -8.9517e-02,\n",
      "          2.9070e-02, -2.6821e-01, -1.2149e-01,  1.0165e-01,  5.0976e-02,\n",
      "         -5.3852e-02,  4.8731e-02, -1.9310e-01, -1.5524e-01,  9.1479e-02,\n",
      "         -2.4590e-01, -1.3595e-01, -1.3183e-01, -9.3112e-02, -9.5224e-02,\n",
      "         -1.6385e-02,  2.1977e-02, -2.7523e-01,  2.1111e-02, -9.6116e-02],\n",
      "        [-1.3615e+00, -5.3569e-01, -8.0135e-01,  2.2180e+00,  5.1468e-01,\n",
      "         -4.0430e+00, -1.3843e+01, -1.0540e-01,  2.5364e+00,  3.5958e+00,\n",
      "         -3.6194e-01, -3.2138e-01,  3.6110e-01, -1.2590e+00,  5.3288e-01,\n",
      "         -4.0090e-01, -1.2572e-01,  3.6562e-01,  5.8834e-01,  4.5710e-01,\n",
      "          1.0848e+00,  8.2982e-01,  1.2152e+00,  5.5330e+00,  1.1134e-01,\n",
      "         -1.0858e+00, -7.8799e-02,  1.1383e+00,  1.2750e-01,  3.8566e-01],\n",
      "        [-3.8246e-03, -1.6759e-01, -3.6663e-02,  7.7296e-02, -2.3274e-02,\n",
      "          1.4493e-01, -1.4725e-01, -9.5991e-02, -1.6709e-01, -2.0851e-02,\n",
      "         -3.8055e-02,  4.2398e-02, -1.4527e-01, -9.3494e-03, -3.2964e-02,\n",
      "         -1.6783e-01, -8.8364e-02, -2.4617e-01, -1.6941e-02,  1.8026e-01,\n",
      "         -1.9757e-01, -6.3896e-02, -5.3766e-02, -1.2556e-01, -5.4424e-04,\n",
      "         -1.4310e-01, -1.9424e-01, -5.5670e-02, -9.1261e-03,  1.6373e-02],\n",
      "        [-9.5344e-02,  1.7163e-01, -5.5423e-02,  1.4212e-01, -1.9873e-01,\n",
      "          9.2041e-02, -1.3463e-01, -2.6291e-02, -8.1461e-02, -1.8804e-01,\n",
      "          9.0322e-02,  5.7958e-02, -7.5238e-03, -1.6820e-01, -7.8453e-02,\n",
      "         -2.1523e-01, -1.6760e-01, -1.3739e-01,  4.9094e-02, -2.5412e-02,\n",
      "          1.0963e-02,  2.3278e-02, -1.5803e-01,  1.4039e-02, -6.0668e-02,\n",
      "         -1.0952e-01, -8.4367e-02, -6.8259e-03, -2.1541e-02, -8.0506e-02],\n",
      "        [-3.3901e-01, -7.4701e-02, -5.7513e+00,  3.7562e-01, -3.3254e+00,\n",
      "          5.0493e+00, -1.1500e+00, -2.6805e-01, -1.7037e+00, -1.4860e+00,\n",
      "          1.5778e-01, -2.4945e-04, -2.9257e+00,  1.0932e-01,  1.1459e+00,\n",
      "         -3.9869e-01,  9.9603e-01, -1.0919e+00, -9.3285e-01, -6.1237e-01,\n",
      "         -6.0623e-02, -2.0620e-01, -2.4413e-02, -2.7886e+00,  1.3869e+00,\n",
      "         -2.1064e-02, -3.1594e-01, -3.9158e+00, -6.8454e-02, -2.2527e-01],\n",
      "        [ 4.6704e-02, -1.7405e+00,  2.0542e+00,  1.2643e+01, -3.8078e-01,\n",
      "          9.0934e+00,  4.6151e+00,  1.2533e-01, -1.6136e-01,  1.5326e+00,\n",
      "          1.6179e+00,  7.5190e-01, -5.6693e-01, -6.8487e-01,  1.8194e+00,\n",
      "         -2.1781e+00,  5.5152e-01, -1.0863e-01, -4.3245e-01,  1.3417e-01,\n",
      "         -2.1153e+00, -1.8269e+00, -2.2101e+00,  4.5555e+00, -1.7354e+00,\n",
      "         -1.2272e-01,  2.4376e-02,  6.6118e-01,  1.2808e-01,  2.3845e-01],\n",
      "        [ 1.9779e+00, -2.2427e+00, -5.3543e+00, -1.2265e+01,  4.0434e-01,\n",
      "         -2.2391e+00, -3.1258e+00,  1.0268e+00, -3.8420e+00, -8.2207e+00,\n",
      "         -8.7128e-01, -8.0521e-01, -6.2537e-01,  1.6261e+00, -3.4073e+00,\n",
      "         -1.0693e+00,  8.8773e-01,  1.0901e-01,  2.4698e-02,  8.2062e-02,\n",
      "          2.4962e+00,  2.5592e+00,  2.7931e+00,  7.6291e+00,  3.4559e-01,\n",
      "         -2.0682e-01, -1.0292e+00, -7.5071e-01,  4.0086e-01,  3.2336e-01],\n",
      "        [-7.0100e-01, -4.9701e-02, -4.6482e+00,  9.2507e+00,  1.2412e+00,\n",
      "         -4.2161e+00, -5.7975e+00, -1.8301e+00,  1.7943e+00,  9.0227e+00,\n",
      "          8.6315e-01, -7.4263e-02,  2.3423e-02,  6.4283e-01, -2.4541e+00,\n",
      "          8.9484e-01,  3.8284e-01,  7.9335e-01, -2.6839e+00, -3.5585e+00,\n",
      "         -2.4161e+00, -2.3946e+00, -2.4872e+00,  9.7630e-01,  6.8225e-01,\n",
      "          7.7447e-01, -9.8104e-01, -4.0829e+00, -1.0982e+00, -2.2316e+00],\n",
      "        [ 3.8341e+00,  7.4764e-01, -1.4694e+00, -8.9476e+00, -1.3913e+00,\n",
      "         -2.9470e+00,  9.9617e+00,  1.2417e-06, -2.1381e-01, -1.8229e+00,\n",
      "         -3.4246e-01,  4.0231e-01, -5.8231e-01,  5.9671e-01,  1.4292e+00,\n",
      "         -9.8682e-02, -4.7440e-01,  4.5012e-01, -5.9154e-01,  1.0119e-01,\n",
      "         -2.8664e+00, -2.6564e+00, -3.5477e+00,  2.6026e+00, -1.7589e-01,\n",
      "         -8.2260e-02, -1.8224e-01, -5.8092e-01,  3.1211e-01,  3.9116e-01],\n",
      "        [ 4.6094e-01,  5.9138e-01, -2.9621e+00,  1.4899e+01, -2.9037e-01,\n",
      "          1.5953e+01, -1.6216e-01, -8.8201e-01, -1.1324e+00,  2.7232e-01,\n",
      "          5.6895e-01,  2.8260e-01, -6.0003e-01, -1.1021e+00,  1.0754e-01,\n",
      "          5.9831e-01, -3.5127e-02,  5.5799e-02, -3.0859e-01, -1.1355e-02,\n",
      "         -2.9427e-01, -4.5362e-01, -3.7843e-01,  4.6177e+00,  1.8681e-01,\n",
      "          1.1272e-01, -1.2381e+00, -4.3186e-02,  9.5633e-02,  5.8026e-02]],\n",
      "       device='cuda:0')), ('fc1.bias', tensor([ 0.1781, -0.0709, -0.1822, -0.9384,  0.5275,  0.2287,  0.0807, -0.8799,\n",
      "         0.3423, -0.5451, -0.6884,  0.0926, -0.2024,  0.1081,  0.2329, -0.3427,\n",
      "        -0.5616, -0.1812, -0.1950,  0.1007, -0.1017,  0.1017, -0.1907, -0.0758,\n",
      "         0.8936, -0.3112,  0.1314,  1.7040,  0.1324, -0.0062], device='cuda:0')), ('fc2.weight_ih_l0', tensor([[-4.0434, -0.8017,  1.3641,  0.3601, -0.7956,  0.1186,  1.5273, -0.8773,\n",
      "          1.3723, -4.5243, -5.0013,  1.4149, -1.1727, -0.4881, -2.5603,  1.2383,\n",
      "         -1.1523,  0.0375,  0.4051, -1.2479,  0.7405, -1.2457,  0.5408, -0.6504,\n",
      "         -0.1324,  1.6596,  1.7394,  5.2874,  0.0982,  2.1033],\n",
      "        [ 1.5929, -0.0688, -1.0678, -3.1815,  1.3959, -1.1600,  6.2072, -3.4528,\n",
      "         -0.7691, -0.0825, -4.6012,  6.6853, -3.8288, -0.3355,  0.1770,  1.4413,\n",
      "          1.7616, -4.4243, -0.0821,  1.8999, -0.2518,  1.2195, -0.4345, -0.3402,\n",
      "          0.4676,  1.4529, -2.2098, -4.9629,  2.6115,  0.1475],\n",
      "        [ 0.0142, -0.4567, -0.0559, -1.4285, -1.3554, -0.3467,  2.0815, -3.2237,\n",
      "          1.1412, -1.1347,  1.1520,  0.1211, -3.6595,  0.8644, -2.0791,  2.4409,\n",
      "         -3.5256,  0.2078,  0.7056, -0.7262,  0.1341, -1.3845,  0.4894,  0.0992,\n",
      "         -6.6565, -0.4060, -0.5309, -1.1142,  0.3382, -1.0345],\n",
      "        [ 1.5766,  0.2412,  2.9364, -1.9228,  1.9899,  0.3385, -1.0157, -1.2348,\n",
      "          1.6889, -6.0497, -3.5823,  0.3920,  2.2650,  0.9861,  0.9550,  2.6780,\n",
      "         -1.6882,  6.2148,  0.7685,  1.2423, -0.0360,  1.2494, -0.9742,  0.2742,\n",
      "         -0.6280,  1.4364, -2.5547,  3.7521, -4.4628,  0.0232]],\n",
      "       device='cuda:0')), ('fc2.weight_hh_l0', tensor([[ 2.9303],\n",
      "        [-0.4088],\n",
      "        [ 0.9178],\n",
      "        [ 4.6875]], device='cuda:0')), ('fc2.bias_ih_l0', tensor([-1.1747,  0.6764,  2.4921, -0.2056], device='cuda:0')), ('fc2.bias_hh_l0', tensor([ 0.1892, -0.4927,  1.9393, -0.7113], device='cuda:0')), ('fc2.weight_ih_l0_reverse', tensor([[-0.9100, -0.3294, -0.6483,  0.3044,  0.8875, -2.6733,  1.7346, -1.3952,\n",
      "          1.9293, -0.6967,  0.3305,  0.7572, -0.0297, -0.5624, -1.1293, -0.8969,\n",
      "         -0.6115, -0.0796, -0.6968, -0.5846,  0.1605, -0.4410, -0.0921, -0.5421,\n",
      "         -1.6476,  1.1244,  0.2687, -2.8054,  0.7711,  1.5892],\n",
      "        [ 0.2431, -0.2281, -0.5519, -0.6831,  1.6738, -0.0822, -1.6967, -1.2695,\n",
      "         -1.5264, -0.4693, -1.6761, -0.5075,  1.4767,  0.6528,  0.5635,  0.3280,\n",
      "          0.2310, -0.3701,  0.1257,  0.2674, -0.5000,  0.5944,  0.3716,  1.0151,\n",
      "          0.8728, -0.1214, -0.4169,  1.0552, -0.7548, -2.0027],\n",
      "        [ 0.9390, -0.3227,  0.4042,  6.5078, -4.5516,  3.3051, -3.2084,  2.6703,\n",
      "         -1.8343,  0.2714,  3.3269, -2.5049, -0.0316,  0.8291,  0.7163, -1.0802,\n",
      "          0.5002,  0.0210,  0.4939, -2.5300,  0.4705,  0.6290,  0.8438,  0.9011,\n",
      "          2.8955, -0.5172,  3.8548, -5.1938,  1.9800, -4.4297],\n",
      "        [ 4.9804, -0.3954,  0.2065, -6.7345, -2.3180,  7.7397,  2.7119,  0.7734,\n",
      "         -0.9702, -3.0753, -1.9976,  1.9289, -1.3227, -0.8208,  3.3056,  4.8280,\n",
      "          2.7267,  0.5369, -0.9545,  0.3459, -0.5767, -0.8775,  0.7539,  0.0422,\n",
      "         -9.0093,  1.7348,  9.1413,  4.4307,  2.8112, -4.0022]],\n",
      "       device='cuda:0')), ('fc2.weight_hh_l0_reverse', tensor([[ 0.9479],\n",
      "        [ 0.2198],\n",
      "        [-0.7971],\n",
      "        [-6.0757]], device='cuda:0')), ('fc2.bias_ih_l0_reverse', tensor([-0.1404,  0.4109, -0.8771, -0.2987], device='cuda:0')), ('fc2.bias_hh_l0_reverse', tensor([ 0.1308,  0.7718, -1.2960, -0.2202], device='cuda:0')), ('fc2.weight_ih_l1', tensor([[ 0.0147, -2.9985],\n",
      "        [ 0.2724,  1.1377],\n",
      "        [ 3.5559, -1.5958],\n",
      "        [-3.4702,  4.9784]], device='cuda:0')), ('fc2.weight_hh_l1', tensor([[ 1.5450],\n",
      "        [ 1.5873],\n",
      "        [ 0.7581],\n",
      "        [-0.1862]], device='cuda:0')), ('fc2.bias_ih_l1', tensor([-2.2893,  2.0903,  0.0796, -0.6420], device='cuda:0')), ('fc2.bias_hh_l1', tensor([-0.9205,  0.5802,  0.7585,  0.9697], device='cuda:0')), ('fc2.weight_ih_l1_reverse', tensor([[ 2.1764,  1.0054],\n",
      "        [-2.5547, -0.2767],\n",
      "        [ 3.2568, -1.1270],\n",
      "        [ 0.3414,  1.9028]], device='cuda:0')), ('fc2.weight_hh_l1_reverse', tensor([[-3.7122],\n",
      "        [ 2.5972],\n",
      "        [-1.8764],\n",
      "        [-0.7639]], device='cuda:0')), ('fc2.bias_ih_l1_reverse', tensor([-1.0007,  1.0072,  0.4489,  2.3881], device='cuda:0')), ('fc2.bias_hh_l1_reverse', tensor([-0.0700,  1.3341,  0.4822,  2.5063], device='cuda:0')), ('fc2.weight_ih_l2', tensor([[ 0.9128, -2.8041],\n",
      "        [-0.9597,  0.0776],\n",
      "        [ 1.7395, -1.7938],\n",
      "        [ 1.1258, -0.9884]], device='cuda:0')), ('fc2.weight_hh_l2', tensor([[-0.2865],\n",
      "        [ 2.0407],\n",
      "        [-0.3053],\n",
      "        [ 5.4969]], device='cuda:0')), ('fc2.bias_ih_l2', tensor([-1.1802,  0.4225, -0.1575,  0.9285], device='cuda:0')), ('fc2.bias_hh_l2', tensor([-1.3054,  1.6431,  0.4728,  0.2523], device='cuda:0')), ('fc2.weight_ih_l2_reverse', tensor([[-0.6366,  0.9957],\n",
      "        [-3.4073,  0.8770],\n",
      "        [ 4.9058, -1.0582],\n",
      "        [-0.6563, -4.6066]], device='cuda:0')), ('fc2.weight_hh_l2_reverse', tensor([[-1.8662],\n",
      "        [ 4.2367],\n",
      "        [-0.4464],\n",
      "        [ 1.4399]], device='cuda:0')), ('fc2.bias_ih_l2_reverse', tensor([-0.5193,  1.5958,  0.9397,  0.8863], device='cuda:0')), ('fc2.bias_hh_l2_reverse', tensor([-1.1284,  0.9590, -0.5099, -0.1839], device='cuda:0')), ('combination_layer.weight', tensor([[4.0947, 3.7951],\n",
      "        [4.3556, 3.0395]], device='cuda:0')), ('combination_layer.bias', tensor([0.4343, 0.2262], device='cuda:0')), ('projection.weight', tensor([[-1.5973, -1.7090]], device='cuda:0')), ('projection.bias', tensor([0.8865], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(torch.load(f'data/models/{tail}/LSTM_best.pth')['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.12387124449014664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import numpy as np\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(all_y_test, all_test_prob)\n",
    "# Find the optimal threshold\n",
    "optimal_idx = np.argmin(np.sqrt(np.square(1-tpr) + np.square(fpr)))\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.14033539593219757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(all_y_test, all_test_prob)\n",
    "# Add a last threshold corresponding to recall = 0.\n",
    "thresholds = np.append(thresholds, 1)\n",
    "\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "# Find the optimal threshold\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.14\n",
      "Best F1 Score: 0.3928753180661578\n"
     ]
    }
   ],
   "source": [
    "# Convert all_y_test and all_test_prob to numpy arrays for easier manipulation\n",
    "all_y_test = np.array(all_y_test)\n",
    "all_test_prob = np.array(all_test_prob)\n",
    "\n",
    "# Initialize variables to store the best threshold and its corresponding F1 score\n",
    "best_threshold = 0.0\n",
    "best_f1 = 0.0\n",
    "\n",
    "# Iterate over a range of possible threshold values (e.g., 0 to 1, step 0.01)\n",
    "for threshold in np.arange(0.0, 1.01, 0.01):\n",
    "    # Convert probabilities to binary predictions based on the current threshold\n",
    "    predictions = (all_test_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate F1 score for the current threshold\n",
    "    f1 = f1_score(all_y_test, predictions, zero_division=1)\n",
    "    \n",
    "    # Update best threshold and F1 score if the current F1 score is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "# Print the best threshold and its corresponding F1 score\n",
    "print(f\"Best Threshold: {best_threshold}\")\n",
    "print(f\"Best F1 Score: {best_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Training on CPU') # On mac book GPU is not possible =() \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(np.array(train)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    times = math.floor(data.shape[0]/batch_size)\n",
    "    remainder = data.shape[0]%times\n",
    "    a = 0\n",
    "    start = 0\n",
    "    end = start+batch_size\n",
    "    if remainder ==0:\n",
    "        a +=1\n",
    "    while a<times:\n",
    "        temp = pad(data[start:end,],0)\n",
    "        x = torch.from_numpy(temp[:,:,1:-1]).float() # without icustay_id and without aki_stage columns\n",
    "        y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "        X_batches.append(x)\n",
    "        y_batches.append(y)\n",
    "        start = end\n",
    "        end = start+batch_size\n",
    "        a +=1\n",
    "    temp = pad(data[start:data.shape[0]],0)\n",
    "    x = torch.from_numpy(temp[:,:,1:-1]).float()\n",
    "    y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "    X_batches.append(x)\n",
    "    y_batches.append(y)\n",
    "    if len(X_batches) != len(y_batches):\n",
    "        print(\"length error\")\n",
    "    return X_batches, y_batches # arrays\n",
    "\n",
    "# batching\n",
    "X_train, y_train = batch(train, batch_size) # to count weights\n",
    "\n",
    "# counting balance of the classes\n",
    "y = []\n",
    "for i in y_train:\n",
    "    for element in i:\n",
    "        y.append(element.item())\n",
    "\n",
    "#  weights\n",
    "counter=collections.Counter(y)\n",
    "print(counter)\n",
    "zeroes = counter[0]\n",
    "ones = counter[1]\n",
    "\n",
    "X_test, y_test = batch(test, test.shape[0]) \n",
    "X_val, y_val = batch(validation, validation.shape[0])\n",
    "X_val = X_val[0]\n",
    "y_val = y_val[0]\n",
    "X_test = X_test[0]\n",
    "y_test = y_test[0]\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# setup\n",
    "\n",
    "bi_directional = True\n",
    "n_epochs = 50\n",
    "lr = 0.001\n",
    "features = len(X_train[0][0][0])\n",
    "print(features)\n",
    "# features = \n",
    "emb_size = round(features/1)\n",
    "number_layers = 3\n",
    "dropout = 0 # dropout\n",
    "\n",
    "##########################\n",
    "input_size = features\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size \n",
    "        self.output_size = output_size\n",
    "        self.number_layers = number_layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "        # in bidirectional encoder we have  forward and backward hidden states\n",
    "        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "        # Create affine layer to project to the classes \n",
    "        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "        #dropout layer for regularizetion of a sequence\n",
    "        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "        h = self.relu(self.combination_layer(h))\n",
    "        h = self.dropout_layer(h)\n",
    "        h = self.projection(h) \n",
    "        return h\n",
    "\n",
    "#create a network \n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "#print(nn_model)\n",
    "#print(list(nn_model.parameters()))\n",
    "\n",
    "\n",
    "# BCE Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss() # class imbalance\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(round(zeroes/ones,0))) # class imbalance\n",
    "#print(round(zeroes/ones,0))\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=lr) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count unique values\n",
    "unique, counts = np.unique(y_val, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "# Count NaN values\n",
    "nan_count = np.isnan(y_val).sum()\n",
    "print(f\"Number of NaN values: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all nans in X_val with 0\n",
    "X_val[torch.isnan(X_val)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_original = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_original = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training loop (full data 3.5 hours)\n",
    "\n",
    "epochs = n_epochs\n",
    "starttime = datetime.now() # datetime object containing current date and time\n",
    "train_losses, validation_losses = [], []\n",
    "best = 0\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    print (\"\\n Epoch [%d] out of %d\" % (epoch + 1, epochs))\n",
    "    running_loss = 0.0\n",
    "    validation_loss = 0.0\n",
    "    roc_auc = 0.0\n",
    "    pr_auc = 0.0\n",
    "    m = 0\n",
    "    \n",
    "    #train\n",
    "    #print(list(nn_model.parameters())[0])\n",
    "    # pbar = tqdm(X_train, desc=f\"Epoch {epoch+1}\")\n",
    "    # for i in pbar:\n",
    "    #     # zero the parameter gradients\n",
    "    #     optimizer.zero_grad() # zero the gradient buffers not to consider gradients of previous iterations\n",
    "    #     X_batch = X_train[m]\n",
    "    #     y_batch = y_train[m]\n",
    "    #     # print(X_batch.shape)\n",
    "    #     # forward + backward + optimize\n",
    "    #     outputs = nn_model(X_batch)\n",
    "    #     outputs = torch.flatten(outputs)\n",
    "    #     y_batch = y_batch.type_as(outputs)\n",
    "    #     loss = criterion(outputs, y_batch)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step() # Does the update\n",
    "    #     running_loss += loss.item()\n",
    "    #     m +=1\n",
    "    #     pbar.set_postfix({\"Training Loss\": running_loss/len(X_train)})\n",
    "        \n",
    "   \n",
    "    #validation \n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        v_out = nn_model(X_val) \n",
    "        v_out = torch.flatten(v_out) \n",
    "        y_val = y_val.type_as(v_out)\n",
    "        v_loss = criterion(v_out, y_val)\n",
    "        validation_loss = v_loss.item()\n",
    "        # auc and pr auc\n",
    "        val_prob = torch.nn.Sigmoid() (v_out)\n",
    "        print(type(v_out))\n",
    "        print(v_out)\n",
    "        print(val_prob)\n",
    "        print(y_val)\n",
    "        roc_auc = roc_auc_score(y_val,val_prob) \n",
    "        \n",
    "    validation_losses.append(validation_loss) \n",
    "    train_losses.append(running_loss/len(X_train)) \n",
    "    print(f\"Training loss: {running_loss/len(X_train):.3f}.. \" f\"Validation loss: {validation_loss:.3f}.. \")\n",
    "    print(f\"AUC: {roc_auc:.2f}\")  \n",
    "    nn_model.train()\n",
    "    \n",
    "    \n",
    "    if roc_auc > best:\n",
    "        best = roc_auc\n",
    "        PATH = './LSTMbest.pth' \n",
    "        torch.save(nn_model.state_dict(), PATH) # save the model\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "       \n",
    "print('Finished Training')\n",
    "print(\"starttime =\", starttime)\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "PATH = './LSTM.pth' \n",
    "torch.save(nn_model.state_dict(), PATH) # save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "PATH = './LSTM.pth'\n",
    "nn_model.load_state_dict(torch.load(PATH))\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    t_out = nn_model(X_test)\n",
    "    t_out = torch.flatten(t_out)\n",
    "    y_test = y_test.type_as(t_out)\n",
    "    t_loss = criterion(t_out, y_test)\n",
    "    test_loss = t_loss.item()\n",
    "    # auc and pr auc\n",
    "    test_prob = torch.nn.Sigmoid() (t_out)\n",
    "    roc_auc = roc_auc_score(y_test,test_prob) \n",
    "    pr_auc = average_precision_score(y_test,test_prob)\n",
    "    # convert output probabilities to class labels\n",
    "    test_pred = (test_prob > 0.5).float()\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = accuracy_score(y_test.cpu().numpy(), test_pred.cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}.. \" f\"ROC AUC: {roc_auc:.2f}.. \" f\"PR AUC: {pr_auc:.2f}.. \")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a freshly initialized model on test\n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "# nn_model.load_state_dict(torch.load(PATH))\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    t_out = nn_model(X_test)\n",
    "    t_out = torch.flatten(t_out)\n",
    "    y_test = y_test.type_as(t_out)\n",
    "    t_loss = criterion(t_out, y_test)\n",
    "    test_loss = t_loss.item()\n",
    "    # auc and pr auc\n",
    "    test_prob = torch.nn.Sigmoid() (t_out)\n",
    "    roc_auc = roc_auc_score(y_test,test_prob) \n",
    "    pr_auc = average_precision_score(y_test,test_prob)\n",
    "    print(f\"Test loss: {test_loss:.3f}.. \" f\"ROC AUC: {roc_auc:.2f}.. \" f\"PR AUC: {pr_auc:.2f}.. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './i-Bidir_3_lr_0.001_nodropbest.pth'\n",
    "\n",
    "# save the model\n",
    "#torch.save(nn_model.state_dict(), PATH)\n",
    "\n",
    "# code to load saved model\n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "nn_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test) # single batch with zero padding to the max shape 635208"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = nn_model(X_test)\n",
    "pred = torch.nn.Sigmoid() (logits)\n",
    "pred = pred.detach().numpy()\n",
    "pred = pred.reshape(-1,1)\n",
    "print(\"Performance on full X_test where it has no batching: is padded to max dimentions. \\n\")\n",
    "print (\"Area Under ROC Curve: %0.2f\" % roc_auc_score(y_test, pred, average = 'micro')  )\n",
    "brier = round(metrics.brier_score_loss(y_test, pred, sample_weight=None, pos_label=None),3)\n",
    "print(\"Brier score : {:.3f}\".format(brier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('padded_lstm.npy', 'wb') as f:\n",
    "    np.save(f, y_test)\n",
    "    np.save(f, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = X_test.shape[1] #133\n",
    "icustays = X_test.shape[0]\n",
    "times = []\n",
    "auc_s = []\n",
    "t = 0\n",
    "\n",
    "while t < timestamps:\n",
    "    times.append(t+1)\n",
    "    row = t\n",
    "    i = 0\n",
    "    prob_t = []\n",
    "    y_t = []\n",
    "    while i < icustays:\n",
    "        prob_t.append(pred[row])\n",
    "        y_t.append(y_test[row])\n",
    "        row += timestamps\n",
    "        i +=1\n",
    "    prob_t = np.array(prob_t).reshape(-1,1)\n",
    "    y_t = np.array(y_t).reshape(-1,1)\n",
    "    auc_s.append(roc_auc_score(y_t, prob_t, average = 'micro'))\n",
    "    t +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.DataFrame(auc_s, columns = ['AUC'])\n",
    "df['Timestamps'] = times\n",
    "#df[120:133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "sns.lineplot(x=\"Timestamps\", y=\"AUC\", color = 'g',\n",
    "             data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing to LogR, XGB, RF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = batch(test, test.shape[0]) \n",
    "X_test = X_test[0]\n",
    "y_test = y_test[0]\n",
    "\n",
    "\n",
    "def to_one_label (model, label_list,X_test,index_list):\n",
    "    # evaluate on a test set\n",
    "    labels = np.array(label_list)\n",
    "    labels = labels.reshape(-1,1)\n",
    "    labels = labels.astype(int)\n",
    "    logits = model(X_test)\n",
    "    pred = torch.nn.Sigmoid() (logits)\n",
    "    max_rows = pred.shape[1]\n",
    "    predictions = pred.detach().numpy()\n",
    "    predictions = predictions.reshape(-1,1) \n",
    "    # select 1 per icu stay id by index\n",
    "    prob_1_label = []\n",
    "    row = 0\n",
    "    prev = 0\n",
    "    for i in index_list:\n",
    "        prob_1_label.append(predictions[row+i-prev])\n",
    "        row += pred.shape[1]\n",
    "        prev = i\n",
    "    prob_1_label = np.array(prob_1_label).reshape(-1,1)\n",
    "    \n",
    "    return labels, prob_1_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance (y_test, pred_probabilities):\n",
    "    # performance\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    # compute roc auc\n",
    "    roc_auc = roc_auc_score(y_test, pred_probabilities, average = 'micro')\n",
    "    # compute Precision_Recall curves\n",
    "    precision, recall, _ = precision_recall_curve(y_test, pred_probabilities)\n",
    "    # compute PR_AUC\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "       \n",
    "    # I add confusion matrix\n",
    "    optimal_cut_off = round(thresholds[np.argmax(tpr - fpr)],4)\n",
    "    a = np.where(pred_probabilities > optimal_cut_off, 1, 0)\n",
    "    brier = round(metrics.brier_score_loss(y_test, pred_probabilities, sample_weight=None, pos_label=None),3)\n",
    "    predictions = np.where(pred_probabilities > optimal_cut_off, 1, 0)  \n",
    "    \n",
    "    print (\"Area Under ROC Curve: %0.2f\" % roc_auc  )\n",
    "    #print (\"Area Under PR Curve(AP): %0.2f\" % pr_auc  ) \n",
    "    print(\"Brier score : {:.3f}\".format(brier))\n",
    "    #print('Accuracy for Classifier : {:.2f}'.format(accuracy_score(y_test, predictions)))\n",
    "    #print('Cut off: ' + str(optimal_cut_off))\n",
    "    matrix = metrics.confusion_matrix(y_test, a, labels=None, normalize=None)\n",
    "    #print(str(matrix))\n",
    "    \n",
    "    #f.write(\"\\n Area Under ROC Curve: \" +str(roc_auc))\n",
    "    #f.write(\"\\n Area Under PR Curve(AP): \" + str(pr_auc))\n",
    "    #f.write(\"\\n Brier score: \" +str(brier))\n",
    "    #f.write('\\n Accuracy for Classifier '+str(round((accuracy_score(labels, predictions)),3)))\n",
    "    #f.write(\"\\n Cut off: \" +str(optimal_cut_off))\n",
    "    #f.write(str(matrix))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels, prob_1_label = to_one_label (nn_model, label_list,X_test,index_list)\n",
    "performance(labels,prob_1_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save labels, prob_1_label\n",
    "\n",
    "with open('test.npy', 'wb') as f:\n",
    "    #np.save(f, labels)\n",
    "    np.save(f, prob_1_label)\n",
    "with open('test.npy', 'rb') as f:\n",
    "    #lstm_labels = np.load(f)\n",
    "    lstm_prob = np.load(f)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply integrated gradients, we first create an IntegratedGradients object, providing the model object.\n",
    "ig = IntegratedGradients(nn_model)\n",
    "# To compute the integrated gradients, we use the attribute method of the IntegratedGradients object. The method takes\n",
    "# tensor(s) of input examples (matching the forward function of the model), and returns the input attributions for the\n",
    "# given examples. A target index, defining the index of the output for which gradients are computed is 1, \n",
    "# corresponding to AKI (1/0).\n",
    "\n",
    "#The input tensor provided should require grad, so we call requires_grad_ on the tensor. The attribute method also \n",
    "# takes a baseline, which is the starting point from which gradients are integrated. The default value is just the \n",
    "# 0 tensor, which is a reasonable baseline / default for this task.\n",
    "\n",
    "#The returned values of the attribute method are the attributions, which match the size of the given inputs, and delta,\n",
    "# which approximates the error between the approximated integral and true integral.\n",
    "print(datetime.now())\n",
    "X_test.requires_grad_()\n",
    "attr, delta = ig.attribute(X_test,target=1, return_convergence_delta=True)\n",
    "attr = attr.detach().numpy()\n",
    "attr= np.reshape(attr,(-1,35))\n",
    "importances = np.mean(attr, axis=0)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[:,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[:,4].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_importances(feature_names, importances, title=\"LSTM Average Feature Importances\", axis_title=\"Features\"):\n",
    "    print(title)\n",
    "    i = 0\n",
    "    while i < features:\n",
    "        print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "        i +=1\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    \n",
    "visualize_feature_importances(feature_names, importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df =  pd.DataFrame(importances, columns = ['Feature Importance'])\n",
    "lstm_df['Features'] = feature_names\n",
    "lstm_df = lstm_df.sort_values(by = ['Feature Importance'], ascending = False, ignore_index = True)\n",
    "#lstm_df[\"Feature Importance\"] =  lstm_df[\"Feature Importance\"]\n",
    "#lstm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df[\"Feature Importance\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df)\n",
    "ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df, color = 'grey')\n",
    "ax.set_xlabel('Feature Importance', fontsize = 15)\n",
    "ax.set_ylabel(\"Features\",fontsize=15)\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 6)\n",
    "plt.title('LSTM feature Importances')\n",
    "plt.savefig('LSTM_feature_importance_grey.png', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df['abs'] = abs(lstm_df['Feature Importance'])\n",
    "lstm_df = lstm_df.sort_values(by = ['abs'], ascending = False, ignore_index = True)\n",
    "lstm_df_10 = lstm_df.head(10)\n",
    "#lstm_df_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df_10, palette=\"mako\")\n",
    "\n",
    "ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df_10, color = 'darkgreen')\n",
    "ax.set_xlabel('Feature Importance', fontsize = 15)\n",
    "ax.set_ylabel(\"Features\",fontsize=15)\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 10)\n",
    "plt.title('LSTM top 10 features by feature importance')\n",
    "plt.savefig('LSTM_top10_feature_importance_darkgreen.png', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graphs (y_test,pred_probabilities, classifier_name, plot_name, algorithm):\n",
    "    \n",
    "    def bin_total(y_true, y_prob, n_bins):\n",
    "        bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "        # In sklearn.calibration.calibration_curve, the last value in the array is always 0.\n",
    "        binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "        return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "    def missing_bin(bin_array):\n",
    "        midpoint = \" \"    \n",
    "        if bin_array[0]==0:\n",
    "            midpoint = \"5%, \"\n",
    "        if bin_array[1]==0:\n",
    "            midpoint = midpoint + \"15%, \"\n",
    "        if bin_array[2]==0:\n",
    "            midpoint = midpoint + \"25%, \"\n",
    "        if bin_array[3]==0:\n",
    "            midpoint = midpoint + \"35%, \" \n",
    "        if bin_array[4]==0:\n",
    "            midpoint = midpoint + \"45%, \"\n",
    "        if bin_array[5]==0:\n",
    "            midpoint = midpoint + \"55%, \"\n",
    "        if bin_array[6]==0:\n",
    "            midpoint = midpoint + \"65%, \"\n",
    "        if bin_array[7]==0:\n",
    "            midpoint = midpoint + \"75%, \"\n",
    "        if bin_array[8]==0:\n",
    "            midpoint = midpoint + \"85%, \"\n",
    "        if bin_array[9]==0:\n",
    "            midpoint = midpoint + \"95%, \"\n",
    "        return \"The missing bins have midpoint values of \"+ str(midpoint)\n",
    "    \n",
    "    # performance\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    # compute roc auc\n",
    "    roc_auc = roc_auc_score(y_test, pred_probabilities, average = 'micro')\n",
    "    # compute Precision_Recall curves\n",
    "    precision, recall, _ = precision_recall_curve(y_test, pred_probabilities)\n",
    "    # compute PR_AUC\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "\n",
    "    # compute calibration curve\n",
    "    LR_y, LR_x = calibration_curve(y_test, pred_probabilities, n_bins=10)\n",
    "    #find out which one are the missing bins\n",
    "    bin_array = bin_total(y_test, pred_probabilities , n_bins=10)\n",
    "    print(missing_bin(bin_array))\n",
    "\n",
    "    print(\"plot curves and save in one png file\")\n",
    "    #save three plots in one png file\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, figsize=(7, 24))\n",
    "    fig.subplots_adjust(wspace=0.3, hspace= 0.3)\n",
    "    fig.suptitle('Evaluation of '+ plot_name)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    \n",
    "    # plot roc curve\n",
    "    ax1.plot(fpr, tpr,'C2', label=algorithm+\" \"+\"Classifier \" + str(classifier_name) + \", auc=\" +str(round(roc_auc,2)))\n",
    "    ax1.title.set_text('ROC AUC')\n",
    "    ax1.set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot PR curve\n",
    "    ax2.plot(recall, precision,'C2', label=algorithm+\" \"+\"Classifier \" + str(classifier_name) + \", auc=\"+str(round(pr_auc,2)))\n",
    "    ax2.title.set_text('PR AUC')\n",
    "    ax2.set(xlabel='Recall', ylabel='Precision')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot calibration curve\n",
    "    ax3.plot(LR_x, LR_y, 'C2',marker='o', linewidth=1, label='LR')\n",
    "    line = mlines.Line2D([0, 1], [0, 1], color='black')\n",
    "    transform = ax3.transAxes\n",
    "    line.set_transform(transform)\n",
    "    ax3.add_line(line)\n",
    "    ax3.title.set_text('Calibration plot for '+str(plot_name))\n",
    "    ax3.set(xlabel= 'Predicted probability', ylabel= 'True probability in each bin')\n",
    "    ax3.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.savefig(plot_name+\".png\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(pred_probabilities, y_test, dist_name):\n",
    "    #probabilities distributions graphs\n",
    "    true_1 = pd.DataFrame(pred_probabilities, columns=['Predicted probabilities'])\n",
    "    true_1['labels'] = y_test.tolist()\n",
    "    true_0 = true_1.copy(deep = True) \n",
    "    indexNames = true_1[true_1['labels'] == 0].index\n",
    "    true_1.drop(indexNames , inplace=True)\n",
    "    indexNames = true_0[ true_0['labels'] == 1 ].index\n",
    "    true_0.drop(indexNames , inplace=True)\n",
    "    true_1.drop(columns=['labels'], inplace = True)\n",
    "    true_0.drop(columns=['labels'], inplace = True)\n",
    "    \n",
    "    sns.distplot(true_1['Predicted probabilities'], hist = False, kde = True,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3,\"color\": \"g\"}, label = 'Class 1')\n",
    "    plt.ylabel('Density')\n",
    "    sns.distplot(true_0['Predicted probabilities'], hist = False, kde = True,\n",
    "                     kde_kws = {'shade': True, 'linewidth': 3}, label = 'Class 0')\n",
    "    plt.title('Density Plot'+ dist_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distribution(prob_1_label, labels.flatten(), \" Bidirectional LSTM no imputation \")\n",
    "plt.savefig('dist_LSTM_bi_NOimp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"None vs. Any AKI\"    ###change every time #Moderate vs. Severe #None vs. Any #Others vs. Severe\n",
    "plot_name = \"LSTM NO imputation\"\n",
    "build_graphs(labels.flatten(), prob_1_label.flatten(), classifier_name, plot_name, \"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(labels, prob_1_label)\n",
    "fpr, tpr, thresholds = roc_curve(labels, prob_1_label)\n",
    "optimal_cut_off = round(thresholds[np.argmax(tpr - fpr)],2)\n",
    "prediction = np.where(prob_1_label > optimal_cut_off, 1, 0)\n",
    "f1 = f1_score(labels,prediction)\n",
    "prauc =auc(recall, precision)\n",
    "print('F1 = %.3f, PR auc =%.3f' % (f1,prauc))\n",
    "\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(labels[labels==1]) / len(labels)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall,precision, marker='.', label='LSTM')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search grid \n",
    "layers = [1,2,3]\n",
    "l_rate = [0.001, 0.0001]\n",
    "drop = [0,0.2]\n",
    "bidirectionality = [True,False]\n",
    "#loops count\n",
    "hypercount = 0\n",
    "# static parameters\n",
    "n_epochs = 80\n",
    "emb_size = round(features/1)\n",
    "input_size = features\n",
    "output_size = 1\n",
    "###############################\n",
    "\n",
    "f = open('lstm_no_imp_uni.txt', 'w+') #change with or without imp\n",
    "\n",
    "for q1 in bidirectionality:\n",
    "    for q2 in layers:\n",
    "        for q3 in drop:\n",
    "            for q4 in l_rate:\n",
    "                hypercount +=1\n",
    "                name = \"i-Bidir_\" if q1 else \"i-Onedir_\"\n",
    "                name = name+str(q2) + \"_lr_\"+str(q4)\n",
    "                name = name+\"_drop\"+str(q3) if q3 == 0.2 else name+\"_nodrop\"\n",
    "                #set parameters\n",
    "                bi_directional = q1\n",
    "                lr = q4\n",
    "                number_layers = q2\n",
    "                dropout = q3 # dropout\n",
    "                print('hypercount: %d' % hypercount)\n",
    "                print('\\n')\n",
    "                print(name)\n",
    "                f.write('\\n\\n' + str(name)+ '\\n\\n')\n",
    "                    \n",
    "                # create the NN\n",
    "                class Net(nn.Module):\n",
    "                    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "                        super(Net, self).__init__()\n",
    "                        self.input_size = input_size\n",
    "                        self.emb_size = emb_size \n",
    "                        self.output_size = output_size\n",
    "                        self.number_layers = number_layers\n",
    "                        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "                        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "                        # in bidirectional encoder we have  forward and backward hidden states\n",
    "                        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "                        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "                        # Create affine layer to project to the classes \n",
    "                        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "                        #dropout layer for regularizetion of a sequence\n",
    "                        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "                        self.relu = nn.ReLU()\n",
    "\n",
    "                    def forward(self, x):\n",
    "                        h = self.relu(self.fc1(x))\n",
    "                        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "                        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "                        h = self.relu(self.combination_layer(h))\n",
    "                        h = self.dropout_layer(h)\n",
    "                        h = self.projection(h) \n",
    "                        return h\n",
    "\n",
    "                #create a network \n",
    "                nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "                print(nn_model)\n",
    "                #print(list(nn_model.parameters()))\n",
    "                \n",
    "                # BCE Loss and optimizer\n",
    "                criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(round(zeroes/ones,0))) # class imbalance\n",
    "                #print(round(zeroes/ones,0))\n",
    "                optimizer = optim.Adam(nn_model.parameters(), lr=lr) \n",
    "    \n",
    "    \n",
    "                # TRAINING LOOP \n",
    "                epochs = n_epochs\n",
    "                starttime = datetime.now() # datetime object containing current date and time\n",
    "                train_losses, validation_losses = [], []\n",
    "                best = 0\n",
    "                patience = 0\n",
    "                old_auc = 0\n",
    "                old_pr = 0\n",
    "\n",
    "                for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "                    print (\"\\n Epoch [%d] out of %d\" % (epoch + 1, epochs))\n",
    "                    running_loss = 0.0\n",
    "                    validation_loss = 0.0\n",
    "                    roc_auc = 0.0\n",
    "                    pr_auc = 0.0\n",
    "                    m = 0\n",
    "                    \n",
    "                    #train\n",
    "                    #print(list(nn_model.parameters())[0])\n",
    "                    for i in X_train:\n",
    "                        # zero the parameter gradients\n",
    "                        optimizer.zero_grad() # zero the gradient buffers not to consider gradients of previous iterations\n",
    "                        X_batch = X_train[m]\n",
    "                        y_batch = y_train[m]\n",
    "                        # forward + backward + optimize\n",
    "                        outputs = nn_model(X_batch)\n",
    "                        outputs = torch.flatten(outputs)\n",
    "                        y_batch = y_batch.type_as(outputs)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                        loss.backward()\n",
    "                        optimizer.step() # Does the update\n",
    "                        running_loss += loss.item()\n",
    "                        m +=1\n",
    "                    #validation \n",
    "                    nn_model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        v_out = nn_model(X_val) \n",
    "                        v_out = torch.flatten(v_out) \n",
    "                        y_val = y_val.type_as(v_out)\n",
    "                        v_loss = criterion(v_out, y_val)\n",
    "                        validation_loss = v_loss.item()\n",
    "                        # auc and pr auc\n",
    "                        val_prob = torch.nn.Sigmoid() (v_out)\n",
    "                        precision, recall, thresholds = precision_recall_curve(y_val, val_prob)\n",
    "                        pr_auc = auc(recall, precision)\n",
    "                        roc_auc = roc_auc_score(y_val,val_prob) \n",
    "\n",
    "                    validation_losses.append(validation_loss) \n",
    "                    train_losses.append(running_loss/len(X_train)) \n",
    "                    print(f\"Training loss: {running_loss/len(X_train):.3f}.. \" f\"Validation loss: {validation_loss:.3f}.. \")\n",
    "                    print(f\"AUC: {roc_auc:.2f} \" f\"PR AUC: {pr_auc:.2f} \")  \n",
    "                    nn_model.train()\n",
    "\n",
    "                    \n",
    "                    if roc_auc > best:\n",
    "                        best = roc_auc\n",
    "                        PATH1 = './'+str(name)+'best.pth' \n",
    "                        torch.save(nn_model.state_dict(), PATH1) # save the model\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    if roc_auc == old_auc and pr_auc==old_pr:\n",
    "                        patience +=1\n",
    "                    old_auc = roc_auc\n",
    "                    old_pr = pr_auc\n",
    "                    if patience ==10:\n",
    "                        print(\"out of patience\")\n",
    "                        break\n",
    "\n",
    "                print('\\n Finished Training')\n",
    "                print(\"starttime =\", starttime)\n",
    "                now = datetime.now()\n",
    "                print(\"endtime =\", now)\n",
    "                # end of training loop\n",
    "                \n",
    "                PATH2 = './'+str(name)+'last.pth' \n",
    "                torch.save(nn_model.state_dict(), PATH2) # save the model\n",
    "                print('\\n Last model \\n')\n",
    "                labels, probs = to_one_label(nn_model,label_list,X_test,index_list)\n",
    "                performance (nn_model, labels, probs)\n",
    "                \n",
    "                #load the best model\n",
    "                best_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "                best_model.load_state_dict(torch.load(PATH1))\n",
    "                print('\\n Best model \\n')\n",
    "                labels, probs = to_one_label(best_model,label_list,X_test,index_list)\n",
    "                performance (best_model, labels, probs)\n",
    "f.close() \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
