{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-17 18:43:35.346931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jori152b/.conda/envs/medinf_cuda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "#data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "# NN\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score, roc_auc_score, auc, accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from captum.attr import IntegratedGradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Training on CPU') # On mac book GPU is not possible =() \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter as constant \n",
    "\n",
    "TESTING = False \n",
    "TEST_SIZE = 0.05\n",
    "\n",
    "SPLIT_SIZE = 0.2 \n",
    "\n",
    "NORMALIZATION = 'min-max' \n",
    "\n",
    "CAPPING_THRESHOLD_UPPER = 0.99\n",
    "CAPPING_THRESHOLD_LOWER = 0.01\n",
    "\n",
    "# How much time the prediction should occur (hours)\n",
    "HOURS_AHEAD = 48\n",
    "\n",
    "NORM_TYPE = 'min_max'\n",
    "\n",
    "RANDOM = 42\n",
    "\n",
    "# LSTM\n",
    "batch_size = 5\n",
    "\n",
    "# naming model and plot\n",
    "classifier_name = \"None vs. Any AKI\"    ###change every time #Moderate vs. Severe #None vs. Any #Others vs. Severe\n",
    "plot_name = \"adult_AnyAKI_LR\"    ###change every time\n",
    "\n",
    "base_data_path=r\"/home/jori152b/DIR/horse/jori152b-medinf/KP_MedInf/model_development/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions used later\n",
    "\n",
    "def cap_data(df):\n",
    "    print(\"Capping between the {} and {} quantile\".format(CAPPING_THRESHOLD_LOWER, CAPPING_THRESHOLD_UPPER))\n",
    "    cap_mask = df.columns.difference(['icustay_id', 'charttime', 'aki_stage'])\n",
    "    df[cap_mask] = df[cap_mask].clip(df[cap_mask].quantile(CAPPING_THRESHOLD_LOWER),\n",
    "                                     df[cap_mask].quantile(CAPPING_THRESHOLD_UPPER),\n",
    "                                     axis=1)\n",
    "\n",
    "    return df\n",
    " \n",
    "    \n",
    "def normalise_data(df, norm_mask):\n",
    "    print(\"Normalizing in [0,1] with {} normalization\".format(NORMALIZATION))\n",
    "    \n",
    "    min_values = df[norm_mask].min()\n",
    "    max_values = df[norm_mask].max()\n",
    "    \n",
    "    # Skip normalization for constant columns\n",
    "    for column in norm_mask:\n",
    "        if min_values[column] != max_values[column]:\n",
    "            df[column] = (df[column] - min_values[column]) / (max_values[column] - min_values[column])\n",
    "    \n",
    "    normalization_parameters = {column: {'min': min_values[column], 'max': max_values[column]} for column in norm_mask}\n",
    "    \n",
    "    return df, normalization_parameters\n",
    "\n",
    "\n",
    "# impute missing value in resampleing data with most common based on each id\n",
    "def fast_mode(df, key_cols, value_col):\n",
    "    \"\"\" Calculate a column mode, by group, ignoring null values. \n",
    "    \n",
    "    key_cols : list of str - Columns to groupby for calculation of mode.\n",
    "    value_col : str - Column for which to calculate the mode. \n",
    "\n",
    "    Return\n",
    "    pandas.DataFrame\n",
    "        One row for the mode of value_col per key_cols group. If ties, returns the one which is sorted first. \"\"\"\n",
    "    return (df.groupby(key_cols + [value_col]).size() \n",
    "              .to_frame('counts').reset_index() \n",
    "              .sort_values('counts', ascending=False) \n",
    "              .drop_duplicates(subset=key_cols)).drop('counts',axis=1)\n",
    "\n",
    "\n",
    "#get max shape of 3d array\n",
    "def get_dimensions(array, level=0):   \n",
    "    yield level, len(array)\n",
    "    try:\n",
    "        for row in array:\n",
    "            yield from get_dimensions(row, level + 1)\n",
    "    except TypeError: #not an iterable\n",
    "        pass\n",
    "\n",
    "def get_max_shape(array):\n",
    "    dimensions = defaultdict(int)\n",
    "    for level, length in get_dimensions(array):\n",
    "        dimensions[level] = max(dimensions[level], length)\n",
    "    return [value for _, value in sorted(dimensions.items())]\n",
    "\n",
    "#pad the ragged 3d array to rectangular shape based on max size\n",
    "def iterate_nested_array(array, index=()):\n",
    "    try:\n",
    "        for idx, row in enumerate(array):\n",
    "            yield from iterate_nested_array(row, (*index, idx)) \n",
    "    except TypeError: # final level            \n",
    "        yield (*index, slice(len(array))), array # think of the types\n",
    "\n",
    "def pad(array, fill_value):\n",
    "    dimensions = get_max_shape(array)\n",
    "    result = np.full(dimensions, fill_value, dtype = np.float64)  \n",
    "    for index, value in iterate_nested_array(array):\n",
    "        result[index] = value \n",
    "    return result\n",
    "\n",
    "def bin_total(y_true, y_prob, n_bins):\n",
    "    bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "    # In sklearn.calibration.calibration_curve,\n",
    "    # the last value in the array is always 0.\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "def missing_bin(bin_array):\n",
    "    midpoint = \" \"    \n",
    "    if bin_array[0]==0:\n",
    "        midpoint = \"5%, \"\n",
    "    if bin_array[1]==0:\n",
    "        midpoint = midpoint + \"15%, \"\n",
    "    if bin_array[2]==0:\n",
    "        midpoint = midpoint + \"25%, \"\n",
    "    if bin_array[3]==0:\n",
    "        midpoint = midpoint + \"35%, \" \n",
    "    if bin_array[4]==0:\n",
    "        midpoint = midpoint + \"45%, \"\n",
    "    if bin_array[5]==0:\n",
    "        midpoint = midpoint + \"55%, \"\n",
    "    if bin_array[6]==0:\n",
    "        midpoint = midpoint + \"65%, \"\n",
    "    if bin_array[7]==0:\n",
    "        midpoint = midpoint + \"75%, \"\n",
    "    if bin_array[8]==0:\n",
    "        midpoint = midpoint + \"85%, \"\n",
    "    if bin_array[9]==0:\n",
    "        midpoint = midpoint + \"95%, \"\n",
    "    return \"The missing bins have midpoint values of \"+ str(midpoint)\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    times = math.floor(data.shape[0]/batch_size)\n",
    "    remainder = data.shape[0]%times\n",
    "    a = 0\n",
    "    start = 0\n",
    "    end = start+batch_size\n",
    "    if remainder ==0:\n",
    "        a +=1\n",
    "    while a<times:\n",
    "        temp = pad(data[start:end,],0)\n",
    "        x = torch.from_numpy(temp[:,:,1:-1]).float() # without icustay_id and without aki_stage columns\n",
    "        y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "        X_batches.append(x)\n",
    "        y_batches.append(y)\n",
    "        start = end\n",
    "        end = start+batch_size\n",
    "        a +=1\n",
    "    temp = pad(data[start:data.shape[0]],0)\n",
    "    x = torch.from_numpy(temp[:,:,1:-1]).float()\n",
    "    y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "    X_batches.append(x)\n",
    "    y_batches.append(y)\n",
    "    if len(X_batches) != len(y_batches):\n",
    "        print(\"length error\")\n",
    "    return X_batches, y_batches # arrays\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size \n",
    "        self.output_size = output_size\n",
    "        self.number_layers = number_layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "        # in bidirectional encoder we have  forward and backward hidden states\n",
    "        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "        # Create affine layer to project to the classes \n",
    "        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "        #dropout layer for regularizetion of a sequence\n",
    "        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "        h = self.relu(self.combination_layer(h))\n",
    "        h = self.dropout_layer(h)\n",
    "        h = self.projection(h) \n",
    "        return h\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: load best features\n",
    "optimal_features = np.load(\"data/optimal_features.npy\", allow_pickle=True)\n",
    "# Extracting feature names (keys) from optimal_features\n",
    "optimal_feature_names = [feature[0] for feature in optimal_features]\n",
    "# include also aki_stage and icustay_id\n",
    "optimal_feature_names.extend(['aki_stage', 'icustay_id', 'charttime'])\n",
    "print(optimal_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original = pd.read_csv(\"data/preprocessed/preprocessed_data_6H.csv\")\n",
    "original = pd.read_csv(os.path.join(base_data_path, \"resampled\", \"aki_stage_X_original_24H.csv\"))\n",
    "extended = pd.read_csv(os.path.join(base_data_path, \"resampled\", \"aki_stage_X_extended_24H.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(original.columns))\n",
    "print(len(extended.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extended.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import os\n",
    "import gc\n",
    "\n",
    "results = {}\n",
    "\n",
    "datasets = [\n",
    "    \"aki_stage_X_extended_6H.csv\",\n",
    "              ]\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    tail = dataset\n",
    "    data_path = os.path.join(base_data_path,\"resampled\", dataset)\n",
    "    X = pd.read_csv(data_path)\n",
    "    # take only head \n",
    "    # X = X.head(10000)\n",
    "\n",
    "    # For training a testing model, take only icu_stay_id, charttime,creatinine_mean,uo_rt_6hr,aki_stage\n",
    "    # X = X[['icustay_id', 'charttime', 'creatinine_mean', 'uo_rt_6hr', 'aki_stage']]\n",
    "\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage',)\n",
    "    numeric_feat.remove('icustay_id',)\n",
    "\n",
    "\n",
    "    # normalize data and cap features\n",
    "    # X = cap_data(X)\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    \n",
    "    print(len(X.columns))\n",
    "    print(X.columns)\n",
    "\n",
    "    # X = X.sort_values(by=['icustay_id', 'charttime'])\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    seq_lengths = X.groupby(['icustay_id'],as_index=False).size().sort_values(by = ['size'],ascending=False)\n",
    "    sequence_length = seq_lengths.max() # the longest sequence per icustay-id\n",
    "\n",
    "    #AL re-write as try except to make it work as hadm_id is not used if only one csv file is used and none are merged\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    id_list = X['icustay_id'].unique()\n",
    "    # take the common id list defined earlier\n",
    "    # id_list = common_id_list\n",
    "    \n",
    "    id_train, id_test_val = train_test_split(id_list, test_size = SPLIT_SIZE, random_state = 42) # train set is 80%)\n",
    "    # remaining 20% split in halves as test and validation 10% and 10%\n",
    "    id_valid, id_test = train_test_split(id_test_val, test_size = 0.5, random_state = 42) # test 10% valid 10%\n",
    "\n",
    "    # move (\"aki_stage\") to last column\n",
    "    X = X.reindex(columns = [col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "    test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index = True) \n",
    "    validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id']) \n",
    "\n",
    "    test = test.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "    train = train.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "    validation = validation.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "\n",
    "    train.drop(['charttime'], axis=1, inplace = True)  \n",
    "    test.drop(['charttime'], axis=1, inplace = True)\n",
    "    validation.drop(['charttime'], axis=1, inplace = True)\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "\n",
    "\n",
    "    # flatten the train, test and validation data\n",
    "    train_flat = np.concatenate(train, axis=0)\n",
    "    test_flat = np.concatenate(test, axis=0)\n",
    "    validation_flat = np.concatenate(validation, axis=0)\n",
    "\n",
    "    # get the labels\n",
    "    train_labels = np.array([x[-1] for x in train_flat])\n",
    "    test_labels = np.array([x[-1] for x in test_flat])\n",
    "    validation_labels = np.array([x[-1] for x in validation_flat])\n",
    "\n",
    "    # get the features\n",
    "    train_features = np.array([x[1:-1] for x in train_flat])\n",
    "    validation_features = np.array([x[1:-1] for x in validation_flat])\n",
    "    test_features = np.array([x[1:-1] for x in test_flat])\n",
    "\n",
    "    # create the XGBoost classifier\n",
    "    xgb = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM)\n",
    "\n",
    "    # train the classifier\n",
    "    xgb.fit(train_features, train_labels)\n",
    "\n",
    "    # get the predictions\n",
    "    train_predictions = xgb.predict(train_features)\n",
    "    test_predictions = xgb.predict(test_features)\n",
    "    validation_predictions = xgb.predict(validation_features)\n",
    "\n",
    "    # get the accuracy\n",
    "    train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "    test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    validation_accuracy = accuracy_score(validation_labels, validation_predictions)\n",
    "\n",
    "    # get the probabilities of the positive class\n",
    "    training_prob = xgb.predict_proba([x[1:-1] for x in train_flat])[:, 1]\n",
    "    test_prob = xgb.predict_proba([x[1:-1] for x in test_flat])[:, 1]\n",
    "    validation_prob = xgb.predict_proba([x[1:-1] for x in validation_flat])[:, 1]\n",
    "\n",
    "    # calculate ROC AUC and PR AUC for the training set\n",
    "    training_roc_auc = roc_auc_score(train_labels, training_prob)\n",
    "    training_pr_auc = average_precision_score(train_labels, training_prob)\n",
    "\n",
    "    # calculate ROC AUC and PR AUC for the test set\n",
    "    test_roc_auc = roc_auc_score(test_labels, test_prob)\n",
    "    test_pr_auc = average_precision_score(test_labels, test_prob)\n",
    "\n",
    "    # calculate ROC AUC and PR AUC for the validation set\n",
    "    validation_roc_auc = roc_auc_score(validation_labels, validation_prob)\n",
    "    validation_pr_auc = average_precision_score(validation_labels, validation_prob)\n",
    "    \n",
    "    print(f\"Results for {tail}\")\n",
    "    print(f\"Train accuracy: {train_accuracy:.3f}.. Train ROC AUC: {training_roc_auc:.2f}.. Train PR AUC: {training_pr_auc:.2f}..\")\n",
    "    print(f\"Test accuracy: {test_accuracy:.3f}.. Test ROC AUC: {test_roc_auc:.2f}.. Test PR AUC: {test_pr_auc:.2f}..\")\n",
    "    print(f\"Validation accuracy: {validation_accuracy:.3f}.. Validation ROC AUC: {validation_roc_auc:.2f}.. Validation PR AUC: {validation_pr_auc:.2f}..\")\n",
    "\n",
    "    now = datetime.now()\n",
    "    out_path = os.path.join(base_data_path, \"models\", f\"{tail}_{now.strftime('%Y%m%d%H%M%S')}\")\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    # save the xgb model\n",
    "    xgb.save_model(f'{out_path}/xgb.model')\n",
    "    # save normalization parameters\n",
    "    try:\n",
    "        np.save(f'{out_path}/normalization_parameters.npy', normalization_parameters)\n",
    "    except:\n",
    "        pass\n",
    "    # save the train feature names\n",
    "    np.save(f'{out_path}/train_feature_names.npy', X.columns[2:-1])\n",
    "\n",
    "    results[tail] = {'train_accuracy': train_accuracy, 'test_accuracy': test_accuracy, 'validation_accuracy': validation_accuracy,\n",
    "                                'train_roc_auc': training_roc_auc, 'test_roc_auc': test_roc_auc, 'validation_roc_auc': validation_roc_auc,\n",
    "                                'train_pr_auc': training_pr_auc, 'test_pr_auc': test_pr_auc, 'validation_pr_auc': validation_pr_auc}\n",
    "\n",
    "    # save results dict\n",
    "    np.save(f'{out_path}/results.npy', results)\n",
    "    \n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter search (grid search)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "results = {}\n",
    "\n",
    "datasets = [\n",
    "    \"aki_stage_X_extended_6H.csv\",\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "\n",
    "    tail = dataset\n",
    "    data_path = os.path.join(base_data_path,\"resampled\", dataset)\n",
    "    X = pd.read_csv(data_path)\n",
    "    # take only head \n",
    "    # X = X.head(10000)\n",
    "\n",
    "    # For training a testing model, take only icu_stay_id, charttime,creatinine_mean,uo_rt_6hr,aki_stage\n",
    "    # X = X[['icustay_id', 'charttime', 'creatinine_mean', 'uo_rt_6hr', 'aki_stage']]\n",
    "\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage',)\n",
    "    numeric_feat.remove('icustay_id',)\n",
    "\n",
    "\n",
    "    # normalize data and cap features\n",
    "    # X = cap_data(X)\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    \n",
    "    print(len(X.columns))\n",
    "    print(X.columns)\n",
    "\n",
    "    # X = X.sort_values(by=['icustay_id', 'charttime'])\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    seq_lengths = X.groupby(['icustay_id'],as_index=False).size().sort_values(by = ['size'],ascending=False)\n",
    "    sequence_length = seq_lengths.max() # the longest sequence per icustay-id\n",
    "\n",
    "    #AL re-write as try except to make it work as hadm_id is not used if only one csv file is used and none are merged\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    id_list = X['icustay_id'].unique()\n",
    "    # take the common id list defined earlier\n",
    "    # id_list = common_id_list\n",
    "    \n",
    "    id_train, id_test_val = train_test_split(id_list, test_size = SPLIT_SIZE, random_state = 42) # train set is 80%)\n",
    "    # remaining 20% split in halves as test and validation 10% and 10%\n",
    "    id_valid, id_test = train_test_split(id_test_val, test_size = 0.5, random_state = 42) # test 10% valid 10%\n",
    "\n",
    "    # move (\"aki_stage\") to last column\n",
    "    X = X.reindex(columns = [col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "    test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index = True) \n",
    "    validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id']) \n",
    "\n",
    "    test = test.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "    train = train.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "    validation = validation.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "\n",
    "    train.drop(['charttime'], axis=1, inplace = True)  \n",
    "    test.drop(['charttime'], axis=1, inplace = True)\n",
    "    validation.drop(['charttime'], axis=1, inplace = True)\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "\n",
    "\n",
    "    # flatten the train, test and validation data\n",
    "    train_flat = np.concatenate(train, axis=0)\n",
    "    test_flat = np.concatenate(test, axis=0)\n",
    "    validation_flat = np.concatenate(validation, axis=0)\n",
    "\n",
    "    # get the labels\n",
    "    train_labels = np.array([x[-1] for x in train_flat])\n",
    "    test_labels = np.array([x[-1] for x in test_flat])\n",
    "    validation_labels = np.array([x[-1] for x in validation_flat])\n",
    "\n",
    "    # get the features\n",
    "    train_features = np.array([x[1:-1] for x in train_flat])\n",
    "    validation_features = np.array([x[1:-1] for x in validation_flat])\n",
    "    test_features = np.array([x[1:-1] for x in test_flat])\n",
    "    \n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    # Create the base model\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM)\n",
    "    \n",
    "    # Instantiate GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    \n",
    "    # Perform grid search on the training data\n",
    "    grid_search.fit(train_features, train_labels)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_xgb = grid_search.best_estimator_\n",
    "    \n",
    "    # Print the best parameters\n",
    "    print(\"Best parameters:\", grid_search.best_params_)\n",
    "    \n",
    "    # # Use the best model for predictions\n",
    "    # train_predictions = best_xgb.predict(train_features)\n",
    "    # test_predictions = best_xgb.predict(test_features)\n",
    "    # validation_predictions = best_xgb.predict(validation_features)\n",
    "    \n",
    "    # # Calculate accuracies\n",
    "    # train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "    # test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    # validation_accuracy = accuracy_score(validation_labels, validation_predictions)\n",
    "    \n",
    "    # # Get probabilities\n",
    "    # training_prob = best_xgb.predict_proba(train_features)[:, 1]\n",
    "    # test_prob = best_xgb.predict_proba(test_features)[:, 1]\n",
    "    # validation_prob = best_xgb.predict_proba(validation_features)[:, 1]\n",
    "    \n",
    "    # # Calculate ROC AUC and PR AUC\n",
    "    # training_roc_auc = roc_auc_score(train_labels, training_prob)\n",
    "    # training_pr_auc = average_precision_score(train_labels, training_prob)\n",
    "    # test_roc_auc = roc_auc_score(test_labels, test_prob)\n",
    "    # test_pr_auc = average_precision_score(test_labels, test_prob)\n",
    "    # validation_roc_auc = roc_auc_score(validation_labels, validation_prob)\n",
    "    # validation_pr_auc = average_precision_score(validation_labels, validation_prob)\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter search (bayesian optimization)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "results = {}\n",
    "\n",
    "datasets = [\n",
    "    \"aki_stage_X_extended_6H.csv\",\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    tail = dataset\n",
    "    data_path = os.path.join(base_data_path,\"resampled\", dataset)\n",
    "    X = pd.read_csv(data_path)\n",
    "\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage',)\n",
    "    numeric_feat.remove('icustay_id',)\n",
    "\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "\n",
    "    # X = X.sort_values(by=['icustay_id', 'charttime'])\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    seq_lengths = X.groupby(['icustay_id'],as_index=False).size().sort_values(by = ['size'],ascending=False)\n",
    "    sequence_length = seq_lengths.max() # the longest sequence per icustay-id\n",
    "\n",
    "    #AL re-write as try except to make it work as hadm_id is not used if only one csv file is used and none are merged\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    id_list = X['icustay_id'].unique()\n",
    "\n",
    "    \n",
    "    id_train, id_test_val = train_test_split(id_list, test_size = SPLIT_SIZE, random_state = 42) # train set is 80%)\n",
    "    # remaining 20% split in halves as test and validation 10% and 10%\n",
    "    id_valid, id_test = train_test_split(id_test_val, test_size = 0.5, random_state = 42) # test 10% valid 10%\n",
    "\n",
    "    X = X.reindex(columns = [col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "    test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index = True) \n",
    "    validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id']) \n",
    "\n",
    "    test = test.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "    train = train.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "    validation = validation.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "\n",
    "    train.drop(['charttime'], axis=1, inplace = True)  \n",
    "    test.drop(['charttime'], axis=1, inplace = True)\n",
    "    validation.drop(['charttime'], axis=1, inplace = True)\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "\n",
    "\n",
    "    # flatten the train, test and validation data\n",
    "    train_flat = np.concatenate(train, axis=0)\n",
    "    test_flat = np.concatenate(test, axis=0)\n",
    "    validation_flat = np.concatenate(validation, axis=0)\n",
    "\n",
    "    # get the labels\n",
    "    train_labels = np.array([x[-1] for x in train_flat])\n",
    "    test_labels = np.array([x[-1] for x in test_flat])\n",
    "    validation_labels = np.array([x[-1] for x in validation_flat])\n",
    "\n",
    "    # get the features\n",
    "    train_features = np.array([x[1:-1] for x in train_flat])\n",
    "    validation_features = np.array([x[1:-1] for x in validation_flat])\n",
    "    test_features = np.array([x[1:-1] for x in test_flat])\n",
    "    \n",
    "    param_space = {\n",
    "    'n_estimators': Integer(300, 2000),\n",
    "    'subsample': Real(0.1, 0.6),\n",
    "    'min_child_weight': Integer(10, 100),\n",
    "    'gamma': Real(0.5, 0.99, prior='log-uniform')\n",
    "}\n",
    "\n",
    "    # Create the base model with fixed parameters\n",
    "    xgb = XGBClassifier(\n",
    "        max_depth=8,  # Fixed parameter\n",
    "        learning_rate=0.025,  # Fixed parameter\n",
    "        colsample_bytree=1.0,  # Fixed parameter\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=RANDOM\n",
    "    )\n",
    "    \n",
    "    # Instantiate BayesSearchCV\n",
    "    bayes_search = BayesSearchCV(\n",
    "        estimator=xgb,\n",
    "        search_spaces=param_space,\n",
    "        n_iter=10,  # number of iterations\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        scoring='roc_auc',\n",
    "        random_state=RANDOM\n",
    "    )\n",
    "    \n",
    "    # Perform Bayesian optimization on the training data\n",
    "    bayes_search.fit(train_features, train_labels)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_xgb = bayes_search.best_estimator_\n",
    "    \n",
    "    # Print the best parameters\n",
    "    print(\"Best parameters:\", bayes_search.best_params_)\n",
    "    \n",
    "    # # Use the best model for predictions\n",
    "    # train_predictions = best_xgb.predict(train_features)\n",
    "    # test_predictions = best_xgb.predict(test_features)\n",
    "    # validation_predictions = best_xgb.predict(validation_features)\n",
    "    \n",
    "    # # Calculate accuracies\n",
    "    # train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "    # test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    # validation_accuracy = accuracy_score(validation_labels, validation_predictions)\n",
    "    \n",
    "    # # Get probabilities\n",
    "    # training_prob = best_xgb.predict_proba(train_features)[:, 1]\n",
    "    # test_prob = best_xgb.predict_proba(test_features)[:, 1]\n",
    "    # validation_prob = best_xgb.predict_proba(validation_features)[:, 1]\n",
    "    \n",
    "    # # Calculate ROC AUC and PR AUC\n",
    "    # training_roc_auc = roc_auc_score(train_labels, training_prob)\n",
    "    # training_pr_auc = average_precision_score(train_labels, training_prob)\n",
    "    # test_roc_auc = roc_auc_score(test_labels, test_prob)\n",
    "    # test_pr_auc = average_precision_score(test_labels, test_prob)\n",
    "    # validation_roc_auc = roc_auc_score(validation_labels, validation_prob)\n",
    "    # validation_pr_auc = average_precision_score(validation_labels, validation_prob)\n",
    "    \n",
    "    # # Print results\n",
    "    # print(f\"Results for {tail}\")\n",
    "    # print(f\"Train accuracy: {train_accuracy:.3f}.. Train ROC AUC: {training_roc_auc:.2f}.. Train PR AUC: {training_pr_auc:.2f}..\")\n",
    "    # print(f\"Test accuracy: {test_accuracy:.3f}.. Test ROC AUC: {test_roc_auc:.2f}.. Test PR AUC: {test_pr_auc:.2f}..\")\n",
    "    # print(f\"Validation accuracy: {validation_accuracy:.3f}.. Validation ROC AUC: {validation_roc_auc:.2f}.. Validation PR AUC: {validation_pr_auc:.2f}..\")\n",
    "    \n",
    "    # # Save results and model\n",
    "    # now = datetime.now()\n",
    "    # out_path = os.path.join(base_data_path, \"models\", f\"{tail}_{now.strftime('%Y%m%d%H%M%S')}\")\n",
    "    # os.makedirs(out_path, exist_ok=True)\n",
    "    \n",
    "    # best_xgb.save_model(f'{out_path}/best_xgb.model')\n",
    "    # np.save(f'{out_path}/best_params.npy', bayes_search.best_params_)\n",
    "    # np.save(f'{out_path}/train_feature_names.npy', X.columns[2:-1])\n",
    "    \n",
    "    # results[tail] = {\n",
    "    #     'train_accuracy': train_accuracy, 'test_accuracy': test_accuracy, 'validation_accuracy': validation_accuracy,\n",
    "    #     'train_roc_auc': training_roc_auc, 'test_roc_auc': test_roc_auc, 'validation_roc_auc': validation_roc_auc,\n",
    "    #     'train_pr_auc': training_pr_auc, 'test_pr_auc': test_pr_auc, 'validation_pr_auc': validation_pr_auc,\n",
    "    #     'best_params': bayes_search.best_params_\n",
    "    # }\n",
    "    \n",
    "    # np.save(f'{out_path}/results.npy', results)\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = [\n",
    "    \"data/preprocessed/preprocessed_data_1H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_2H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_4H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_6H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_8H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_12H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_24H.csv\",\n",
    "]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    X = pd.read_csv(data_path)\n",
    "    X.drop(['height_first', 'hadm_id', 'weight_first', 'inr_max'], axis=1, inplace = True)\n",
    "    # write back to the same file\n",
    "    X.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1262305/4060018899.py:59: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grouped_data = X.groupby('icustay_id').apply(lambda x: x.drop('icustay_id', axis=1).to_numpy())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872509\n",
      "Processing fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1262305/4060018899.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_1262305/4060018899.py:85: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 results:\n",
      "Train accuracy: 0.837, ROC AUC: 0.79, PR AUC: 0.56, Brier: 0.1220\n",
      "Validation accuracy: 0.821, ROC AUC: 0.77, PR AUC: 0.54, ROC AUC: 0.77, PR AUC: 0.54, Brier: 0.1329\n",
      "Processing fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1262305/4060018899.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_1262305/4060018899.py:85: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 results:\n",
      "Train accuracy: 0.834, ROC AUC: 0.79, PR AUC: 0.56, Brier: 0.1241\n",
      "Validation accuracy: 0.832, ROC AUC: 0.77, PR AUC: 0.53, ROC AUC: 0.77, PR AUC: 0.53, Brier: 0.1263\n",
      "Processing fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1262305/4060018899.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_1262305/4060018899.py:85: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 results:\n",
      "Train accuracy: 0.833, ROC AUC: 0.79, PR AUC: 0.56, Brier: 0.1245\n",
      "Validation accuracy: 0.836, ROC AUC: 0.78, PR AUC: 0.53, ROC AUC: 0.78, PR AUC: 0.53, Brier: 0.1235\n",
      "Processing fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1262305/4060018899.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_1262305/4060018899.py:85: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 results:\n",
      "Train accuracy: 0.834, ROC AUC: 0.79, PR AUC: 0.57, Brier: 0.1233\n",
      "Validation accuracy: 0.828, ROC AUC: 0.76, PR AUC: 0.51, ROC AUC: 0.76, PR AUC: 0.51, Brier: 0.1293\n",
      "Processing fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1262305/4060018899.py:84: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_1262305/4060018899.py:85: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 results:\n",
      "Train accuracy: 0.834, ROC AUC: 0.79, PR AUC: 0.56, Brier: 0.1234\n",
      "Validation accuracy: 0.828, ROC AUC: 0.77, PR AUC: 0.54, ROC AUC: 0.77, PR AUC: 0.54, Brier: 0.1283\n",
      "\n",
      "Average scores across 5 folds:\n",
      "train_accuracy: 0.8343\n",
      "val_accuracy: 0.8290\n",
      "train_roc_auc: 0.7924\n",
      "val_roc_auc: 0.7701\n",
      "train_pr_auc: 0.5630\n",
      "val_pr_auc: 0.5306\n",
      "roc_auc: 0.7701\n",
      "pr_auc: 0.5306\n",
      "train_brier: 0.1234\n",
      "val_brier: 0.1280\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, roc_curve, precision_recall_curve, f1_score, auc, brier_score_loss\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score, roc_auc_score, auc, accuracy_score\n",
    "\n",
    "results = {}\n",
    "\n",
    "datasets = [\n",
    "    # \"aki_stage_X_extended_1H.csv\",\n",
    "    # \"aki_stage_X_extended_2H.csv\",\n",
    "    # \"aki_stage_X_extended_4H.csv\",\n",
    "    # \"aki_stage_X_extended_6H.csv\",\n",
    "    # \"aki_stage_X_extended_8H.csv\",\n",
    "    \"aki_stage_X_extended_12H.csv\",\n",
    "    # \"aki_stage_X_extended_24H.csv\",\n",
    "    # \"aki_stage_X_original_1H.csv\",\n",
    "    # \"aki_stage_X_original_2H.csv\",\n",
    "    # \"aki_stage_X_original_4H.csv\",\n",
    "    # \"aki_stage_X_original_6H.csv\",\n",
    "    # \"aki_stage_X_original_8H.csv\",\n",
    "    # \"aki_stage_X_original_12H.csv\",\n",
    "    # \"aki_stage_X_original_24H.csv\",\n",
    "              ]\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    tail = dataset\n",
    "    data_path = os.path.join(base_data_path,\"resampled\", dataset)\n",
    "    X = pd.read_csv(data_path)\n",
    "\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage')\n",
    "    numeric_feat.remove('icustay_id')\n",
    "\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    id_list = X['icustay_id'].unique()\n",
    "\n",
    "    id_list.sort()\n",
    "\n",
    "    # Move \"aki_stage\" to last column\n",
    "    X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    X.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "    # Group by icustay_id and convert to numpy arrays\n",
    "    grouped_data = X.groupby('icustay_id').apply(lambda x: x.drop('icustay_id', axis=1).to_numpy())\n",
    "\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_results = []\n",
    "\n",
    "    print(len(X))\n",
    "\n",
    "    for fold, (id_train_idx, id_val_idx) in enumerate(kf.split(id_list), 1):\n",
    "        print(f\"Processing fold {fold}\")\n",
    "        id_train = [id_list[idx] for idx in id_train_idx]\n",
    "        id_val = [id_list[idx] for idx in id_val_idx]       \n",
    "\n",
    "        train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "        validation = X[X.icustay_id.isin(id_val)].sort_values(by=['icustay_id']) \n",
    "\n",
    "        train = train.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "        validation = validation.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "\n",
    "        try:\n",
    "            X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "        validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "\n",
    "        # flatten the train, test and validation data\n",
    "        train_flat = np.concatenate(train, axis=0)\n",
    "        validation_flat = np.concatenate(validation, axis=0)\n",
    "\n",
    "        # get the labels\n",
    "        train_labels = np.array([x[-1] for x in train_flat])\n",
    "        val_labels = np.array([x[-1] for x in validation_flat])\n",
    "\n",
    "        # get the features\n",
    "        train_features = np.array([x[1:-1] for x in train_flat])\n",
    "        val_features = np.array([x[1:-1] for x in validation_flat])\n",
    "\n",
    "        # Create and train the XGBoost classifier\n",
    "        # xgb = XGBClassifier(n_estimators=1000, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "        xgb = XGBClassifier(\n",
    "            gamma=0.6775421629797868, \n",
    "            min_child_weight=93, \n",
    "            n_estimators=478, \n",
    "            subsample=0.31666640093511256,\n",
    "            max_depth=8,  # Fixed parameter\n",
    "            learning_rate=0.025,  # Fixed parameter\n",
    "            colsample_bytree=1.0,  # Fixed parameter\n",
    "            eval_metric='mlogloss',\n",
    "            random_state=RANDOM\n",
    "        )\n",
    "            \n",
    "        xgb.fit(train_features, train_labels)\n",
    "\n",
    "        # Make predictions\n",
    "        train_predictions = xgb.predict(train_features)\n",
    "        val_predictions = xgb.predict(val_features)\n",
    "\n",
    "        # unique values in the labels\n",
    "        unique_labels = np.unique(np.concatenate([train_labels, val_labels]))\n",
    "        unique_labels_pred = np.unique(np.concatenate([train_predictions, val_predictions]))\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "        val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        \n",
    "\n",
    "        # Calculate probabilities\n",
    "        train_prob = xgb.predict_proba(train_features)[:, 1]\n",
    "        val_prob = xgb.predict_proba(val_features)[:, 1]\n",
    "\n",
    "\n",
    "        # Calculate ROC AUC and PR AUC\n",
    "        train_roc_auc = roc_auc_score(train_labels, train_prob)\n",
    "        train_pr_auc = average_precision_score(train_labels, train_prob)\n",
    "        val_roc_auc = roc_auc_score(val_labels, val_prob)\n",
    "        val_pr_auc = average_precision_score(val_labels, val_prob)\n",
    "        train_brier = brier_score_loss(train_labels, train_prob)\n",
    "        val_brier = brier_score_loss(val_labels, val_prob)\n",
    "                \n",
    "        # compute roc auc\n",
    "        roc_auc = roc_auc_score(val_labels, val_prob, average = 'micro')\n",
    "        # compute Precision_Recall curves\n",
    "        precision, recall, _ = precision_recall_curve(val_labels, val_prob)\n",
    "        # compute PR_AUC\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "\n",
    "        fold_results.append({\n",
    "            'fold': fold,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'train_roc_auc': train_roc_auc,\n",
    "            'val_roc_auc': val_roc_auc,\n",
    "            'train_pr_auc': train_pr_auc,\n",
    "            'val_pr_auc': val_pr_auc,\n",
    "            'roc_auc': roc_auc,\n",
    "            'pr_auc': pr_auc,\n",
    "            'train_brier': train_brier,\n",
    "            'val_brier': val_brier\n",
    "        })\n",
    "\n",
    "        print(f\"Fold {fold} results:\")\n",
    "        print(f\"Train accuracy: {train_accuracy:.3f}, ROC AUC: {train_roc_auc:.2f}, PR AUC: {train_pr_auc:.2f}, Brier: {train_brier:.4f}\")\n",
    "        print(f\"Validation accuracy: {val_accuracy:.3f}, ROC AUC: {val_roc_auc:.2f}, PR AUC: {val_pr_auc:.2f}, ROC AUC: {roc_auc:.2f}, PR AUC: {pr_auc:.2f}, Brier: {val_brier:.4f}\")\n",
    "\n",
    "    # Calculate average scores across folds\n",
    "    avg_scores = {\n",
    "        'train_accuracy': np.mean([r['train_accuracy'] for r in fold_results]),\n",
    "        'val_accuracy': np.mean([r['val_accuracy'] for r in fold_results]),\n",
    "        'train_roc_auc': np.mean([r['train_roc_auc'] for r in fold_results]),\n",
    "        'val_roc_auc': np.mean([r['val_roc_auc'] for r in fold_results]),\n",
    "        'train_pr_auc': np.mean([r['train_pr_auc'] for r in fold_results]),\n",
    "        'val_pr_auc': np.mean([r['val_pr_auc'] for r in fold_results]),\n",
    "        'roc_auc': np.mean([r['roc_auc'] for r in fold_results]),\n",
    "        'pr_auc': np.mean([r['pr_auc'] for r in fold_results]),\n",
    "        'train_brier': np.mean([r['train_brier'] for r in fold_results]),\n",
    "        'val_brier': np.mean([r['val_brier'] for r in fold_results])\n",
    "    }\n",
    "\n",
    "    print(\"\\nAverage scores across 5 folds:\")\n",
    "    for metric, value in avg_scores.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    results[tail] = {\n",
    "        'fold_results': fold_results,\n",
    "        'average_scores': avg_scores\n",
    "    }\n",
    "\n",
    "    now = datetime.now()\n",
    "    out_path = os.path.join(base_data_path, \"models\", f\"{tail}_{now.strftime('%Y%m%d%H%M%S')}\")\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    \n",
    "    # Save results\n",
    "    np.save(f'{out_path}/results.npy', results)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "# Save overall results\n",
    "np.save(os.path.join(base_data_path, \"models\", \"xgb_cross_validation_results.npy\"), results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n",
      "Processing aki_stage_X_extended_6H.csv\n",
      "Normalizing in [0,1] with min-max normalization\n",
      "         icustay_id            charttime  albumin_mean  aniongap_mean  \\\n",
      "1494043      288185  2141-06-22 00:00:00      0.000000       0.135135   \n",
      "1494044      288185  2141-06-22 06:00:00      0.000000       0.135135   \n",
      "1494045      288185  2141-06-22 12:00:00      0.000000       0.000000   \n",
      "1494046      288185  2141-06-22 18:00:00      0.000000       0.000000   \n",
      "1494047      288185  2141-06-23 00:00:00      0.000000       0.148649   \n",
      "...             ...                  ...           ...            ...   \n",
      "1494171      288185  2141-07-24 00:00:00      0.434783       0.148649   \n",
      "1494172      288185  2141-07-24 06:00:00      0.000000       0.000000   \n",
      "1494173      288185  2141-07-24 12:00:00      0.000000       0.000000   \n",
      "1494174      288185  2141-07-24 18:00:00      0.000000       0.000000   \n",
      "1494175      288185  2141-07-25 00:00:00      0.000000       0.000000   \n",
      "\n",
      "         bands_mean  bicarbonate_mean  bilirubin_mean  bun_mean  calcium  \\\n",
      "1494043         0.0          0.516667        0.050725  0.160714      0.0   \n",
      "1494044         0.0          0.500000        0.000000  0.167857      0.0   \n",
      "1494045         0.0          0.000000        0.000000  0.000000      0.0   \n",
      "1494046         0.0          0.000000        0.000000  0.000000      0.0   \n",
      "1494047         0.0          0.500000        0.050725  0.167857      0.0   \n",
      "...             ...               ...             ...       ...      ...   \n",
      "1494171         0.0          0.500000        0.044686  0.114286      0.0   \n",
      "1494172         0.0          0.000000        0.000000  0.000000      0.0   \n",
      "1494173         0.0          0.000000        0.000000  0.000000      0.0   \n",
      "1494174         0.0          0.000000        0.000000  0.000000      0.0   \n",
      "1494175         0.0          0.000000        0.000000  0.000000      0.0   \n",
      "\n",
      "         calcium_mean  ...  sedative  vasopressor  vent  admission_age  \\\n",
      "1494043      0.258170  ...       0.0          0.0   0.0       0.070134   \n",
      "1494044      0.000000  ...       0.0          0.0   0.0       0.070134   \n",
      "1494045      0.000000  ...       0.0          0.0   0.0       0.070134   \n",
      "1494046      0.000000  ...       0.0          0.0   0.0       0.070134   \n",
      "1494047      0.264706  ...       0.0          0.0   0.0       0.070134   \n",
      "...               ...  ...       ...          ...   ...            ...   \n",
      "1494171      0.277778  ...       0.0          0.0   0.0       0.070134   \n",
      "1494172      0.000000  ...       0.0          0.0   0.0       0.070134   \n",
      "1494173      0.000000  ...       0.0          0.0   0.0       0.070134   \n",
      "1494174      0.000000  ...       0.0          0.0   0.0       0.070134   \n",
      "1494175      0.000000  ...       0.0          0.0   0.0       0.070134   \n",
      "\n",
      "         gender_F  gender_M  weight_first  height_first   inr_max  aki_stage  \n",
      "1494043      True     False      0.030855      0.714286  0.065574        0.0  \n",
      "1494044      True     False      0.030855      0.714286  0.065574        0.0  \n",
      "1494045      True     False      0.030855      0.714286  0.065574        0.0  \n",
      "1494046      True     False      0.030855      0.714286  0.065574        0.0  \n",
      "1494047      True     False      0.030855      0.714286  0.065574        0.0  \n",
      "...           ...       ...           ...           ...       ...        ...  \n",
      "1494171      True     False      0.030855      0.714286  0.065574        0.0  \n",
      "1494172      True     False      0.030855      0.714286  0.065574        0.0  \n",
      "1494173      True     False      0.030855      0.714286  0.065574        0.0  \n",
      "1494174      True     False      0.030855      0.714286  0.065574        0.0  \n",
      "1494175      True     False      0.030855      0.714286  0.065574        0.0  \n",
      "\n",
      "[133 rows x 47 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1262305/1590844739.py:116: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = list(train.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences in train: 37801\n",
      "Longest sequence in train: 133\n",
      "Shortest sequence in train: 1\n",
      "Number of columns within a sequence in train: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1262305/1590844739.py:122: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = list(test.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
      "/tmp/ipykernel_1262305/1590844739.py:123: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = list(validation.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train: 1182\n",
      "Epoch 2/200, Train Loss: 0.3512, Train Accuracy: 0.8705, Val Loss: 0.0445, Val AUC: 0.9872, Val Accuracy: 0.9956 Val Brier Score: 0.0049\n",
      "Model saved with AUC: 0.9872 and Brier Score: 0.0049\n",
      "Epoch 3/200, Train Loss: 0.3273, Train Accuracy: 0.8750, Val Loss: 0.0344, Val AUC: 0.9905, Val Accuracy: 0.9945 Val Brier Score: 0.0045\n",
      "Model saved with AUC: 0.9905 and Brier Score: 0.0045\n",
      "Epoch 4/200, Train Loss: 0.3211, Train Accuracy: 0.8756, Val Loss: 0.0138, Val AUC: 0.9921, Val Accuracy: 0.9945 Val Brier Score: 0.0037\n",
      "Model saved with AUC: 0.9921 and Brier Score: 0.0037\n",
      "Epoch 5/200, Train Loss: 0.3165, Train Accuracy: 0.8780, Val Loss: 0.0160, Val AUC: 0.9897, Val Accuracy: 0.9951 Val Brier Score: 0.0047\n",
      "No improvement for 1 epochs.\n",
      "Epoch 6/200, Train Loss: 0.3141, Train Accuracy: 0.8794, Val Loss: 0.0116, Val AUC: 0.9944, Val Accuracy: 0.9951 Val Brier Score: 0.0034\n",
      "Model saved with AUC: 0.9944 and Brier Score: 0.0034\n",
      "Epoch 7/200, Train Loss: 0.3111, Train Accuracy: 0.8803, Val Loss: 0.0122, Val AUC: 0.9940, Val Accuracy: 0.9951 Val Brier Score: 0.0035\n",
      "No improvement for 1 epochs.\n",
      "Epoch 8/200, Train Loss: 0.3081, Train Accuracy: 0.8805, Val Loss: 0.0115, Val AUC: 0.9941, Val Accuracy: 0.9949 Val Brier Score: 0.0035\n",
      "No improvement for 2 epochs.\n",
      "Epoch 9/200, Train Loss: 0.3084, Train Accuracy: 0.8814, Val Loss: 0.0115, Val AUC: 0.9947, Val Accuracy: 0.9951 Val Brier Score: 0.0035\n",
      "Model saved with AUC: 0.9947 and Brier Score: 0.0035\n",
      "Epoch 10/200, Train Loss: 0.3038, Train Accuracy: 0.8819, Val Loss: 0.0110, Val AUC: 0.9951, Val Accuracy: 0.9958 Val Brier Score: 0.0033\n",
      "Model saved with AUC: 0.9951 and Brier Score: 0.0033\n",
      "Epoch 11/200, Train Loss: 0.3043, Train Accuracy: 0.8819, Val Loss: 0.0114, Val AUC: 0.9945, Val Accuracy: 0.9949 Val Brier Score: 0.0035\n",
      "No improvement for 1 epochs.\n",
      "Epoch 12/200, Train Loss: 0.3038, Train Accuracy: 0.8836, Val Loss: 0.0115, Val AUC: 0.9948, Val Accuracy: 0.9947 Val Brier Score: 0.0035\n",
      "No improvement for 2 epochs.\n",
      "Epoch 13/200, Train Loss: 0.3047, Train Accuracy: 0.8822, Val Loss: 0.0114, Val AUC: 0.9954, Val Accuracy: 0.9949 Val Brier Score: 0.0034\n",
      "Model saved with AUC: 0.9954 and Brier Score: 0.0034\n",
      "Epoch 14/200, Train Loss: 0.3074, Train Accuracy: 0.8826, Val Loss: 0.0121, Val AUC: 0.9958, Val Accuracy: 0.9954 Val Brier Score: 0.0033\n",
      "Model saved with AUC: 0.9958 and Brier Score: 0.0033\n",
      "Epoch 15/200, Train Loss: 0.3043, Train Accuracy: 0.8832, Val Loss: 0.0108, Val AUC: 0.9953, Val Accuracy: 0.9949 Val Brier Score: 0.0034\n",
      "No improvement for 1 epochs.\n",
      "Epoch 16/200, Train Loss: 0.3027, Train Accuracy: 0.8803, Val Loss: 0.0114, Val AUC: 0.9950, Val Accuracy: 0.9951 Val Brier Score: 0.0034\n",
      "No improvement for 2 epochs.\n",
      "Epoch 17/200, Train Loss: 0.3079, Train Accuracy: 0.8797, Val Loss: 0.0122, Val AUC: 0.9951, Val Accuracy: 0.9949 Val Brier Score: 0.0034\n",
      "No improvement for 3 epochs.\n",
      "Epoch 18/200, Train Loss: 0.3040, Train Accuracy: 0.8798, Val Loss: 0.0115, Val AUC: 0.9953, Val Accuracy: 0.9947 Val Brier Score: 0.0035\n",
      "No improvement for 4 epochs.\n",
      "Epoch 19/200, Train Loss: 0.3027, Train Accuracy: 0.8825, Val Loss: 0.0118, Val AUC: 0.9955, Val Accuracy: 0.9949 Val Brier Score: 0.0034\n",
      "No improvement for 5 epochs.\n",
      "Epoch 20/200, Train Loss: 0.3061, Train Accuracy: 0.8821, Val Loss: 0.0141, Val AUC: 0.9947, Val Accuracy: 0.9956 Val Brier Score: 0.0034\n",
      "No improvement for 6 epochs.\n",
      "Epoch 21/200, Train Loss: 0.3102, Train Accuracy: 0.8804, Val Loss: 0.0113, Val AUC: 0.9953, Val Accuracy: 0.9956 Val Brier Score: 0.0031\n",
      "No improvement for 7 epochs.\n",
      "Epoch 22/200, Train Loss: 0.3068, Train Accuracy: 0.8816, Val Loss: 0.0143, Val AUC: 0.9929, Val Accuracy: 0.9947 Val Brier Score: 0.0038\n",
      "No improvement for 8 epochs.\n",
      "Epoch 23/200, Train Loss: 0.3082, Train Accuracy: 0.8809, Val Loss: 0.0119, Val AUC: 0.9945, Val Accuracy: 0.9951 Val Brier Score: 0.0035\n",
      "No improvement for 9 epochs.\n",
      "Epoch 24/200, Train Loss: 0.3047, Train Accuracy: 0.8831, Val Loss: 0.0112, Val AUC: 0.9952, Val Accuracy: 0.9956 Val Brier Score: 0.0032\n",
      "No improvement for 10 epochs.\n",
      "Early stopping at epoch 24. Best AUC: 0.9958. Best Brier Score: 0.0032\n"
     ]
    }
   ],
   "source": [
    "# lstm with whole sequences\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, brier_score_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "results = {}\n",
    "\n",
    "datasets = [\n",
    "    # \"aki_stage_X_extended_1H.csv\",\n",
    "    # \"aki_stage_X_extended_2H.csv\",\n",
    "    # \"aki_stage_X_extended_4H.csv\",\n",
    "    \"aki_stage_X_extended_6H.csv\",\n",
    "    # \"aki_stage_X_extended_8H.csv\",\n",
    "    # \"aki_stage_X_extended_12H.csv\",\n",
    "    # \"aki_stage_X_extended_24H.csv\",\n",
    "    # \"aki_stage_X_original_1H.csv\",\n",
    "    # \"aki_stage_X_original_2H.csv\",\n",
    "    # \"aki_stage_X_original_4H.csv\",\n",
    "    # \"aki_stage_X_original_6H.csv\",\n",
    "    # \"aki_stage_X_original_8H.csv\",\n",
    "    # \"aki_stage_X_original_12H.csv\",\n",
    "    # \"aki_stage_X_original_24H.csv\",\n",
    "              ]\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch_data = data[i:i+batch_size]\n",
    "        \n",
    "        # Pad sequences to the same length within the batch\n",
    "        max_seq_length = max(len(seq) for seq in batch_data)\n",
    "        \n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        for seq in batch_data:\n",
    "            padded_seq = np.pad(seq, ((0, max_seq_length - len(seq)), (0, 0)), mode='constant')\n",
    "            X_batch.append(padded_seq[:, 1:-1])  # Exclude icustay_id and aki_stage\n",
    "            y_batch.append(padded_seq[-1, -1])  # Take the aki_stage of the first row\n",
    "        \n",
    "        X_batches.append(torch.FloatTensor(X_batch))\n",
    "        y_batches.append(torch.LongTensor(y_batch))\n",
    "    \n",
    "    return X_batches, y_batches\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, emb_size, num_layers=number_layers, \n",
    "                            bidirectional=bi_directional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(emb_size * (2 if bi_directional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        \n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "for dataset in datasets:\n",
    "    tail = dataset\n",
    "    data_path = os.path.join(base_data_path,\"resampled\", dataset)\n",
    "    print(f\"Processing {tail}\")\n",
    "\n",
    "    X = pd.read_csv(data_path)\n",
    "    # X = X.head(10000)  # only take the first 10000 rows\n",
    "\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage')\n",
    "    numeric_feat.remove('icustay_id')\n",
    "\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    id_list = X['icustay_id'].unique()\n",
    "    id_train, id_test_val = train_test_split(id_list, test_size=SPLIT_SIZE, random_state=42)\n",
    "    id_valid, id_test = train_test_split(id_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "    X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id', 'charttime'])\n",
    "    # print rows with first icustay_id\n",
    "    print(train[train.icustay_id == id_train[0]])\n",
    "    \n",
    "    test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id', 'charttime'])\n",
    "    validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "    train.drop(['charttime'], axis=1, inplace=True)\n",
    "    test.drop(['charttime'], axis=1, inplace=True)\n",
    "    validation.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "    train = list(train.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
    "    train.sort(key=len, reverse=True)  # Sort by sequence length in descending order\n",
    "    print(\"Number of sequences in train:\", len(train))\n",
    "    print(\"Longest sequence in train:\", max(len(seq) for seq in train))\n",
    "    print(\"Shortest sequence in train:\", min(len(seq) for seq in train))\n",
    "    print(\"Number of columns within a sequence in train:\", len(train[0][0]))\n",
    "    test = list(test.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
    "    validation = list(validation.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
    "\n",
    "    batch_size = 32  # You may need to adjust this\n",
    "    X_train, y_train = batch(train, batch_size)\n",
    "    X_test, y_test = batch(test, batch_size)\n",
    "    X_val, y_val = batch(validation, batch_size)\n",
    "\n",
    "    print(f\"Number of batches in train: {len(X_train)}\")\n",
    "    os.makedirs(os.path.join(base_data_path, \"models\", tail), exist_ok=True)\n",
    "    writer = SummaryWriter(os.path.join(base_data_path, \"logs\", tail, datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "    input_size = X_train[0].shape[2]\n",
    "    output_size = 1\n",
    "    emb_size = round(input_size / 1)\n",
    "    number_layers = 3\n",
    "    dropout = 0.1\n",
    "    bi_directional = True\n",
    "\n",
    "    nn_model = Net(input_size, emb_size, output_size, bi_directional, number_layers, dropout).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=0.01)\n",
    "\n",
    "    use_pretrained = False\n",
    "    best_auc = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    if use_pretrained:\n",
    "        model_path = f'data/models/{tail}/LSTM_best.pth'\n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path)\n",
    "            nn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            best_auc = checkpoint.get('best_auc', 0)\n",
    "            start_epoch = checkpoint.get('epoch', 0)\n",
    "            print(f\"Loaded pretrained model from {model_path} with AUC {best_auc} at epoch {start_epoch}.\")\n",
    "        else:\n",
    "            print(f\"No pretrained model found, starting training from scratch.\")\n",
    "\n",
    "    patience = 10  # Number of epochs to wait for improvement\n",
    "    no_improvement_counter = 0  # Counter to track epochs without improvement\n",
    "\n",
    "    n_epochs = 200\n",
    "    best_auc = 0  # Initialize the best AUC for comparison\n",
    "\n",
    "    for epoch in range(start_epoch + 1, n_epochs):\n",
    "        nn_model.train()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "\n",
    "        for i, (X_batch, y_batch) in enumerate(zip(X_train, y_train)):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = nn_model(X_batch)\n",
    "            loss = criterion(outputs, y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            train_accuracy = accuracy_score(y_batch.cpu().numpy(), predicted.cpu().numpy())\n",
    "            running_accuracy += train_accuracy\n",
    "\n",
    "            writer.add_scalar('Training/Loss', loss.item(), epoch * len(X_train) + i)\n",
    "            writer.add_scalar('Training/Accuracy', train_accuracy, epoch * len(X_train) + i)\n",
    "\n",
    "        # Validation phase\n",
    "        nn_model.eval()\n",
    "        total_v_loss = 0\n",
    "        all_y_val = []\n",
    "        all_val_prob = []\n",
    "        all_accuracy = 0\n",
    "\n",
    "        for X_val_batch, y_val_batch in zip(X_val, y_val):\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                v_out = nn_model(X_val_batch)\n",
    "                v_loss = criterion(v_out, y_val_batch.float())\n",
    "                val_prob = torch.sigmoid(v_out)\n",
    "                total_v_loss += v_loss.item()\n",
    "                all_y_val.extend(y_val_batch.cpu().numpy())\n",
    "                all_val_prob.extend(val_prob.cpu().numpy())\n",
    "\n",
    "                predicted = val_prob > 0.5\n",
    "                val_accuracy = accuracy_score(y_val_batch.cpu().numpy(), predicted.cpu().numpy())\n",
    "                all_accuracy += val_accuracy\n",
    "\n",
    "        avg_v_loss = total_v_loss / len(X_val)\n",
    "        roc_auc = roc_auc_score(all_y_val, all_val_prob)\n",
    "        avg_accuracy = all_accuracy / len(X_val)\n",
    "        brier_score = brier_score_loss(all_y_val, all_val_prob)\n",
    "\n",
    "        writer.add_scalar('Validation/Loss', avg_v_loss, epoch)\n",
    "        writer.add_scalar('Validation/AUC', roc_auc, epoch)\n",
    "        writer.add_scalar('Validation/Accuracy', avg_accuracy, epoch)\n",
    "        writer.add_scalar('Validation/Brier_Score', brier_score, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, \"\n",
    "            f\"Train Loss: {running_loss/len(X_train):.4f}, \"\n",
    "            f\"Train Accuracy: {running_accuracy/len(X_train):.4f}, \"\n",
    "            f\"Val Loss: {avg_v_loss:.4f}, \"\n",
    "            f\"Val AUC: {roc_auc:.4f}, \"\n",
    "            f\"Val Accuracy: {avg_accuracy:.4f}\",\n",
    "            f\"Val Brier Score: {brier_score:.4f}\")\n",
    "        # Early stopping logic\n",
    "        if roc_auc > best_auc:\n",
    "            best_auc = roc_auc\n",
    "            no_improvement_counter = 0  # Reset counter\n",
    "            save_path = f'data/models/{tail}/LSTM_best.pth'\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            torch.save({\n",
    "                'model_state_dict': nn_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_auc': best_auc,\n",
    "                'epoch': epoch,\n",
    "                'brier_score': brier_score\n",
    "            }, save_path)\n",
    "            print(f\"Model saved with AUC: {roc_auc:.4f} and Brier Score: {brier_score:.4f}\")\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "            print(f\"No improvement for {no_improvement_counter} epochs.\")\n",
    "\n",
    "        # Stop if no improvement for 'patience' epochs\n",
    "        if no_improvement_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best AUC: {best_auc:.4f}. Best Brier Score: {brier_score:.4f}\")\n",
    "            break\n",
    "\n",
    "    writer.close()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda:0\n",
      "Processing aki_stage_X_extended_12H.csv\n",
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1262305/2342088124.py:115: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = list(train.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
      "/tmp/ipykernel_1262305/2342088124.py:117: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = list(test.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
      "/tmp/ipykernel_1262305/2342088124.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = list(validation.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train: 1182\n",
      "Input size: 44\n",
      "Epoch 2/200, Train Loss: 0.4792, Train Accuracy: 0.8008, Val Loss: 0.8679, Val AUC: 0.5251, Val Accuracy: 0.2408, Val Brier Score: 0.3336\n",
      "Model saved with AUC: 0.5251 and Brier Score: 0.3336\n",
      "Epoch 3/200, Train Loss: 0.4596, Train Accuracy: 0.8004, Val Loss: 0.7814, Val AUC: 0.5752, Val Accuracy: 0.2203, Val Brier Score: 0.2942\n",
      "Model saved with AUC: 0.5752 and Brier Score: 0.2942\n",
      "Epoch 4/200, Train Loss: 0.4582, Train Accuracy: 0.7983, Val Loss: 0.8088, Val AUC: 0.4585, Val Accuracy: 0.2328, Val Brier Score: 0.3082\n",
      "No improvement for 1 epochs.\n",
      "Epoch 5/200, Train Loss: 0.4536, Train Accuracy: 0.7996, Val Loss: 0.7246, Val AUC: 0.4383, Val Accuracy: 0.2509, Val Brier Score: 0.2669\n",
      "No improvement for 2 epochs.\n",
      "Epoch 6/200, Train Loss: 0.4694, Train Accuracy: 0.8009, Val Loss: 0.5679, Val AUC: 0.6527, Val Accuracy: 0.7920, Val Brier Score: 0.1654\n",
      "Model saved with AUC: 0.6527 and Brier Score: 0.1654\n",
      "Epoch 7/200, Train Loss: 0.4480, Train Accuracy: 0.8065, Val Loss: 0.7030, Val AUC: 0.6648, Val Accuracy: 0.3591, Val Brier Score: 0.2594\n",
      "Model saved with AUC: 0.6648 and Brier Score: 0.2594\n",
      "Epoch 8/200, Train Loss: 0.4466, Train Accuracy: 0.8039, Val Loss: 0.5306, Val AUC: 0.7133, Val Accuracy: 0.6701, Val Brier Score: 0.1760\n",
      "Model saved with AUC: 0.7133 and Brier Score: 0.1760\n",
      "Epoch 9/200, Train Loss: 0.4406, Train Accuracy: 0.8071, Val Loss: 0.6105, Val AUC: 0.6813, Val Accuracy: 0.4983, Val Brier Score: 0.2167\n",
      "No improvement for 1 epochs.\n",
      "Epoch 10/200, Train Loss: 0.4385, Train Accuracy: 0.8088, Val Loss: 0.5746, Val AUC: 0.6964, Val Accuracy: 0.5297, Val Brier Score: 0.1983\n",
      "No improvement for 2 epochs.\n",
      "Epoch 11/200, Train Loss: 0.4466, Train Accuracy: 0.8053, Val Loss: 0.5953, Val AUC: 0.6303, Val Accuracy: 0.6652, Val Brier Score: 0.2050\n",
      "No improvement for 3 epochs.\n",
      "Epoch 12/200, Train Loss: 0.4398, Train Accuracy: 0.8093, Val Loss: 0.5406, Val AUC: 0.6729, Val Accuracy: 0.8018, Val Brier Score: 0.1766\n",
      "No improvement for 4 epochs.\n",
      "Epoch 13/200, Train Loss: 0.4370, Train Accuracy: 0.8104, Val Loss: 0.4781, Val AUC: 0.7139, Val Accuracy: 0.8016, Val Brier Score: 0.1513\n",
      "Model saved with AUC: 0.7139 and Brier Score: 0.1513\n",
      "Epoch 14/200, Train Loss: 0.4362, Train Accuracy: 0.8107, Val Loss: 0.5227, Val AUC: 0.6668, Val Accuracy: 0.8006, Val Brier Score: 0.1713\n",
      "No improvement for 1 epochs.\n",
      "Epoch 15/200, Train Loss: 0.4327, Train Accuracy: 0.8118, Val Loss: 0.4505, Val AUC: 0.7375, Val Accuracy: 0.8057, Val Brier Score: 0.1415\n",
      "Model saved with AUC: 0.7375 and Brier Score: 0.1415\n",
      "Epoch 16/200, Train Loss: 0.4301, Train Accuracy: 0.8126, Val Loss: 0.4493, Val AUC: 0.7375, Val Accuracy: 0.8038, Val Brier Score: 0.1411\n",
      "Model saved with AUC: 0.7375 and Brier Score: 0.1411\n",
      "Epoch 17/200, Train Loss: 0.4312, Train Accuracy: 0.8123, Val Loss: 0.4921, Val AUC: 0.7306, Val Accuracy: 0.7533, Val Brier Score: 0.1609\n",
      "No improvement for 1 epochs.\n",
      "Epoch 18/200, Train Loss: 0.4282, Train Accuracy: 0.8131, Val Loss: 0.4462, Val AUC: 0.7410, Val Accuracy: 0.8051, Val Brier Score: 0.1401\n",
      "Model saved with AUC: 0.7410 and Brier Score: 0.1401\n",
      "Epoch 19/200, Train Loss: 0.4263, Train Accuracy: 0.8135, Val Loss: 0.4460, Val AUC: 0.7487, Val Accuracy: 0.7996, Val Brier Score: 0.1406\n",
      "Model saved with AUC: 0.7487 and Brier Score: 0.1406\n",
      "Epoch 20/200, Train Loss: 0.4255, Train Accuracy: 0.8139, Val Loss: 0.4442, Val AUC: 0.7571, Val Accuracy: 0.8012, Val Brier Score: 0.1402\n",
      "Model saved with AUC: 0.7571 and Brier Score: 0.1402\n",
      "Epoch 21/200, Train Loss: 0.4237, Train Accuracy: 0.8144, Val Loss: 0.4679, Val AUC: 0.7601, Val Accuracy: 0.7660, Val Brier Score: 0.1507\n",
      "Model saved with AUC: 0.7601 and Brier Score: 0.1507\n",
      "Epoch 22/200, Train Loss: 0.4230, Train Accuracy: 0.8149, Val Loss: 0.4428, Val AUC: 0.7604, Val Accuracy: 0.8029, Val Brier Score: 0.1397\n",
      "Model saved with AUC: 0.7604 and Brier Score: 0.1397\n",
      "Epoch 23/200, Train Loss: 0.4221, Train Accuracy: 0.8152, Val Loss: 0.4498, Val AUC: 0.7614, Val Accuracy: 0.7904, Val Brier Score: 0.1428\n",
      "Model saved with AUC: 0.7614 and Brier Score: 0.1428\n",
      "Epoch 24/200, Train Loss: 0.4208, Train Accuracy: 0.8157, Val Loss: 0.4445, Val AUC: 0.7659, Val Accuracy: 0.8033, Val Brier Score: 0.1401\n",
      "Model saved with AUC: 0.7659 and Brier Score: 0.1401\n",
      "Epoch 25/200, Train Loss: 0.4201, Train Accuracy: 0.8158, Val Loss: 0.4519, Val AUC: 0.7685, Val Accuracy: 0.7983, Val Brier Score: 0.1433\n",
      "Model saved with AUC: 0.7685 and Brier Score: 0.1433\n",
      "Epoch 26/200, Train Loss: 0.4189, Train Accuracy: 0.8166, Val Loss: 0.4462, Val AUC: 0.7701, Val Accuracy: 0.8004, Val Brier Score: 0.1411\n",
      "Model saved with AUC: 0.7701 and Brier Score: 0.1411\n",
      "Epoch 27/200, Train Loss: 0.4183, Train Accuracy: 0.8167, Val Loss: 0.4441, Val AUC: 0.7635, Val Accuracy: 0.8030, Val Brier Score: 0.1402\n",
      "No improvement for 1 epochs.\n",
      "Epoch 28/200, Train Loss: 0.4169, Train Accuracy: 0.8176, Val Loss: 0.4459, Val AUC: 0.7761, Val Accuracy: 0.8027, Val Brier Score: 0.1409\n",
      "Model saved with AUC: 0.7761 and Brier Score: 0.1409\n",
      "Epoch 29/200, Train Loss: 0.4167, Train Accuracy: 0.8174, Val Loss: 0.4417, Val AUC: 0.7726, Val Accuracy: 0.8052, Val Brier Score: 0.1398\n",
      "No improvement for 1 epochs.\n",
      "Epoch 30/200, Train Loss: 0.4160, Train Accuracy: 0.8178, Val Loss: 0.4433, Val AUC: 0.7705, Val Accuracy: 0.8049, Val Brier Score: 0.1404\n",
      "No improvement for 2 epochs.\n",
      "Epoch 31/200, Train Loss: 0.4151, Train Accuracy: 0.8184, Val Loss: 0.4385, Val AUC: 0.7755, Val Accuracy: 0.8063, Val Brier Score: 0.1387\n",
      "No improvement for 3 epochs.\n",
      "Epoch 32/200, Train Loss: 0.4147, Train Accuracy: 0.8187, Val Loss: 0.4348, Val AUC: 0.7748, Val Accuracy: 0.8073, Val Brier Score: 0.1372\n",
      "No improvement for 4 epochs.\n",
      "Epoch 33/200, Train Loss: 0.4140, Train Accuracy: 0.8186, Val Loss: 0.4345, Val AUC: 0.7763, Val Accuracy: 0.8063, Val Brier Score: 0.1374\n",
      "Model saved with AUC: 0.7763 and Brier Score: 0.1374\n",
      "Epoch 34/200, Train Loss: 0.4140, Train Accuracy: 0.8190, Val Loss: 0.4378, Val AUC: 0.7765, Val Accuracy: 0.8035, Val Brier Score: 0.1387\n",
      "Model saved with AUC: 0.7765 and Brier Score: 0.1387\n",
      "Epoch 35/200, Train Loss: 0.4136, Train Accuracy: 0.8191, Val Loss: 0.4423, Val AUC: 0.7712, Val Accuracy: 0.8039, Val Brier Score: 0.1402\n",
      "No improvement for 1 epochs.\n",
      "Epoch 36/200, Train Loss: 0.4130, Train Accuracy: 0.8193, Val Loss: 0.4349, Val AUC: 0.7736, Val Accuracy: 0.8075, Val Brier Score: 0.1373\n",
      "No improvement for 2 epochs.\n",
      "Epoch 37/200, Train Loss: 0.4126, Train Accuracy: 0.8200, Val Loss: 0.4389, Val AUC: 0.7633, Val Accuracy: 0.8079, Val Brier Score: 0.1382\n",
      "No improvement for 3 epochs.\n",
      "Epoch 38/200, Train Loss: 0.4117, Train Accuracy: 0.8200, Val Loss: 0.4342, Val AUC: 0.7750, Val Accuracy: 0.8067, Val Brier Score: 0.1374\n",
      "No improvement for 4 epochs.\n",
      "Epoch 39/200, Train Loss: 0.4111, Train Accuracy: 0.8205, Val Loss: 0.4461, Val AUC: 0.7752, Val Accuracy: 0.7985, Val Brier Score: 0.1423\n",
      "No improvement for 5 epochs.\n",
      "Epoch 40/200, Train Loss: 0.4111, Train Accuracy: 0.8206, Val Loss: 0.4298, Val AUC: 0.7765, Val Accuracy: 0.8101, Val Brier Score: 0.1353\n",
      "No improvement for 6 epochs.\n",
      "Epoch 41/200, Train Loss: 0.4105, Train Accuracy: 0.8207, Val Loss: 0.4374, Val AUC: 0.7741, Val Accuracy: 0.8026, Val Brier Score: 0.1388\n",
      "No improvement for 7 epochs.\n",
      "Epoch 42/200, Train Loss: 0.4118, Train Accuracy: 0.8203, Val Loss: 0.4335, Val AUC: 0.7714, Val Accuracy: 0.8062, Val Brier Score: 0.1370\n",
      "No improvement for 8 epochs.\n",
      "Epoch 43/200, Train Loss: 0.4103, Train Accuracy: 0.8208, Val Loss: 0.4327, Val AUC: 0.7666, Val Accuracy: 0.8100, Val Brier Score: 0.1360\n",
      "No improvement for 9 epochs.\n",
      "Epoch 44/200, Train Loss: 0.4094, Train Accuracy: 0.8211, Val Loss: 0.4348, Val AUC: 0.7751, Val Accuracy: 0.8044, Val Brier Score: 0.1379\n",
      "No improvement for 10 epochs.\n",
      "Epoch 45/200, Train Loss: 0.4091, Train Accuracy: 0.8211, Val Loss: 0.4378, Val AUC: 0.7738, Val Accuracy: 0.7997, Val Brier Score: 0.1393\n",
      "No improvement for 11 epochs.\n",
      "Epoch 46/200, Train Loss: 0.4086, Train Accuracy: 0.8217, Val Loss: 0.4299, Val AUC: 0.7761, Val Accuracy: 0.8068, Val Brier Score: 0.1359\n",
      "No improvement for 12 epochs.\n",
      "Epoch 47/200, Train Loss: 0.4088, Train Accuracy: 0.8216, Val Loss: 0.4226, Val AUC: 0.7828, Val Accuracy: 0.8109, Val Brier Score: 0.1329\n",
      "Model saved with AUC: 0.7828 and Brier Score: 0.1329\n",
      "Epoch 48/200, Train Loss: 0.4079, Train Accuracy: 0.8221, Val Loss: 0.4293, Val AUC: 0.7805, Val Accuracy: 0.8051, Val Brier Score: 0.1360\n",
      "No improvement for 1 epochs.\n",
      "Epoch 49/200, Train Loss: 0.4078, Train Accuracy: 0.8221, Val Loss: 0.4319, Val AUC: 0.7795, Val Accuracy: 0.8035, Val Brier Score: 0.1369\n",
      "No improvement for 2 epochs.\n",
      "Epoch 50/200, Train Loss: 0.4069, Train Accuracy: 0.8226, Val Loss: 0.4371, Val AUC: 0.7778, Val Accuracy: 0.7988, Val Brier Score: 0.1391\n",
      "No improvement for 3 epochs.\n",
      "Epoch 51/200, Train Loss: 0.4075, Train Accuracy: 0.8224, Val Loss: 0.4257, Val AUC: 0.7793, Val Accuracy: 0.8097, Val Brier Score: 0.1341\n",
      "No improvement for 4 epochs.\n",
      "Epoch 52/200, Train Loss: 0.4070, Train Accuracy: 0.8224, Val Loss: 0.4271, Val AUC: 0.7793, Val Accuracy: 0.8059, Val Brier Score: 0.1350\n",
      "No improvement for 5 epochs.\n",
      "Epoch 53/200, Train Loss: 0.4074, Train Accuracy: 0.8220, Val Loss: 0.4283, Val AUC: 0.7795, Val Accuracy: 0.8073, Val Brier Score: 0.1354\n",
      "No improvement for 6 epochs.\n",
      "Epoch 54/200, Train Loss: 0.4059, Train Accuracy: 0.8231, Val Loss: 0.4275, Val AUC: 0.7809, Val Accuracy: 0.8077, Val Brier Score: 0.1349\n",
      "No improvement for 7 epochs.\n",
      "Epoch 55/200, Train Loss: 0.4051, Train Accuracy: 0.8234, Val Loss: 0.4263, Val AUC: 0.7842, Val Accuracy: 0.8071, Val Brier Score: 0.1348\n",
      "Model saved with AUC: 0.7842 and Brier Score: 0.1348\n",
      "Epoch 56/200, Train Loss: 0.4055, Train Accuracy: 0.8228, Val Loss: 0.4277, Val AUC: 0.7812, Val Accuracy: 0.8077, Val Brier Score: 0.1350\n",
      "No improvement for 1 epochs.\n",
      "Epoch 57/200, Train Loss: 0.4070, Train Accuracy: 0.8223, Val Loss: 0.4320, Val AUC: 0.7769, Val Accuracy: 0.8025, Val Brier Score: 0.1368\n",
      "No improvement for 2 epochs.\n",
      "Epoch 58/200, Train Loss: 0.4048, Train Accuracy: 0.8231, Val Loss: 0.4282, Val AUC: 0.7822, Val Accuracy: 0.8043, Val Brier Score: 0.1354\n",
      "No improvement for 3 epochs.\n",
      "Epoch 59/200, Train Loss: 0.4042, Train Accuracy: 0.8234, Val Loss: 0.4306, Val AUC: 0.7828, Val Accuracy: 0.8042, Val Brier Score: 0.1366\n",
      "No improvement for 4 epochs.\n",
      "Epoch 60/200, Train Loss: 0.4045, Train Accuracy: 0.8236, Val Loss: 0.4300, Val AUC: 0.7800, Val Accuracy: 0.8060, Val Brier Score: 0.1360\n",
      "No improvement for 5 epochs.\n",
      "Epoch 61/200, Train Loss: 0.4038, Train Accuracy: 0.8238, Val Loss: 0.4294, Val AUC: 0.7818, Val Accuracy: 0.8064, Val Brier Score: 0.1358\n",
      "No improvement for 6 epochs.\n",
      "Epoch 62/200, Train Loss: 0.4039, Train Accuracy: 0.8236, Val Loss: 0.4266, Val AUC: 0.7781, Val Accuracy: 0.8124, Val Brier Score: 0.1340\n",
      "No improvement for 7 epochs.\n",
      "Epoch 63/200, Train Loss: 0.4029, Train Accuracy: 0.8241, Val Loss: 0.4276, Val AUC: 0.7817, Val Accuracy: 0.8082, Val Brier Score: 0.1350\n",
      "No improvement for 8 epochs.\n",
      "Epoch 64/200, Train Loss: 0.4026, Train Accuracy: 0.8243, Val Loss: 0.4248, Val AUC: 0.7825, Val Accuracy: 0.8105, Val Brier Score: 0.1340\n",
      "No improvement for 9 epochs.\n",
      "Epoch 65/200, Train Loss: 0.4025, Train Accuracy: 0.8248, Val Loss: 0.4271, Val AUC: 0.7822, Val Accuracy: 0.8074, Val Brier Score: 0.1349\n",
      "No improvement for 10 epochs.\n",
      "Epoch 66/200, Train Loss: 0.4020, Train Accuracy: 0.8241, Val Loss: 0.4300, Val AUC: 0.7828, Val Accuracy: 0.8035, Val Brier Score: 0.1364\n",
      "No improvement for 11 epochs.\n",
      "Epoch 67/200, Train Loss: 0.4016, Train Accuracy: 0.8249, Val Loss: 0.4282, Val AUC: 0.7825, Val Accuracy: 0.8044, Val Brier Score: 0.1356\n",
      "No improvement for 12 epochs.\n",
      "Epoch 68/200, Train Loss: 0.4010, Train Accuracy: 0.8251, Val Loss: 0.4267, Val AUC: 0.7834, Val Accuracy: 0.8067, Val Brier Score: 0.1348\n",
      "No improvement for 13 epochs.\n",
      "Epoch 69/200, Train Loss: 0.4014, Train Accuracy: 0.8248, Val Loss: 0.4265, Val AUC: 0.7825, Val Accuracy: 0.8063, Val Brier Score: 0.1347\n",
      "No improvement for 14 epochs.\n",
      "Epoch 70/200, Train Loss: 0.4002, Train Accuracy: 0.8260, Val Loss: 0.4275, Val AUC: 0.7849, Val Accuracy: 0.8039, Val Brier Score: 0.1354\n",
      "Model saved with AUC: 0.7849 and Brier Score: 0.1354\n",
      "Epoch 71/200, Train Loss: 0.4003, Train Accuracy: 0.8257, Val Loss: 0.4229, Val AUC: 0.7840, Val Accuracy: 0.8123, Val Brier Score: 0.1329\n",
      "No improvement for 1 epochs.\n",
      "Epoch 72/200, Train Loss: 0.4001, Train Accuracy: 0.8255, Val Loss: 0.4241, Val AUC: 0.7835, Val Accuracy: 0.8106, Val Brier Score: 0.1333\n",
      "No improvement for 2 epochs.\n",
      "Epoch 73/200, Train Loss: 0.4000, Train Accuracy: 0.8259, Val Loss: 0.4303, Val AUC: 0.7844, Val Accuracy: 0.8028, Val Brier Score: 0.1367\n",
      "No improvement for 3 epochs.\n",
      "Epoch 74/200, Train Loss: 0.3992, Train Accuracy: 0.8256, Val Loss: 0.4239, Val AUC: 0.7802, Val Accuracy: 0.8123, Val Brier Score: 0.1332\n",
      "No improvement for 4 epochs.\n",
      "Epoch 75/200, Train Loss: 0.3998, Train Accuracy: 0.8257, Val Loss: 0.4269, Val AUC: 0.7856, Val Accuracy: 0.8052, Val Brier Score: 0.1352\n",
      "Model saved with AUC: 0.7856 and Brier Score: 0.1352\n",
      "Epoch 76/200, Train Loss: 0.3996, Train Accuracy: 0.8260, Val Loss: 0.4274, Val AUC: 0.7816, Val Accuracy: 0.8082, Val Brier Score: 0.1347\n",
      "No improvement for 1 epochs.\n",
      "Epoch 77/200, Train Loss: 0.3987, Train Accuracy: 0.8265, Val Loss: 0.4243, Val AUC: 0.7867, Val Accuracy: 0.8079, Val Brier Score: 0.1342\n",
      "Model saved with AUC: 0.7867 and Brier Score: 0.1342\n",
      "Epoch 78/200, Train Loss: 0.3984, Train Accuracy: 0.8262, Val Loss: 0.4245, Val AUC: 0.7841, Val Accuracy: 0.8089, Val Brier Score: 0.1342\n",
      "No improvement for 1 epochs.\n",
      "Epoch 79/200, Train Loss: 0.3986, Train Accuracy: 0.8261, Val Loss: 0.4244, Val AUC: 0.7817, Val Accuracy: 0.8124, Val Brier Score: 0.1334\n",
      "No improvement for 2 epochs.\n",
      "Epoch 80/200, Train Loss: 0.3978, Train Accuracy: 0.8267, Val Loss: 0.4244, Val AUC: 0.7843, Val Accuracy: 0.8103, Val Brier Score: 0.1339\n",
      "No improvement for 3 epochs.\n",
      "Epoch 81/200, Train Loss: 0.3974, Train Accuracy: 0.8265, Val Loss: 0.4209, Val AUC: 0.7881, Val Accuracy: 0.8120, Val Brier Score: 0.1325\n",
      "Model saved with AUC: 0.7881 and Brier Score: 0.1325\n",
      "Epoch 82/200, Train Loss: 0.3970, Train Accuracy: 0.8272, Val Loss: 0.4260, Val AUC: 0.7821, Val Accuracy: 0.8109, Val Brier Score: 0.1342\n",
      "No improvement for 1 epochs.\n",
      "Epoch 83/200, Train Loss: 0.3972, Train Accuracy: 0.8268, Val Loss: 0.4244, Val AUC: 0.7855, Val Accuracy: 0.8063, Val Brier Score: 0.1341\n",
      "No improvement for 2 epochs.\n",
      "Epoch 84/200, Train Loss: 0.3971, Train Accuracy: 0.8271, Val Loss: 0.4242, Val AUC: 0.7865, Val Accuracy: 0.8087, Val Brier Score: 0.1340\n",
      "No improvement for 3 epochs.\n",
      "Epoch 85/200, Train Loss: 0.3962, Train Accuracy: 0.8275, Val Loss: 0.4256, Val AUC: 0.7833, Val Accuracy: 0.8091, Val Brier Score: 0.1344\n",
      "No improvement for 4 epochs.\n",
      "Epoch 86/200, Train Loss: 0.3953, Train Accuracy: 0.8278, Val Loss: 0.4247, Val AUC: 0.7832, Val Accuracy: 0.8112, Val Brier Score: 0.1337\n",
      "No improvement for 5 epochs.\n",
      "Epoch 87/200, Train Loss: 0.3961, Train Accuracy: 0.8277, Val Loss: 0.4256, Val AUC: 0.7842, Val Accuracy: 0.8080, Val Brier Score: 0.1345\n",
      "No improvement for 6 epochs.\n",
      "Epoch 88/200, Train Loss: 0.3952, Train Accuracy: 0.8278, Val Loss: 0.4241, Val AUC: 0.7824, Val Accuracy: 0.8129, Val Brier Score: 0.1333\n",
      "No improvement for 7 epochs.\n",
      "Epoch 89/200, Train Loss: 0.3954, Train Accuracy: 0.8272, Val Loss: 0.4270, Val AUC: 0.7823, Val Accuracy: 0.8077, Val Brier Score: 0.1349\n",
      "No improvement for 8 epochs.\n",
      "Epoch 90/200, Train Loss: 0.3944, Train Accuracy: 0.8284, Val Loss: 0.4305, Val AUC: 0.7805, Val Accuracy: 0.8041, Val Brier Score: 0.1363\n",
      "No improvement for 9 epochs.\n",
      "Epoch 91/200, Train Loss: 0.3943, Train Accuracy: 0.8282, Val Loss: 0.4238, Val AUC: 0.7821, Val Accuracy: 0.8119, Val Brier Score: 0.1335\n",
      "No improvement for 10 epochs.\n",
      "Epoch 92/200, Train Loss: 0.3946, Train Accuracy: 0.8281, Val Loss: 0.4237, Val AUC: 0.7839, Val Accuracy: 0.8119, Val Brier Score: 0.1332\n",
      "No improvement for 11 epochs.\n",
      "Epoch 93/200, Train Loss: 0.3982, Train Accuracy: 0.8266, Val Loss: 0.4259, Val AUC: 0.7831, Val Accuracy: 0.8118, Val Brier Score: 0.1337\n",
      "No improvement for 12 epochs.\n",
      "Epoch 94/200, Train Loss: 0.3941, Train Accuracy: 0.8286, Val Loss: 0.4264, Val AUC: 0.7818, Val Accuracy: 0.8101, Val Brier Score: 0.1342\n",
      "No improvement for 13 epochs.\n",
      "Epoch 95/200, Train Loss: 0.3940, Train Accuracy: 0.8283, Val Loss: 0.4250, Val AUC: 0.7827, Val Accuracy: 0.8108, Val Brier Score: 0.1338\n",
      "No improvement for 14 epochs.\n",
      "Epoch 96/200, Train Loss: 0.3935, Train Accuracy: 0.8283, Val Loss: 0.4312, Val AUC: 0.7812, Val Accuracy: 0.8043, Val Brier Score: 0.1365\n",
      "No improvement for 15 epochs.\n",
      "Epoch 97/200, Train Loss: 0.3934, Train Accuracy: 0.8289, Val Loss: 0.4311, Val AUC: 0.7795, Val Accuracy: 0.8049, Val Brier Score: 0.1366\n",
      "No improvement for 16 epochs.\n",
      "Epoch 98/200, Train Loss: 0.3941, Train Accuracy: 0.8283, Val Loss: 0.4257, Val AUC: 0.7815, Val Accuracy: 0.8095, Val Brier Score: 0.1341\n",
      "No improvement for 17 epochs.\n",
      "Epoch 99/200, Train Loss: 0.3927, Train Accuracy: 0.8292, Val Loss: 0.4314, Val AUC: 0.7792, Val Accuracy: 0.8082, Val Brier Score: 0.1358\n",
      "No improvement for 18 epochs.\n",
      "Epoch 100/200, Train Loss: 0.3923, Train Accuracy: 0.8290, Val Loss: 0.4297, Val AUC: 0.7775, Val Accuracy: 0.8091, Val Brier Score: 0.1353\n",
      "No improvement for 19 epochs.\n",
      "Epoch 101/200, Train Loss: 0.3928, Train Accuracy: 0.8289, Val Loss: 0.4267, Val AUC: 0.7813, Val Accuracy: 0.8102, Val Brier Score: 0.1344\n",
      "No improvement for 20 epochs.\n",
      "Epoch 102/200, Train Loss: 0.3935, Train Accuracy: 0.8286, Val Loss: 0.4270, Val AUC: 0.7768, Val Accuracy: 0.8112, Val Brier Score: 0.1343\n",
      "No improvement for 21 epochs.\n",
      "Epoch 103/200, Train Loss: 0.3923, Train Accuracy: 0.8288, Val Loss: 0.4300, Val AUC: 0.7807, Val Accuracy: 0.8065, Val Brier Score: 0.1359\n",
      "No improvement for 22 epochs.\n",
      "Epoch 104/200, Train Loss: 0.3919, Train Accuracy: 0.8291, Val Loss: 0.4268, Val AUC: 0.7816, Val Accuracy: 0.8100, Val Brier Score: 0.1341\n",
      "No improvement for 23 epochs.\n",
      "Epoch 105/200, Train Loss: 0.3915, Train Accuracy: 0.8297, Val Loss: 0.4304, Val AUC: 0.7803, Val Accuracy: 0.8047, Val Brier Score: 0.1363\n",
      "No improvement for 24 epochs.\n",
      "Epoch 106/200, Train Loss: 0.3914, Train Accuracy: 0.8294, Val Loss: 0.4312, Val AUC: 0.7791, Val Accuracy: 0.8058, Val Brier Score: 0.1362\n",
      "No improvement for 25 epochs.\n",
      "Epoch 107/200, Train Loss: 0.3910, Train Accuracy: 0.8297, Val Loss: 0.4268, Val AUC: 0.7811, Val Accuracy: 0.8109, Val Brier Score: 0.1342\n",
      "No improvement for 26 epochs.\n",
      "Epoch 108/200, Train Loss: 0.3914, Train Accuracy: 0.8294, Val Loss: 0.4309, Val AUC: 0.7804, Val Accuracy: 0.8075, Val Brier Score: 0.1361\n",
      "No improvement for 27 epochs.\n",
      "Epoch 109/200, Train Loss: 0.3909, Train Accuracy: 0.8296, Val Loss: 0.4254, Val AUC: 0.7815, Val Accuracy: 0.8115, Val Brier Score: 0.1337\n",
      "No improvement for 28 epochs.\n",
      "Epoch 110/200, Train Loss: 0.3900, Train Accuracy: 0.8300, Val Loss: 0.4299, Val AUC: 0.7819, Val Accuracy: 0.8081, Val Brier Score: 0.1353\n",
      "No improvement for 29 epochs.\n",
      "Epoch 111/200, Train Loss: 0.3906, Train Accuracy: 0.8299, Val Loss: 0.4295, Val AUC: 0.7821, Val Accuracy: 0.8068, Val Brier Score: 0.1353\n",
      "No improvement for 30 epochs.\n",
      "Epoch 112/200, Train Loss: 0.3905, Train Accuracy: 0.8300, Val Loss: 0.4269, Val AUC: 0.7843, Val Accuracy: 0.8103, Val Brier Score: 0.1342\n",
      "No improvement for 31 epochs.\n",
      "Epoch 113/200, Train Loss: 0.3910, Train Accuracy: 0.8300, Val Loss: 0.4268, Val AUC: 0.7823, Val Accuracy: 0.8086, Val Brier Score: 0.1346\n",
      "No improvement for 32 epochs.\n",
      "Epoch 114/200, Train Loss: 0.3902, Train Accuracy: 0.8300, Val Loss: 0.4266, Val AUC: 0.7833, Val Accuracy: 0.8097, Val Brier Score: 0.1340\n",
      "No improvement for 33 epochs.\n",
      "Epoch 115/200, Train Loss: 0.3898, Train Accuracy: 0.8299, Val Loss: 0.4289, Val AUC: 0.7788, Val Accuracy: 0.8090, Val Brier Score: 0.1349\n",
      "No improvement for 34 epochs.\n",
      "Epoch 116/200, Train Loss: 0.3895, Train Accuracy: 0.8304, Val Loss: 0.4271, Val AUC: 0.7843, Val Accuracy: 0.8096, Val Brier Score: 0.1342\n",
      "No improvement for 35 epochs.\n",
      "Epoch 117/200, Train Loss: 0.3894, Train Accuracy: 0.8306, Val Loss: 0.4272, Val AUC: 0.7829, Val Accuracy: 0.8099, Val Brier Score: 0.1342\n",
      "No improvement for 36 epochs.\n",
      "Epoch 118/200, Train Loss: 0.3887, Train Accuracy: 0.8306, Val Loss: 0.4354, Val AUC: 0.7783, Val Accuracy: 0.8041, Val Brier Score: 0.1373\n",
      "No improvement for 37 epochs.\n",
      "Epoch 119/200, Train Loss: 0.3888, Train Accuracy: 0.8306, Val Loss: 0.4292, Val AUC: 0.7836, Val Accuracy: 0.8080, Val Brier Score: 0.1350\n",
      "No improvement for 38 epochs.\n",
      "Epoch 120/200, Train Loss: 0.3883, Train Accuracy: 0.8308, Val Loss: 0.4307, Val AUC: 0.7803, Val Accuracy: 0.8062, Val Brier Score: 0.1356\n",
      "No improvement for 39 epochs.\n",
      "Epoch 121/200, Train Loss: 0.3891, Train Accuracy: 0.8307, Val Loss: 0.4259, Val AUC: 0.7817, Val Accuracy: 0.8086, Val Brier Score: 0.1341\n",
      "No improvement for 40 epochs.\n",
      "Epoch 122/200, Train Loss: 0.3890, Train Accuracy: 0.8306, Val Loss: 0.4298, Val AUC: 0.7790, Val Accuracy: 0.8072, Val Brier Score: 0.1351\n",
      "No improvement for 41 epochs.\n",
      "Epoch 123/200, Train Loss: 0.3883, Train Accuracy: 0.8309, Val Loss: 0.4293, Val AUC: 0.7831, Val Accuracy: 0.8087, Val Brier Score: 0.1345\n",
      "No improvement for 42 epochs.\n",
      "Epoch 124/200, Train Loss: 0.3889, Train Accuracy: 0.8309, Val Loss: 0.4314, Val AUC: 0.7817, Val Accuracy: 0.8069, Val Brier Score: 0.1353\n",
      "No improvement for 43 epochs.\n",
      "Epoch 125/200, Train Loss: 0.3878, Train Accuracy: 0.8312, Val Loss: 0.4310, Val AUC: 0.7815, Val Accuracy: 0.8108, Val Brier Score: 0.1344\n",
      "No improvement for 44 epochs.\n",
      "Epoch 126/200, Train Loss: 0.3880, Train Accuracy: 0.8311, Val Loss: 0.4284, Val AUC: 0.7824, Val Accuracy: 0.8107, Val Brier Score: 0.1339\n",
      "No improvement for 45 epochs.\n",
      "Epoch 127/200, Train Loss: 0.3873, Train Accuracy: 0.8314, Val Loss: 0.4321, Val AUC: 0.7792, Val Accuracy: 0.8073, Val Brier Score: 0.1357\n",
      "No improvement for 46 epochs.\n",
      "Epoch 128/200, Train Loss: 0.3872, Train Accuracy: 0.8315, Val Loss: 0.4332, Val AUC: 0.7802, Val Accuracy: 0.8092, Val Brier Score: 0.1354\n",
      "No improvement for 47 epochs.\n",
      "Epoch 129/200, Train Loss: 0.3883, Train Accuracy: 0.8313, Val Loss: 0.4324, Val AUC: 0.7796, Val Accuracy: 0.8053, Val Brier Score: 0.1363\n",
      "No improvement for 48 epochs.\n",
      "Epoch 130/200, Train Loss: 0.3871, Train Accuracy: 0.8312, Val Loss: 0.4298, Val AUC: 0.7800, Val Accuracy: 0.8063, Val Brier Score: 0.1356\n",
      "No improvement for 49 epochs.\n",
      "Epoch 131/200, Train Loss: 0.3868, Train Accuracy: 0.8316, Val Loss: 0.4272, Val AUC: 0.7847, Val Accuracy: 0.8097, Val Brier Score: 0.1340\n",
      "No improvement for 50 epochs.\n",
      "Early stopping at epoch 131. Best AUC: 0.7881. Best Brier Score: 0.1340\n"
     ]
    }
   ],
   "source": [
    "# lstm continous (create subsequences)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, brier_score_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "results = {}\n",
    "\n",
    "datasets = [\n",
    "    # \"aki_stage_X_extended_1H.csv\",\n",
    "    # \"aki_stage_X_extended_2H.csv\",\n",
    "    # \"aki_stage_X_extended_4H.csv\",\n",
    "    # \"aki_stage_X_extended_6H.csv\",\n",
    "    # \"aki_stage_X_extended_8H.csv\",\n",
    "    \"aki_stage_X_extended_12H.csv\",\n",
    "    # \"aki_stage_X_extended_24H.csv\",\n",
    "    # \"aki_stage_X_original_1H.csv\",\n",
    "    # \"aki_stage_X_original_2H.csv\",\n",
    "    # \"aki_stage_X_original_4H.csv\",\n",
    "    # \"aki_stage_X_original_6H.csv\",\n",
    "    # \"aki_stage_X_original_8H.csv\",\n",
    "    # \"aki_stage_X_original_12H.csv\",\n",
    "    # \"aki_stage_X_original_24H.csv\",\n",
    "]\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch_data = data[i:i+batch_size]\n",
    "        \n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        # Generate subsequences for each sequence in the batch\n",
    "        for seq in batch_data:\n",
    "            for j in range(1, len(seq) + 1):\n",
    "                subsequence = seq[:j]  # From index 0 to j (increasing subsequence)\n",
    "                X_batch.append(subsequence[:, 1:-1])  # Exclude icustay_id and aki_stage\n",
    "                y_batch.append(subsequence[-1, -1])  # Label of the last row of the subsequence\n",
    "        \n",
    "        # Pad sequences within the batch to the same length\n",
    "        max_seq_length = max(len(seq) for seq in X_batch)\n",
    "        X_batch_padded = [np.pad(seq, ((0, max_seq_length - len(seq)), (0, 0)), mode='constant') for seq in X_batch]\n",
    "        \n",
    "        X_batches.append(torch.FloatTensor(X_batch_padded))\n",
    "        y_batches.append(torch.LongTensor(y_batch))\n",
    "    \n",
    "    return X_batches, y_batches\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, emb_size, num_layers=number_layers, \n",
    "                            bidirectional=bi_directional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(emb_size * (2 if bi_directional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        \n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "for dataset in datasets:\n",
    "    tail = dataset\n",
    "    data_path = os.path.join(base_data_path, \"resampled\", dataset)\n",
    "    print(f\"Processing {tail}\")\n",
    "\n",
    "    X = pd.read_csv(data_path)\n",
    "\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage')\n",
    "    numeric_feat.remove('icustay_id')\n",
    "\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    id_list = X['icustay_id'].unique()\n",
    "    id_train, id_test_val = train_test_split(id_list, test_size=SPLIT_SIZE, random_state=42)\n",
    "    id_valid, id_test = train_test_split(id_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "    X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id', 'charttime'])\n",
    "    test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id', 'charttime'])\n",
    "    validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "    train.drop(['charttime'], axis=1, inplace=True)\n",
    "    test.drop(['charttime'], axis=1, inplace=True)\n",
    "    validation.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "    train = list(train.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
    "    train.sort(key=len, reverse=True)\n",
    "    test = list(test.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
    "    validation = list(validation.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
    "\n",
    "    batch_size = 32\n",
    "    X_train, y_train = batch(train, batch_size)\n",
    "    X_test, y_test = batch(test, batch_size)\n",
    "    X_val, y_val = batch(validation, batch_size)\n",
    "\n",
    "    print(f\"Number of batches in train: {len(X_train)}\")\n",
    "    os.makedirs(os.path.join(base_data_path, \"models\", tail), exist_ok=True)\n",
    "    writer = SummaryWriter(os.path.join(base_data_path, \"logs\", tail, datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "\n",
    "    input_size = X_train[0].shape[2]\n",
    "    print(f\"Input size: {input_size}\")\n",
    "    output_size = 1\n",
    "    emb_size = round(input_size / 2) \n",
    "    number_layers = 3\n",
    "    dropout = 0.2\n",
    "    bi_directional = False\n",
    "\n",
    "    nn_model = Net(input_size, emb_size, output_size, bi_directional, number_layers, dropout).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "    use_pretrained = False\n",
    "    best_auc = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    patience = 50\n",
    "    no_improvement_counter = 0\n",
    "    n_epochs = 200\n",
    "\n",
    "    for epoch in range(start_epoch + 1, n_epochs):\n",
    "        nn_model.train()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "\n",
    "        for i, (X_batch, y_batch) in enumerate(zip(X_train, y_train)):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = nn_model(X_batch)\n",
    "            loss = criterion(outputs, y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            train_accuracy = accuracy_score(y_batch.cpu().numpy(), predicted.cpu().numpy())\n",
    "            running_accuracy += train_accuracy\n",
    "\n",
    "            writer.add_scalar('Training/Loss', loss.item(), epoch * len(X_train) + i)\n",
    "            writer.add_scalar('Training/Accuracy', train_accuracy, epoch * len(X_train) + i)\n",
    "\n",
    "        nn_model.eval()\n",
    "        total_v_loss = 0\n",
    "        all_y_val = []\n",
    "        all_val_prob = []\n",
    "        all_accuracy = 0\n",
    "\n",
    "        for X_val_batch, y_val_batch in zip(X_val, y_val):\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                v_out = nn_model(X_val_batch)\n",
    "                v_loss = criterion(v_out, y_val_batch.float())\n",
    "                val_prob = torch.sigmoid(v_out)\n",
    "                total_v_loss += v_loss.item()\n",
    "                all_y_val.extend(y_val_batch.cpu().numpy())\n",
    "                all_val_prob.extend(val_prob.cpu().numpy())\n",
    "\n",
    "                predicted = val_prob > 0.5\n",
    "                val_accuracy = accuracy_score(y_val_batch.cpu().numpy(), predicted.cpu().numpy())\n",
    "                all_accuracy += val_accuracy\n",
    "\n",
    "        avg_v_loss = total_v_loss / len(X_val)\n",
    "        roc_auc = roc_auc_score(all_y_val, all_val_prob)\n",
    "        avg_accuracy = all_accuracy / len(X_val)\n",
    "        brier_score = brier_score_loss(all_y_val, all_val_prob)\n",
    "\n",
    "        writer.add_scalar('Validation/Loss', avg_v_loss, epoch)\n",
    "        writer.add_scalar('Validation/AUC', roc_auc, epoch)\n",
    "        writer.add_scalar('Validation/Accuracy', avg_accuracy, epoch)\n",
    "        writer.add_scalar('Validation/Brier_Score', brier_score, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, \"\n",
    "              f\"Train Loss: {running_loss/len(X_train):.4f}, \"\n",
    "              f\"Train Accuracy: {running_accuracy/len(X_train):.4f}, \"\n",
    "              f\"Val Loss: {avg_v_loss:.4f}, \"\n",
    "              f\"Val AUC: {roc_auc:.4f}, \"\n",
    "              f\"Val Accuracy: {avg_accuracy:.4f}, \"\n",
    "              f\"Val Brier Score: {brier_score:.4f}\")\n",
    "\n",
    "        if roc_auc > best_auc:\n",
    "            best_auc = roc_auc\n",
    "            no_improvement_counter = 0\n",
    "            save_path = f'data/models/{tail}/LSTM_best.pth'\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            torch.save({\n",
    "                'model_state_dict': nn_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer,\n",
    "                'best_auc': best_auc,\n",
    "                'epoch': epoch,\n",
    "                'brier_score': brier_score\n",
    "            }, save_path)\n",
    "            print(f\"Model saved with AUC: {roc_auc:.4f} and Brier Score: {brier_score:.4f}\")\n",
    "        else:\n",
    "            no_improvement_counter += 1\n",
    "            print(f\"No improvement for {no_improvement_counter} epochs.\")\n",
    "\n",
    "        # Stop if no improvement for 'patience' epochs\n",
    "        if no_improvement_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}. Best AUC: {best_auc:.4f}. Best Brier Score: {brier_score:.4f}\")\n",
    "            break\n",
    "\n",
    "    writer.close()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate pretrained lstm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import collections\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "\n",
    "data_path = \"data/preprocessed/preprocessed_data_6H.csv\"\n",
    "tail = data_path.split(\"/\")[-1]\n",
    "print(f\"Processing {tail}\")\n",
    "\n",
    "X = pd.read_csv(data_path)\n",
    "\n",
    "# Preprocessing steps (similar to XGBoost)\n",
    "numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_feat.remove('aki_stage')\n",
    "numeric_feat.remove('icustay_id')\n",
    "\n",
    "X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "try:\n",
    "    X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Split data (you may want to use the same splitting logic as in XGBoost)\n",
    "id_list = X['icustay_id'].unique()\n",
    "# id_list = common_id_list\n",
    "id_train, id_test_val = train_test_split(id_list, test_size=SPLIT_SIZE, random_state=42)\n",
    "id_valid, id_test = train_test_split(id_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index=True)\n",
    "validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id'])\n",
    "\n",
    "train.drop(['charttime'], axis=1, inplace=True)\n",
    "test.drop(['charttime'], axis=1, inplace=True)\n",
    "validation.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "# Prepare data for LSTM\n",
    "# X_train, y_train = batch(train.to_numpy(), batch_size)\n",
    "# X_test, y_test = batch(test.to_numpy(), test.shape[0])\n",
    "# X_val, y_val = batch(validation.to_numpy(), validation.shape[0])\n",
    "X_train, y_train = batch(train, batch_size)\n",
    "X_test, y_test = batch(test, batch_size)\n",
    "X_val, y_val = batch(validation, batch_size)\n",
    "\n",
    "# LSTM parameters\n",
    "input_size = X_train[0].shape[2]  # Subtract 2 for icustay_id and aki_stage\n",
    "output_size = 1\n",
    "emb_size = round(input_size / 1)\n",
    "number_layers = 3\n",
    "dropout = 0\n",
    "bi_directional = True\n",
    "\n",
    "\n",
    "# Assuming Net is defined elsewhere\n",
    "# Assuming X_train, y_train, X_val, y_val, X_test, y_test are defined and split into batches if necessary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "nn_model = Net(input_size, emb_size, output_size, bi_directional, number_layers, dropout).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Test evaluation with F1 score\n",
    "nn_model.load_state_dict(torch.load(f'data/models/{tail}/LSTM_best.pth')['model_state_dict'])\n",
    "nn_model.eval()\n",
    "total_test_loss = 0\n",
    "all_y_test = []\n",
    "all_test_prob = []\n",
    "all_test_f1 = 0\n",
    "\n",
    "for X_test_batch, y_test_batch in zip(X_test, y_test):\n",
    "    X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        t_out = nn_model(X_test_batch)\n",
    "        t_out = torch.flatten(t_out)\n",
    "        y_test_batch = y_test_batch.type_as(t_out)\n",
    "        test_loss = criterion(t_out, y_test_batch)\n",
    "        test_prob = torch.sigmoid(t_out)\n",
    "        total_test_loss += test_loss.item()\n",
    "        all_y_test.extend(y_test_batch.cpu().numpy())\n",
    "        all_test_prob.extend(test_prob.cpu().numpy())\n",
    "        \n",
    "        predicted = torch.sigmoid(t_out) > 0.08\n",
    "        test_f1 = f1_score(y_test_batch.cpu().numpy(), predicted.cpu().numpy(), zero_division=1)\n",
    "        all_test_f1 += test_f1\n",
    "        \n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(all_y_test, all_test_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "os.makedirs('data/plots', exist_ok=True)\n",
    "plt.savefig(f'data/plots/ROC_{tail}.png')  # Save ROC curve\n",
    "plt.close()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(all_y_test, all_test_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='PR curve (area = %0.2f)' % pr_auc)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "os.makedirs('data/plots', exist_ok=True)\n",
    "plt.savefig(f'data/plots/PR_{tail}.png')  # Save PR curve\n",
    "plt.close()\n",
    "\n",
    "print(f\"Test Loss: {total_test_loss / len(X_test):.4f}, \"\n",
    "    f\"Test AUC: {roc_auc:.4f}, \"\n",
    "    f\"Test F1: {all_test_f1 / len(X_test):.4f}, \"\n",
    "    f\"Test PR AUC: {pr_auc:.4f}\")\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.load(f'data/models/{tail}/LSTM_best.pth')['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import numpy as np\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(all_y_test, all_test_prob)\n",
    "# Find the optimal threshold\n",
    "optimal_idx = np.argmin(np.sqrt(np.square(1-tpr) + np.square(fpr)))\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(all_y_test, all_test_prob)\n",
    "# Add a last threshold corresponding to recall = 0.\n",
    "thresholds = np.append(thresholds, 1)\n",
    "\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "# Find the optimal threshold\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all_y_test and all_test_prob to numpy arrays for easier manipulation\n",
    "all_y_test = np.array(all_y_test)\n",
    "all_test_prob = np.array(all_test_prob)\n",
    "\n",
    "# Initialize variables to store the best threshold and its corresponding F1 score\n",
    "best_threshold = 0.0\n",
    "best_f1 = 0.0\n",
    "\n",
    "# Iterate over a range of possible threshold values (e.g., 0 to 1, step 0.01)\n",
    "for threshold in np.arange(0.0, 1.01, 0.01):\n",
    "    # Convert probabilities to binary predictions based on the current threshold\n",
    "    predictions = (all_test_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate F1 score for the current threshold\n",
    "    f1 = f1_score(all_y_test, predictions, zero_division=1)\n",
    "    \n",
    "    # Update best threshold and F1 score if the current F1 score is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "# Print the best threshold and its corresponding F1 score\n",
    "print(f\"Best Threshold: {best_threshold}\")\n",
    "print(f\"Best F1 Score: {best_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Training on CPU') # On mac book GPU is not possible =() \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(np.array(train)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    times = math.floor(data.shape[0]/batch_size)\n",
    "    remainder = data.shape[0]%times\n",
    "    a = 0\n",
    "    start = 0\n",
    "    end = start+batch_size\n",
    "    if remainder ==0:\n",
    "        a +=1\n",
    "    while a<times:\n",
    "        temp = pad(data[start:end,],0)\n",
    "        x = torch.from_numpy(temp[:,:,1:-1]).float() # without icustay_id and without aki_stage columns\n",
    "        y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "        X_batches.append(x)\n",
    "        y_batches.append(y)\n",
    "        start = end\n",
    "        end = start+batch_size\n",
    "        a +=1\n",
    "    temp = pad(data[start:data.shape[0]],0)\n",
    "    x = torch.from_numpy(temp[:,:,1:-1]).float()\n",
    "    y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "    X_batches.append(x)\n",
    "    y_batches.append(y)\n",
    "    if len(X_batches) != len(y_batches):\n",
    "        print(\"length error\")\n",
    "    return X_batches, y_batches # arrays\n",
    "\n",
    "# batching\n",
    "X_train, y_train = batch(train, batch_size) # to count weights\n",
    "\n",
    "# counting balance of the classes\n",
    "y = []\n",
    "for i in y_train:\n",
    "    for element in i:\n",
    "        y.append(element.item())\n",
    "\n",
    "#  weights\n",
    "counter=collections.Counter(y)\n",
    "print(counter)\n",
    "zeroes = counter[0]\n",
    "ones = counter[1]\n",
    "\n",
    "X_test, y_test = batch(test, test.shape[0]) \n",
    "X_val, y_val = batch(validation, validation.shape[0])\n",
    "X_val = X_val[0]\n",
    "y_val = y_val[0]\n",
    "X_test = X_test[0]\n",
    "y_test = y_test[0]\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# setup\n",
    "\n",
    "bi_directional = True\n",
    "n_epochs = 50\n",
    "lr = 0.001\n",
    "features = len(X_train[0][0][0])\n",
    "print(features)\n",
    "# features = \n",
    "emb_size = round(features/1)\n",
    "number_layers = 3\n",
    "dropout = 0 # dropout\n",
    "\n",
    "##########################\n",
    "input_size = features\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size \n",
    "        self.output_size = output_size\n",
    "        self.number_layers = number_layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "        # in bidirectional encoder we have  forward and backward hidden states\n",
    "        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "        # Create affine layer to project to the classes \n",
    "        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "        #dropout layer for regularizetion of a sequence\n",
    "        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "        h = self.relu(self.combination_layer(h))\n",
    "        h = self.dropout_layer(h)\n",
    "        h = self.projection(h) \n",
    "        return h\n",
    "\n",
    "#create a network \n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "#print(nn_model)\n",
    "#print(list(nn_model.parameters()))\n",
    "\n",
    "\n",
    "# BCE Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss() # class imbalance\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(round(zeroes/ones,0))) # class imbalance\n",
    "#print(round(zeroes/ones,0))\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=lr) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count unique values\n",
    "unique, counts = np.unique(y_val, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "# Count NaN values\n",
    "nan_count = np.isnan(y_val).sum()\n",
    "print(f\"Number of NaN values: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all nans in X_val with 0\n",
    "X_val[torch.isnan(X_val)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_original = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_original = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training loop (full data 3.5 hours)\n",
    "\n",
    "epochs = n_epochs\n",
    "starttime = datetime.now() # datetime object containing current date and time\n",
    "train_losses, validation_losses = [], []\n",
    "best = 0\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    print (\"\\n Epoch [%d] out of %d\" % (epoch + 1, epochs))\n",
    "    running_loss = 0.0\n",
    "    validation_loss = 0.0\n",
    "    roc_auc = 0.0\n",
    "    pr_auc = 0.0\n",
    "    m = 0\n",
    "    \n",
    "    #train\n",
    "    #print(list(nn_model.parameters())[0])\n",
    "    # pbar = tqdm(X_train, desc=f\"Epoch {epoch+1}\")\n",
    "    # for i in pbar:\n",
    "    #     # zero the parameter gradients\n",
    "    #     optimizer.zero_grad() # zero the gradient buffers not to consider gradients of previous iterations\n",
    "    #     X_batch = X_train[m]\n",
    "    #     y_batch = y_train[m]\n",
    "    #     # print(X_batch.shape)\n",
    "    #     # forward + backward + optimize\n",
    "    #     outputs = nn_model(X_batch)\n",
    "    #     outputs = torch.flatten(outputs)\n",
    "    #     y_batch = y_batch.type_as(outputs)\n",
    "    #     loss = criterion(outputs, y_batch)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step() # Does the update\n",
    "    #     running_loss += loss.item()\n",
    "    #     m +=1\n",
    "    #     pbar.set_postfix({\"Training Loss\": running_loss/len(X_train)})\n",
    "        \n",
    "   \n",
    "    #validation \n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        v_out = nn_model(X_val) \n",
    "        v_out = torch.flatten(v_out) \n",
    "        y_val = y_val.type_as(v_out)\n",
    "        v_loss = criterion(v_out, y_val)\n",
    "        validation_loss = v_loss.item()\n",
    "        # auc and pr auc\n",
    "        val_prob = torch.nn.Sigmoid() (v_out)\n",
    "        print(type(v_out))\n",
    "        print(v_out)\n",
    "        print(val_prob)\n",
    "        print(y_val)\n",
    "        roc_auc = roc_auc_score(y_val,val_prob) \n",
    "        \n",
    "    validation_losses.append(validation_loss) \n",
    "    train_losses.append(running_loss/len(X_train)) \n",
    "    print(f\"Training loss: {running_loss/len(X_train):.3f}.. \" f\"Validation loss: {validation_loss:.3f}.. \")\n",
    "    print(f\"AUC: {roc_auc:.2f}\")  \n",
    "    nn_model.train()\n",
    "    \n",
    "    \n",
    "    if roc_auc > best:\n",
    "        best = roc_auc\n",
    "        PATH = './LSTMbest.pth' \n",
    "        torch.save(nn_model.state_dict(), PATH) # save the model\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "       \n",
    "print('Finished Training')\n",
    "print(\"starttime =\", starttime)\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "PATH = './LSTM.pth' \n",
    "torch.save(nn_model.state_dict(), PATH) # save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "PATH = './LSTM.pth'\n",
    "nn_model.load_state_dict(torch.load(PATH))\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    t_out = nn_model(X_test)\n",
    "    t_out = torch.flatten(t_out)\n",
    "    y_test = y_test.type_as(t_out)\n",
    "    t_loss = criterion(t_out, y_test)\n",
    "    test_loss = t_loss.item()\n",
    "    # auc and pr auc\n",
    "    test_prob = torch.nn.Sigmoid() (t_out)\n",
    "    roc_auc = roc_auc_score(y_test,test_prob) \n",
    "    pr_auc = average_precision_score(y_test,test_prob)\n",
    "    # convert output probabilities to class labels\n",
    "    test_pred = (test_prob > 0.5).float()\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = accuracy_score(y_test.cpu().numpy(), test_pred.cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}.. \" f\"ROC AUC: {roc_auc:.2f}.. \" f\"PR AUC: {pr_auc:.2f}.. \")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a freshly initialized model on test\n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "# nn_model.load_state_dict(torch.load(PATH))\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    t_out = nn_model(X_test)\n",
    "    t_out = torch.flatten(t_out)\n",
    "    y_test = y_test.type_as(t_out)\n",
    "    t_loss = criterion(t_out, y_test)\n",
    "    test_loss = t_loss.item()\n",
    "    # auc and pr auc\n",
    "    test_prob = torch.nn.Sigmoid() (t_out)\n",
    "    roc_auc = roc_auc_score(y_test,test_prob) \n",
    "    pr_auc = average_precision_score(y_test,test_prob)\n",
    "    print(f\"Test loss: {test_loss:.3f}.. \" f\"ROC AUC: {roc_auc:.2f}.. \" f\"PR AUC: {pr_auc:.2f}.. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './i-Bidir_3_lr_0.001_nodropbest.pth'\n",
    "\n",
    "# save the model\n",
    "#torch.save(nn_model.state_dict(), PATH)\n",
    "\n",
    "# code to load saved model\n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "nn_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test) # single batch with zero padding to the max shape 635208"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = nn_model(X_test)\n",
    "pred = torch.nn.Sigmoid() (logits)\n",
    "pred = pred.detach().numpy()\n",
    "pred = pred.reshape(-1,1)\n",
    "print(\"Performance on full X_test where it has no batching: is padded to max dimentions. \\n\")\n",
    "print (\"Area Under ROC Curve: %0.2f\" % roc_auc_score(y_test, pred, average = 'micro')  )\n",
    "brier = round(metrics.brier_score_loss(y_test, pred, sample_weight=None, pos_label=None),3)\n",
    "print(\"Brier score : {:.3f}\".format(brier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('padded_lstm.npy', 'wb') as f:\n",
    "    np.save(f, y_test)\n",
    "    np.save(f, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = X_test.shape[1] #133\n",
    "icustays = X_test.shape[0]\n",
    "times = []\n",
    "auc_s = []\n",
    "t = 0\n",
    "\n",
    "while t < timestamps:\n",
    "    times.append(t+1)\n",
    "    row = t\n",
    "    i = 0\n",
    "    prob_t = []\n",
    "    y_t = []\n",
    "    while i < icustays:\n",
    "        prob_t.append(pred[row])\n",
    "        y_t.append(y_test[row])\n",
    "        row += timestamps\n",
    "        i +=1\n",
    "    prob_t = np.array(prob_t).reshape(-1,1)\n",
    "    y_t = np.array(y_t).reshape(-1,1)\n",
    "    auc_s.append(roc_auc_score(y_t, prob_t, average = 'micro'))\n",
    "    t +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.DataFrame(auc_s, columns = ['AUC'])\n",
    "df['Timestamps'] = times\n",
    "#df[120:133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "sns.lineplot(x=\"Timestamps\", y=\"AUC\", color = 'g',\n",
    "             data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing to LogR, XGB, RF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = batch(test, test.shape[0]) \n",
    "X_test = X_test[0]\n",
    "y_test = y_test[0]\n",
    "\n",
    "\n",
    "def to_one_label (model, label_list,X_test,index_list):\n",
    "    # evaluate on a test set\n",
    "    labels = np.array(label_list)\n",
    "    labels = labels.reshape(-1,1)\n",
    "    labels = labels.astype(int)\n",
    "    logits = model(X_test)\n",
    "    pred = torch.nn.Sigmoid() (logits)\n",
    "    max_rows = pred.shape[1]\n",
    "    predictions = pred.detach().numpy()\n",
    "    predictions = predictions.reshape(-1,1) \n",
    "    # select 1 per icu stay id by index\n",
    "    prob_1_label = []\n",
    "    row = 0\n",
    "    prev = 0\n",
    "    for i in index_list:\n",
    "        prob_1_label.append(predictions[row+i-prev])\n",
    "        row += pred.shape[1]\n",
    "        prev = i\n",
    "    prob_1_label = np.array(prob_1_label).reshape(-1,1)\n",
    "    \n",
    "    return labels, prob_1_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance (y_test, pred_probabilities):\n",
    "    # performance\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    # compute roc auc\n",
    "    roc_auc = roc_auc_score(y_test, pred_probabilities, average = 'micro')\n",
    "    # compute Precision_Recall curves\n",
    "    precision, recall, _ = precision_recall_curve(y_test, pred_probabilities)\n",
    "    # compute PR_AUC\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "       \n",
    "    # I add confusion matrix\n",
    "    optimal_cut_off = round(thresholds[np.argmax(tpr - fpr)],4)\n",
    "    a = np.where(pred_probabilities > optimal_cut_off, 1, 0)\n",
    "    brier = round(metrics.brier_score_loss(y_test, pred_probabilities, sample_weight=None, pos_label=None),3)\n",
    "    predictions = np.where(pred_probabilities > optimal_cut_off, 1, 0)  \n",
    "    \n",
    "    print (\"Area Under ROC Curve: %0.2f\" % roc_auc  )\n",
    "    #print (\"Area Under PR Curve(AP): %0.2f\" % pr_auc  ) \n",
    "    print(\"Brier score : {:.3f}\".format(brier))\n",
    "    #print('Accuracy for Classifier : {:.2f}'.format(accuracy_score(y_test, predictions)))\n",
    "    #print('Cut off: ' + str(optimal_cut_off))\n",
    "    matrix = metrics.confusion_matrix(y_test, a, labels=None, normalize=None)\n",
    "    #print(str(matrix))\n",
    "    \n",
    "    #f.write(\"\\n Area Under ROC Curve: \" +str(roc_auc))\n",
    "    #f.write(\"\\n Area Under PR Curve(AP): \" + str(pr_auc))\n",
    "    #f.write(\"\\n Brier score: \" +str(brier))\n",
    "    #f.write('\\n Accuracy for Classifier '+str(round((accuracy_score(labels, predictions)),3)))\n",
    "    #f.write(\"\\n Cut off: \" +str(optimal_cut_off))\n",
    "    #f.write(str(matrix))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels, prob_1_label = to_one_label (nn_model, label_list,X_test,index_list)\n",
    "performance(labels,prob_1_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save labels, prob_1_label\n",
    "\n",
    "with open('test.npy', 'wb') as f:\n",
    "    #np.save(f, labels)\n",
    "    np.save(f, prob_1_label)\n",
    "with open('test.npy', 'rb') as f:\n",
    "    #lstm_labels = np.load(f)\n",
    "    lstm_prob = np.load(f)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply integrated gradients, we first create an IntegratedGradients object, providing the model object.\n",
    "ig = IntegratedGradients(nn_model)\n",
    "# To compute the integrated gradients, we use the attribute method of the IntegratedGradients object. The method takes\n",
    "# tensor(s) of input examples (matching the forward function of the model), and returns the input attributions for the\n",
    "# given examples. A target index, defining the index of the output for which gradients are computed is 1, \n",
    "# corresponding to AKI (1/0).\n",
    "\n",
    "#The input tensor provided should require grad, so we call requires_grad_ on the tensor. The attribute method also \n",
    "# takes a baseline, which is the starting point from which gradients are integrated. The default value is just the \n",
    "# 0 tensor, which is a reasonable baseline / default for this task.\n",
    "\n",
    "#The returned values of the attribute method are the attributions, which match the size of the given inputs, and delta,\n",
    "# which approximates the error between the approximated integral and true integral.\n",
    "print(datetime.now())\n",
    "X_test.requires_grad_()\n",
    "attr, delta = ig.attribute(X_test,target=1, return_convergence_delta=True)\n",
    "attr = attr.detach().numpy()\n",
    "attr= np.reshape(attr,(-1,35))\n",
    "importances = np.mean(attr, axis=0)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[:,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[:,4].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_importances(feature_names, importances, title=\"LSTM Average Feature Importances\", axis_title=\"Features\"):\n",
    "    print(title)\n",
    "    i = 0\n",
    "    while i < features:\n",
    "        print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "        i +=1\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    \n",
    "visualize_feature_importances(feature_names, importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df =  pd.DataFrame(importances, columns = ['Feature Importance'])\n",
    "lstm_df['Features'] = feature_names\n",
    "lstm_df = lstm_df.sort_values(by = ['Feature Importance'], ascending = False, ignore_index = True)\n",
    "#lstm_df[\"Feature Importance\"] =  lstm_df[\"Feature Importance\"]\n",
    "#lstm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df[\"Feature Importance\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df)\n",
    "ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df, color = 'grey')\n",
    "ax.set_xlabel('Feature Importance', fontsize = 15)\n",
    "ax.set_ylabel(\"Features\",fontsize=15)\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 6)\n",
    "plt.title('LSTM feature Importances')\n",
    "plt.savefig('LSTM_feature_importance_grey.png', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df['abs'] = abs(lstm_df['Feature Importance'])\n",
    "lstm_df = lstm_df.sort_values(by = ['abs'], ascending = False, ignore_index = True)\n",
    "lstm_df_10 = lstm_df.head(10)\n",
    "#lstm_df_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df_10, palette=\"mako\")\n",
    "\n",
    "ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df_10, color = 'darkgreen')\n",
    "ax.set_xlabel('Feature Importance', fontsize = 15)\n",
    "ax.set_ylabel(\"Features\",fontsize=15)\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 10)\n",
    "plt.title('LSTM top 10 features by feature importance')\n",
    "plt.savefig('LSTM_top10_feature_importance_darkgreen.png', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graphs (y_test,pred_probabilities, classifier_name, plot_name, algorithm):\n",
    "    \n",
    "    def bin_total(y_true, y_prob, n_bins):\n",
    "        bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "        # In sklearn.calibration.calibration_curve, the last value in the array is always 0.\n",
    "        binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "        return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "    def missing_bin(bin_array):\n",
    "        midpoint = \" \"    \n",
    "        if bin_array[0]==0:\n",
    "            midpoint = \"5%, \"\n",
    "        if bin_array[1]==0:\n",
    "            midpoint = midpoint + \"15%, \"\n",
    "        if bin_array[2]==0:\n",
    "            midpoint = midpoint + \"25%, \"\n",
    "        if bin_array[3]==0:\n",
    "            midpoint = midpoint + \"35%, \" \n",
    "        if bin_array[4]==0:\n",
    "            midpoint = midpoint + \"45%, \"\n",
    "        if bin_array[5]==0:\n",
    "            midpoint = midpoint + \"55%, \"\n",
    "        if bin_array[6]==0:\n",
    "            midpoint = midpoint + \"65%, \"\n",
    "        if bin_array[7]==0:\n",
    "            midpoint = midpoint + \"75%, \"\n",
    "        if bin_array[8]==0:\n",
    "            midpoint = midpoint + \"85%, \"\n",
    "        if bin_array[9]==0:\n",
    "            midpoint = midpoint + \"95%, \"\n",
    "        return \"The missing bins have midpoint values of \"+ str(midpoint)\n",
    "    \n",
    "    # performance\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    # compute roc auc\n",
    "    roc_auc = roc_auc_score(y_test, pred_probabilities, average = 'micro')\n",
    "    # compute Precision_Recall curves\n",
    "    precision, recall, _ = precision_recall_curve(y_test, pred_probabilities)\n",
    "    # compute PR_AUC\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "\n",
    "    # compute calibration curve\n",
    "    LR_y, LR_x = calibration_curve(y_test, pred_probabilities, n_bins=10)\n",
    "    #find out which one are the missing bins\n",
    "    bin_array = bin_total(y_test, pred_probabilities , n_bins=10)\n",
    "    print(missing_bin(bin_array))\n",
    "\n",
    "    print(\"plot curves and save in one png file\")\n",
    "    #save three plots in one png file\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, figsize=(7, 24))\n",
    "    fig.subplots_adjust(wspace=0.3, hspace= 0.3)\n",
    "    fig.suptitle('Evaluation of '+ plot_name)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    \n",
    "    # plot roc curve\n",
    "    ax1.plot(fpr, tpr,'C2', label=algorithm+\" \"+\"Classifier \" + str(classifier_name) + \", auc=\" +str(round(roc_auc,2)))\n",
    "    ax1.title.set_text('ROC AUC')\n",
    "    ax1.set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot PR curve\n",
    "    ax2.plot(recall, precision,'C2', label=algorithm+\" \"+\"Classifier \" + str(classifier_name) + \", auc=\"+str(round(pr_auc,2)))\n",
    "    ax2.title.set_text('PR AUC')\n",
    "    ax2.set(xlabel='Recall', ylabel='Precision')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot calibration curve\n",
    "    ax3.plot(LR_x, LR_y, 'C2',marker='o', linewidth=1, label='LR')\n",
    "    line = mlines.Line2D([0, 1], [0, 1], color='black')\n",
    "    transform = ax3.transAxes\n",
    "    line.set_transform(transform)\n",
    "    ax3.add_line(line)\n",
    "    ax3.title.set_text('Calibration plot for '+str(plot_name))\n",
    "    ax3.set(xlabel= 'Predicted probability', ylabel= 'True probability in each bin')\n",
    "    ax3.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.savefig(plot_name+\".png\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(pred_probabilities, y_test, dist_name):\n",
    "    #probabilities distributions graphs\n",
    "    true_1 = pd.DataFrame(pred_probabilities, columns=['Predicted probabilities'])\n",
    "    true_1['labels'] = y_test.tolist()\n",
    "    true_0 = true_1.copy(deep = True) \n",
    "    indexNames = true_1[true_1['labels'] == 0].index\n",
    "    true_1.drop(indexNames , inplace=True)\n",
    "    indexNames = true_0[ true_0['labels'] == 1 ].index\n",
    "    true_0.drop(indexNames , inplace=True)\n",
    "    true_1.drop(columns=['labels'], inplace = True)\n",
    "    true_0.drop(columns=['labels'], inplace = True)\n",
    "    \n",
    "    sns.distplot(true_1['Predicted probabilities'], hist = False, kde = True,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3,\"color\": \"g\"}, label = 'Class 1')\n",
    "    plt.ylabel('Density')\n",
    "    sns.distplot(true_0['Predicted probabilities'], hist = False, kde = True,\n",
    "                     kde_kws = {'shade': True, 'linewidth': 3}, label = 'Class 0')\n",
    "    plt.title('Density Plot'+ dist_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distribution(prob_1_label, labels.flatten(), \" Bidirectional LSTM no imputation \")\n",
    "plt.savefig('dist_LSTM_bi_NOimp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"None vs. Any AKI\"    ###change every time #Moderate vs. Severe #None vs. Any #Others vs. Severe\n",
    "plot_name = \"LSTM NO imputation\"\n",
    "build_graphs(labels.flatten(), prob_1_label.flatten(), classifier_name, plot_name, \"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(labels, prob_1_label)\n",
    "fpr, tpr, thresholds = roc_curve(labels, prob_1_label)\n",
    "optimal_cut_off = round(thresholds[np.argmax(tpr - fpr)],2)\n",
    "prediction = np.where(prob_1_label > optimal_cut_off, 1, 0)\n",
    "f1 = f1_score(labels,prediction)\n",
    "prauc =auc(recall, precision)\n",
    "print('F1 = %.3f, PR auc =%.3f' % (f1,prauc))\n",
    "\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(labels[labels==1]) / len(labels)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall,precision, marker='.', label='LSTM')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search grid \n",
    "layers = [1,2,3]\n",
    "l_rate = [0.001, 0.0001]\n",
    "drop = [0,0.2]\n",
    "bidirectionality = [True,False]\n",
    "#loops count\n",
    "hypercount = 0\n",
    "# static parameters\n",
    "n_epochs = 80\n",
    "emb_size = round(features/1)\n",
    "input_size = features\n",
    "output_size = 1\n",
    "###############################\n",
    "\n",
    "f = open('lstm_no_imp_uni.txt', 'w+') #change with or without imp\n",
    "\n",
    "for q1 in bidirectionality:\n",
    "    for q2 in layers:\n",
    "        for q3 in drop:\n",
    "            for q4 in l_rate:\n",
    "                hypercount +=1\n",
    "                name = \"i-Bidir_\" if q1 else \"i-Onedir_\"\n",
    "                name = name+str(q2) + \"_lr_\"+str(q4)\n",
    "                name = name+\"_drop\"+str(q3) if q3 == 0.2 else name+\"_nodrop\"\n",
    "                #set parameters\n",
    "                bi_directional = q1\n",
    "                lr = q4\n",
    "                number_layers = q2\n",
    "                dropout = q3 # dropout\n",
    "                print('hypercount: %d' % hypercount)\n",
    "                print('\\n')\n",
    "                print(name)\n",
    "                f.write('\\n\\n' + str(name)+ '\\n\\n')\n",
    "                    \n",
    "                # create the NN\n",
    "                class Net(nn.Module):\n",
    "                    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "                        super(Net, self).__init__()\n",
    "                        self.input_size = input_size\n",
    "                        self.emb_size = emb_size \n",
    "                        self.output_size = output_size\n",
    "                        self.number_layers = number_layers\n",
    "                        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "                        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "                        # in bidirectional encoder we have  forward and backward hidden states\n",
    "                        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "                        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "                        # Create affine layer to project to the classes \n",
    "                        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "                        #dropout layer for regularizetion of a sequence\n",
    "                        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "                        self.relu = nn.ReLU()\n",
    "\n",
    "                    def forward(self, x):\n",
    "                        h = self.relu(self.fc1(x))\n",
    "                        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "                        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "                        h = self.relu(self.combination_layer(h))\n",
    "                        h = self.dropout_layer(h)\n",
    "                        h = self.projection(h) \n",
    "                        return h\n",
    "\n",
    "                #create a network \n",
    "                nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "                print(nn_model)\n",
    "                #print(list(nn_model.parameters()))\n",
    "                \n",
    "                # BCE Loss and optimizer\n",
    "                criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(round(zeroes/ones,0))) # class imbalance\n",
    "                #print(round(zeroes/ones,0))\n",
    "                optimizer = optim.Adam(nn_model.parameters(), lr=lr) \n",
    "    \n",
    "    \n",
    "                # TRAINING LOOP \n",
    "                epochs = n_epochs\n",
    "                starttime = datetime.now() # datetime object containing current date and time\n",
    "                train_losses, validation_losses = [], []\n",
    "                best = 0\n",
    "                patience = 0\n",
    "                old_auc = 0\n",
    "                old_pr = 0\n",
    "\n",
    "                for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "                    print (\"\\n Epoch [%d] out of %d\" % (epoch + 1, epochs))\n",
    "                    running_loss = 0.0\n",
    "                    validation_loss = 0.0\n",
    "                    roc_auc = 0.0\n",
    "                    pr_auc = 0.0\n",
    "                    m = 0\n",
    "                    \n",
    "                    #train\n",
    "                    #print(list(nn_model.parameters())[0])\n",
    "                    for i in X_train:\n",
    "                        # zero the parameter gradients\n",
    "                        optimizer.zero_grad() # zero the gradient buffers not to consider gradients of previous iterations\n",
    "                        X_batch = X_train[m]\n",
    "                        y_batch = y_train[m]\n",
    "                        # forward + backward + optimize\n",
    "                        outputs = nn_model(X_batch)\n",
    "                        outputs = torch.flatten(outputs)\n",
    "                        y_batch = y_batch.type_as(outputs)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                        loss.backward()\n",
    "                        optimizer.step() # Does the update\n",
    "                        running_loss += loss.item()\n",
    "                        m +=1\n",
    "                    #validation \n",
    "                    nn_model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        v_out = nn_model(X_val) \n",
    "                        v_out = torch.flatten(v_out) \n",
    "                        y_val = y_val.type_as(v_out)\n",
    "                        v_loss = criterion(v_out, y_val)\n",
    "                        validation_loss = v_loss.item()\n",
    "                        # auc and pr auc\n",
    "                        val_prob = torch.nn.Sigmoid() (v_out)\n",
    "                        precision, recall, thresholds = precision_recall_curve(y_val, val_prob)\n",
    "                        pr_auc = auc(recall, precision)\n",
    "                        roc_auc = roc_auc_score(y_val,val_prob) \n",
    "\n",
    "                    validation_losses.append(validation_loss) \n",
    "                    train_losses.append(running_loss/len(X_train)) \n",
    "                    print(f\"Training loss: {running_loss/len(X_train):.3f}.. \" f\"Validation loss: {validation_loss:.3f}.. \")\n",
    "                    print(f\"AUC: {roc_auc:.2f} \" f\"PR AUC: {pr_auc:.2f} \")  \n",
    "                    nn_model.train()\n",
    "\n",
    "                    \n",
    "                    if roc_auc > best:\n",
    "                        best = roc_auc\n",
    "                        PATH1 = './'+str(name)+'best.pth' \n",
    "                        torch.save(nn_model.state_dict(), PATH1) # save the model\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    if roc_auc == old_auc and pr_auc==old_pr:\n",
    "                        patience +=1\n",
    "                    old_auc = roc_auc\n",
    "                    old_pr = pr_auc\n",
    "                    if patience ==10:\n",
    "                        print(\"out of patience\")\n",
    "                        break\n",
    "\n",
    "                print('\\n Finished Training')\n",
    "                print(\"starttime =\", starttime)\n",
    "                now = datetime.now()\n",
    "                print(\"endtime =\", now)\n",
    "                # end of training loop\n",
    "                \n",
    "                PATH2 = './'+str(name)+'last.pth' \n",
    "                torch.save(nn_model.state_dict(), PATH2) # save the model\n",
    "                print('\\n Last model \\n')\n",
    "                labels, probs = to_one_label(nn_model,label_list,X_test,index_list)\n",
    "                performance (nn_model, labels, probs)\n",
    "                \n",
    "                #load the best model\n",
    "                best_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "                best_model.load_state_dict(torch.load(PATH1))\n",
    "                print('\\n Best model \\n')\n",
    "                labels, probs = to_one_label(best_model,label_list,X_test,index_list)\n",
    "                performance (best_model, labels, probs)\n",
    "f.close() \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medinf_cuda",
   "language": "python",
   "name": "medinf_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
