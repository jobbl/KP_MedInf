{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for taurus\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# os.chdir(\"/home/jori152b/DIR/horse/jori152b-medinf/KP_MedInf/model_development\")\n",
    "os.chdir(\"../\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "#data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "# NN\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score, roc_auc_score, auc, accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from captum.attr import IntegratedGradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Training on CPU') # On mac book GPU is not possible =() \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter as constant \n",
    "\n",
    "TESTING = False \n",
    "TEST_SIZE = 0.05\n",
    "\n",
    "SPLIT_SIZE = 0.2 \n",
    "\n",
    "NORMALIZATION = 'min-max' \n",
    "\n",
    "CAPPING_THRESHOLD_UPPER = 0.99\n",
    "CAPPING_THRESHOLD_LOWER = 0.01\n",
    "\n",
    "# How much time the prediction should occur (hours)\n",
    "HOURS_AHEAD = 48\n",
    "\n",
    "NORM_TYPE = 'min_max'\n",
    "\n",
    "RANDOM = 42\n",
    "\n",
    "# LSTM\n",
    "batch_size = 5\n",
    "\n",
    "# naming model and plot\n",
    "classifier_name = \"None vs. Any AKI\"    ###change every time #Moderate vs. Severe #None vs. Any #Others vs. Severe\n",
    "plot_name = \"adult_AnyAKI_LR\"    ###change every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions used later\n",
    "\n",
    "def cap_data(df):\n",
    "    print(\"Capping between the {} and {} quantile\".format(CAPPING_THRESHOLD_LOWER, CAPPING_THRESHOLD_UPPER))\n",
    "    cap_mask = df.columns.difference(['icustay_id', 'charttime', 'aki_stage'])\n",
    "    df[cap_mask] = df[cap_mask].clip(df[cap_mask].quantile(CAPPING_THRESHOLD_LOWER),\n",
    "                                     df[cap_mask].quantile(CAPPING_THRESHOLD_UPPER),\n",
    "                                     axis=1)\n",
    "\n",
    "    return df\n",
    " \n",
    "    \n",
    "def normalise_data(df, norm_mask):\n",
    "    print(\"Normalizing in [0,1] with {} normalization\".format(NORMALIZATION))\n",
    "    \n",
    "    min_values = df[norm_mask].min()\n",
    "    max_values = df[norm_mask].max()\n",
    "    \n",
    "    # Skip normalization for constant columns\n",
    "    for column in norm_mask:\n",
    "        if min_values[column] != max_values[column]:\n",
    "            df[column] = (df[column] - min_values[column]) / (max_values[column] - min_values[column])\n",
    "    \n",
    "    normalization_parameters = {column: {'min': min_values[column], 'max': max_values[column]} for column in norm_mask}\n",
    "    \n",
    "    return df, normalization_parameters\n",
    "\n",
    "\n",
    "# impute missing value in resampleing data with most common based on each id\n",
    "def fast_mode(df, key_cols, value_col):\n",
    "    \"\"\" Calculate a column mode, by group, ignoring null values. \n",
    "    \n",
    "    key_cols : list of str - Columns to groupby for calculation of mode.\n",
    "    value_col : str - Column for which to calculate the mode. \n",
    "\n",
    "    Return\n",
    "    pandas.DataFrame\n",
    "        One row for the mode of value_col per key_cols group. If ties, returns the one which is sorted first. \"\"\"\n",
    "    return (df.groupby(key_cols + [value_col]).size() \n",
    "              .to_frame('counts').reset_index() \n",
    "              .sort_values('counts', ascending=False) \n",
    "              .drop_duplicates(subset=key_cols)).drop('counts',axis=1)\n",
    "\n",
    "\n",
    "#get max shape of 3d array\n",
    "def get_dimensions(array, level=0):   \n",
    "    yield level, len(array)\n",
    "    try:\n",
    "        for row in array:\n",
    "            yield from get_dimensions(row, level + 1)\n",
    "    except TypeError: #not an iterable\n",
    "        pass\n",
    "\n",
    "def get_max_shape(array):\n",
    "    dimensions = defaultdict(int)\n",
    "    for level, length in get_dimensions(array):\n",
    "        dimensions[level] = max(dimensions[level], length)\n",
    "    return [value for _, value in sorted(dimensions.items())]\n",
    "\n",
    "#pad the ragged 3d array to rectangular shape based on max size\n",
    "def iterate_nested_array(array, index=()):\n",
    "    try:\n",
    "        for idx, row in enumerate(array):\n",
    "            yield from iterate_nested_array(row, (*index, idx)) \n",
    "    except TypeError: # final level            \n",
    "        yield (*index, slice(len(array))), array # think of the types\n",
    "\n",
    "def pad(array, fill_value):\n",
    "    dimensions = get_max_shape(array)\n",
    "    result = np.full(dimensions, fill_value, dtype = np.float64)  \n",
    "    for index, value in iterate_nested_array(array):\n",
    "        result[index] = value \n",
    "    return result\n",
    "\n",
    "def bin_total(y_true, y_prob, n_bins):\n",
    "    bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "    # In sklearn.calibration.calibration_curve,\n",
    "    # the last value in the array is always 0.\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "def missing_bin(bin_array):\n",
    "    midpoint = \" \"    \n",
    "    if bin_array[0]==0:\n",
    "        midpoint = \"5%, \"\n",
    "    if bin_array[1]==0:\n",
    "        midpoint = midpoint + \"15%, \"\n",
    "    if bin_array[2]==0:\n",
    "        midpoint = midpoint + \"25%, \"\n",
    "    if bin_array[3]==0:\n",
    "        midpoint = midpoint + \"35%, \" \n",
    "    if bin_array[4]==0:\n",
    "        midpoint = midpoint + \"45%, \"\n",
    "    if bin_array[5]==0:\n",
    "        midpoint = midpoint + \"55%, \"\n",
    "    if bin_array[6]==0:\n",
    "        midpoint = midpoint + \"65%, \"\n",
    "    if bin_array[7]==0:\n",
    "        midpoint = midpoint + \"75%, \"\n",
    "    if bin_array[8]==0:\n",
    "        midpoint = midpoint + \"85%, \"\n",
    "    if bin_array[9]==0:\n",
    "        midpoint = midpoint + \"95%, \"\n",
    "    return \"The missing bins have midpoint values of \"+ str(midpoint)\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    times = math.floor(data.shape[0]/batch_size)\n",
    "    remainder = data.shape[0]%times\n",
    "    a = 0\n",
    "    start = 0\n",
    "    end = start+batch_size\n",
    "    if remainder ==0:\n",
    "        a +=1\n",
    "    while a<times:\n",
    "        temp = pad(data[start:end,],0)\n",
    "        x = torch.from_numpy(temp[:,:,1:-1]).float() # without icustay_id and without aki_stage columns\n",
    "        y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "        X_batches.append(x)\n",
    "        y_batches.append(y)\n",
    "        start = end\n",
    "        end = start+batch_size\n",
    "        a +=1\n",
    "    temp = pad(data[start:data.shape[0]],0)\n",
    "    x = torch.from_numpy(temp[:,:,1:-1]).float()\n",
    "    y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "    X_batches.append(x)\n",
    "    y_batches.append(y)\n",
    "    if len(X_batches) != len(y_batches):\n",
    "        print(\"length error\")\n",
    "    return X_batches, y_batches # arrays\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size \n",
    "        self.output_size = output_size\n",
    "        self.number_layers = number_layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "        # in bidirectional encoder we have  forward and backward hidden states\n",
    "        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "        # Create affine layer to project to the classes \n",
    "        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "        #dropout layer for regularizetion of a sequence\n",
    "        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "        h = self.relu(self.combination_layer(h))\n",
    "        h = self.dropout_layer(h)\n",
    "        h = self.projection(h) \n",
    "        return h\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: load best features\n",
    "optimal_features = np.load(\"data/optimal_features.npy\", allow_pickle=True)\n",
    "# Extracting feature names (keys) from optimal_features\n",
    "optimal_feature_names = [feature[0] for feature in optimal_features]\n",
    "# include also aki_stage and icustay_id\n",
    "optimal_feature_names.extend(['aki_stage', 'icustay_id', 'charttime'])\n",
    "print(optimal_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original = pd.read_csv(\"data/preprocessed/preprocessed_data_6H.csv\")\n",
    "extended = pd.read_csv(\"data/preprocessed/preprocessed_data_extended_1H.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(original.columns))\n",
    "print(len(extended.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import os\n",
    "import gc\n",
    "\n",
    "results = {}\n",
    "\n",
    "\n",
    "data_paths = [\n",
    "    # \"data/preprocessed/preprocessed_data_1H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_2H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_4H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_6H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_8H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_12H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_24H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data.csv\",\n",
    "              ]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    \n",
    "    tail = data_path.split(\"/\")[-1]\n",
    "    X = pd.read_csv(data_path)\n",
    "    # take only head \n",
    "    # X = X.head(10000)\n",
    "\n",
    "    # For training a testing model, take only icu_stay_id, charttime,creatinine_mean,uo_rt_6hr,aki_stage\n",
    "    # X = X[['icustay_id', 'charttime', 'creatinine_mean', 'uo_rt_6hr', 'aki_stage']]\n",
    "\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage',)\n",
    "    numeric_feat.remove('icustay_id',)\n",
    "\n",
    "\n",
    "    # normalize data and cap features\n",
    "    # X = cap_data(X)\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    \n",
    "    print(len(X.columns))\n",
    "    print(X.columns)\n",
    "\n",
    "    # X = X.sort_values(by=['icustay_id', 'charttime'])\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    seq_lengths = X.groupby(['icustay_id'],as_index=False).size().sort_values(by = ['size'],ascending=False)\n",
    "    sequence_length = seq_lengths.max() # the longest sequence per icustay-id\n",
    "\n",
    "    #AL re-write as try except to make it work as hadm_id is not used if only one csv file is used and none are merged\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    id_list = X['icustay_id'].unique()\n",
    "    # take the common id list defined earlier\n",
    "    # id_list = common_id_list\n",
    "    \n",
    "    id_train, id_test_val = train_test_split(id_list, test_size = SPLIT_SIZE, random_state = 42) # train set is 80%)\n",
    "    # remaining 20% split in halves as test and validation 10% and 10%\n",
    "    id_valid, id_test = train_test_split(id_test_val, test_size = 0.5, random_state = 42) # test 10% valid 10%\n",
    "\n",
    "    # move (\"aki_stage\") to last column\n",
    "    X = X.reindex(columns = [col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "    test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index = True) \n",
    "    validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id']) \n",
    "\n",
    "    test = test.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "    train = train.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "    validation = validation.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "\n",
    "    train.drop(['charttime'], axis=1, inplace = True)  \n",
    "    test.drop(['charttime'], axis=1, inplace = True)\n",
    "    validation.drop(['charttime'], axis=1, inplace = True)\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "\n",
    "\n",
    "    # flatten the train, test and validation data\n",
    "    train_flat = np.concatenate(train, axis=0)\n",
    "    test_flat = np.concatenate(test, axis=0)\n",
    "    validation_flat = np.concatenate(validation, axis=0)\n",
    "\n",
    "    # get the labels\n",
    "    train_labels = np.array([x[-1] for x in train_flat])\n",
    "    test_labels = np.array([x[-1] for x in test_flat])\n",
    "    validation_labels = np.array([x[-1] for x in validation_flat])\n",
    "\n",
    "    # get the features\n",
    "    train_features = np.array([x[1:-1] for x in train_flat])\n",
    "    validation_features = np.array([x[1:-1] for x in validation_flat])\n",
    "    test_features = np.array([x[1:-1] for x in test_flat])\n",
    "\n",
    "    # create the XGBoost classifier\n",
    "    xgb = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM)\n",
    "\n",
    "    # train the classifier\n",
    "    xgb.fit(train_features, train_labels)\n",
    "\n",
    "    # get the predictions\n",
    "    train_predictions = xgb.predict(train_features)\n",
    "    test_predictions = xgb.predict(test_features)\n",
    "    validation_predictions = xgb.predict(validation_features)\n",
    "\n",
    "    # get the accuracy\n",
    "    train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "    test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    validation_accuracy = accuracy_score(validation_labels, validation_predictions)\n",
    "\n",
    "    # get the probabilities of the positive class\n",
    "    training_prob = xgb.predict_proba([x[1:-1] for x in train_flat])[:, 1]\n",
    "    test_prob = xgb.predict_proba([x[1:-1] for x in test_flat])[:, 1]\n",
    "    validation_prob = xgb.predict_proba([x[1:-1] for x in validation_flat])[:, 1]\n",
    "\n",
    "    # calculate ROC AUC and PR AUC for the training set\n",
    "    training_roc_auc = roc_auc_score(train_labels, training_prob)\n",
    "    training_pr_auc = average_precision_score(train_labels, training_prob)\n",
    "\n",
    "    # calculate ROC AUC and PR AUC for the test set\n",
    "    test_roc_auc = roc_auc_score(test_labels, test_prob)\n",
    "    test_pr_auc = average_precision_score(test_labels, test_prob)\n",
    "\n",
    "    # calculate ROC AUC and PR AUC for the validation set\n",
    "    validation_roc_auc = roc_auc_score(validation_labels, validation_prob)\n",
    "    validation_pr_auc = average_precision_score(validation_labels, validation_prob)\n",
    "    \n",
    "    print(f\"Results for {tail}\")\n",
    "    print(f\"Train accuracy: {train_accuracy:.3f}.. Train ROC AUC: {training_roc_auc:.2f}.. Train PR AUC: {training_pr_auc:.2f}..\")\n",
    "    print(f\"Test accuracy: {test_accuracy:.3f}.. Test ROC AUC: {test_roc_auc:.2f}.. Test PR AUC: {test_pr_auc:.2f}..\")\n",
    "    print(f\"Validation accuracy: {validation_accuracy:.3f}.. Validation ROC AUC: {validation_roc_auc:.2f}.. Validation PR AUC: {validation_pr_auc:.2f}..\")\n",
    "\n",
    "    now = datetime.now()\n",
    "    out_path = f'data/models/{tail}'\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    # save the xgb model\n",
    "    xgb.save_model(f'{out_path}/xgb.model')\n",
    "    # save normalization parameters\n",
    "    try:\n",
    "        np.save(f'{out_path}/normalization_parameters.npy', normalization_parameters)\n",
    "    except:\n",
    "        pass\n",
    "    # save the train feature names\n",
    "    np.save(f'{out_path}/train_feature_names.npy', X.columns[2:-1])\n",
    "\n",
    "    results[tail] = {'train_accuracy': train_accuracy, 'test_accuracy': test_accuracy, 'validation_accuracy': validation_accuracy,\n",
    "                                'train_roc_auc': training_roc_auc, 'test_roc_auc': test_roc_auc, 'validation_roc_auc': validation_roc_auc,\n",
    "                                'train_pr_auc': training_pr_auc, 'test_pr_auc': test_pr_auc, 'validation_pr_auc': validation_pr_auc}\n",
    "\n",
    "    # save results dict\n",
    "    np.save(f'{out_path}/results.npy', results)\n",
    "    \n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = [\n",
    "    \"data/preprocessed/preprocessed_data_1H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_2H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_4H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_6H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_8H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_12H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_24H.csv\",\n",
    "]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    X = pd.read_csv(data_path)\n",
    "    X.drop(['height_first', 'hadm_id', 'weight_first', 'inr_max'], axis=1, inplace = True)\n",
    "    # write back to the same file\n",
    "    X.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "results = {}\n",
    "\n",
    "data_paths = [\n",
    "    \"../data/preprocessed/preprocessed_data_extended_1H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_2H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_3H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_4H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_5H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data_extended_6H.csv\",\n",
    "    \"../data/preprocessed/preprocessed_data.csv\",\n",
    "]\n",
    "\n",
    "for data_path in data_paths:\n",
    "    tail = data_path.split(\"/\")[-1]\n",
    "    X = pd.read_csv(data_path)\n",
    "\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage')\n",
    "    numeric_feat.remove('icustay_id')\n",
    "\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # id_list = X['icustay_id'].unique()\n",
    "    id_list = common_id_list\n",
    "\n",
    "    common_id_list.sort()\n",
    "    print(common_id_list[:10])\n",
    "\n",
    "    print(len(id_list))\n",
    "\n",
    "    # Move \"aki_stage\" to last column\n",
    "    X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    X.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "    # Group by icustay_id and convert to numpy arrays\n",
    "    grouped_data = X.groupby('icustay_id').apply(lambda x: x.drop('icustay_id', axis=1).to_numpy())\n",
    "\n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_results = []\n",
    "\n",
    "    print(len(X))\n",
    "\n",
    "    for fold, (id_train_idx, id_val_idx) in enumerate(kf.split(id_list), 1):\n",
    "        print(f\"Processing fold {fold}\")\n",
    "        id_train = [id_list[idx] for idx in id_train_idx]\n",
    "        id_val = [id_list[idx] for idx in id_val_idx]       \n",
    "\n",
    "        print(len(id_train), len(id_val))\n",
    "        print(id_train[:10], id_val[:10])\n",
    "        train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "        validation = X[X.icustay_id.isin(id_val)].sort_values(by=['icustay_id']) \n",
    "\n",
    "        print(len(train), len(validation))\n",
    "\n",
    "        train = train.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "        validation = validation.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "\n",
    "        try:\n",
    "            X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(len(train), len(validation))\n",
    "\n",
    "        train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "        validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "\n",
    "        print(len(train), len(validation))\n",
    "\n",
    "        # flatten the train, test and validation data\n",
    "        train_flat = np.concatenate(train, axis=0)\n",
    "        validation_flat = np.concatenate(validation, axis=0)\n",
    "\n",
    "        # get the labels\n",
    "        train_labels = np.array([x[-1] for x in train_flat])\n",
    "        val_labels = np.array([x[-1] for x in validation_flat])\n",
    "\n",
    "        # get the features\n",
    "        train_features = np.array([x[1:-1] for x in train_flat])\n",
    "        val_features = np.array([x[1:-1] for x in validation_flat])\n",
    "\n",
    "        # Create and train the XGBoost classifier\n",
    "        xgb = XGBClassifier(n_estimators=1000, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "        xgb.fit(train_features, train_labels)\n",
    "\n",
    "        # Make predictions\n",
    "        train_predictions = xgb.predict(train_features)\n",
    "        val_predictions = xgb.predict(val_features)\n",
    "\n",
    "        # unique values in the labels\n",
    "        unique_labels = np.unique(np.concatenate([train_labels, val_labels]))\n",
    "        print(f\"Unique labels: {unique_labels}\")\n",
    "        unique_labels_pred = np.unique(np.concatenate([train_predictions, val_predictions]))\n",
    "        print(f\"Unique predictions: {unique_labels_pred}\")\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "        val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        \n",
    "\n",
    "        # Calculate probabilities\n",
    "        train_prob = xgb.predict_proba(train_features)[:, 1]\n",
    "        val_prob = xgb.predict_proba(val_features)[:, 1]\n",
    "\n",
    "        # Calculate ROC AUC and PR AUC\n",
    "        train_roc_auc = roc_auc_score(train_labels, train_prob)\n",
    "        train_pr_auc = average_precision_score(train_labels, train_prob)\n",
    "        val_roc_auc = roc_auc_score(val_labels, val_prob)\n",
    "        val_pr_auc = average_precision_score(val_labels, val_prob)\n",
    "\n",
    "        fold_results.append({\n",
    "            'fold': fold,\n",
    "            'train_accuracy': train_accuracy,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'train_roc_auc': train_roc_auc,\n",
    "            'val_roc_auc': val_roc_auc,\n",
    "            'train_pr_auc': train_pr_auc,\n",
    "            'val_pr_auc': val_pr_auc,\n",
    "        })\n",
    "\n",
    "        print(f\"Fold {fold} results:\")\n",
    "        print(f\"Train accuracy: {train_accuracy:.3f}, ROC AUC: {train_roc_auc:.2f}, PR AUC: {train_pr_auc:.2f}\")\n",
    "        print(f\"Validation accuracy: {val_accuracy:.3f}, ROC AUC: {val_roc_auc:.2f}, PR AUC: {val_pr_auc:.2f}\")\n",
    "\n",
    "    # Calculate average scores across folds\n",
    "    avg_scores = {\n",
    "        'train_accuracy': np.mean([r['train_accuracy'] for r in fold_results]),\n",
    "        'val_accuracy': np.mean([r['val_accuracy'] for r in fold_results]),\n",
    "        'train_roc_auc': np.mean([r['train_roc_auc'] for r in fold_results]),\n",
    "        'val_roc_auc': np.mean([r['val_roc_auc'] for r in fold_results]),\n",
    "        'train_pr_auc': np.mean([r['train_pr_auc'] for r in fold_results]),\n",
    "        'val_pr_auc': np.mean([r['val_pr_auc'] for r in fold_results]),\n",
    "    }\n",
    "\n",
    "    print(\"\\nAverage scores across 5 folds:\")\n",
    "    for metric, value in avg_scores.items():\n",
    "        print(f\"{metric}: {value:.3f}\")\n",
    "\n",
    "    results[tail] = {\n",
    "        'fold_results': fold_results,\n",
    "        'average_scores': avg_scores\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    np.save(f'data/results_{tail}.npy', results[tail])\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "# Save overall results\n",
    "np.save('data/comparison_time_bins_results_cross_validated.npy', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and predict    \n",
    "loaded_model = XGBClassifier()\n",
    "loaded_model.load_model('data/models/simple_xgboost_model.model')\n",
    "test_predictions = loaded_model.predict(np.array([x[1:-1] for x in test_flat]))\n",
    "validation_predictions = loaded_model.predict(np.array([x[1:-1] for x in validation_flat]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import collections\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "results = {}\n",
    "\n",
    "data_paths = [\n",
    "    # \"data/preprocessed/preprocessed_data_4H.csv\",\n",
    "    \"data/preprocessed/preprocessed_data_6H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_8H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_4H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_6H.csv\",\n",
    "    # \"data/preprocessed/preprocessed_data_8H.csv\",\n",
    "]\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    times = math.floor(data.shape[0]/batch_size)\n",
    "    remainder = data.shape[0]%times\n",
    "    a = 0\n",
    "    start = 0\n",
    "    end = start+batch_size\n",
    "    if remainder ==0:\n",
    "        a +=1\n",
    "    while a<times:\n",
    "        temp = pad(data[start:end,],0)\n",
    "        x = torch.from_numpy(temp[:,:,1:-1]).float() # without icustay_id and without aki_stage columns\n",
    "        y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "        X_batches.append(x)\n",
    "        y_batches.append(y)\n",
    "        start = end\n",
    "        end = start+batch_size\n",
    "        a +=1\n",
    "    temp = pad(data[start:data.shape[0]],0)\n",
    "    x = torch.from_numpy(temp[:,:,1:-1]).float()\n",
    "    y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "    X_batches.append(x)\n",
    "    y_batches.append(y)\n",
    "    if len(X_batches) != len(y_batches):\n",
    "        print(\"length error\")\n",
    "    return X_batches, y_batches # arrays\n",
    "\n",
    "for data_path in data_paths:\n",
    "    tail = data_path.split(\"/\")[-1]\n",
    "    print(f\"Processing {tail}\")\n",
    "\n",
    "    # ... (keep the data loading and preprocessing steps as they are)\n",
    "    X = pd.read_csv(data_path)\n",
    "    # only take the first 10000 rows\n",
    "    X = X.head(10000) \n",
    "\n",
    "    # Preprocessing steps (similar to XGBoost)\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage')\n",
    "    numeric_feat.remove('icustay_id')\n",
    "\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Split data (you may want to use the same splitting logic as in XGBoost)\n",
    "    id_list = X['icustay_id'].unique()\n",
    "    # id_list = common_id_list\n",
    "    id_train, id_test_val = train_test_split(id_list, test_size=SPLIT_SIZE, random_state=42)\n",
    "    id_valid, id_test = train_test_split(id_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "    X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "    test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index=True)\n",
    "    validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id'])\n",
    "    \n",
    "    test = test.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "    train = train.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "    validation = validation.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "\n",
    "    train.drop(['charttime'], axis=1, inplace=True)\n",
    "    test.drop(['charttime'], axis=1, inplace=True)\n",
    "    validation.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "    # train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    # test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    # validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "    \n",
    "    train = np.array(sorted(list(train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)),key=len, reverse=True))\n",
    "    print(\"train shape\", train.shape)\n",
    "    test = np.array(list(test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)))\n",
    "    validation = np.array(list(validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)))\n",
    "    \n",
    "    # Prepare data for LSTM\n",
    "    # X_train, y_train = batch(train.to_numpy(), batch_size)\n",
    "    # X_test, y_test = batch(test.to_numpy(), test.shape[0])\n",
    "    # X_val, y_val = batch(validation.to_numpy(), validation.shape[0])\n",
    "    X_train, y_train = batch(train, batch_size)\n",
    "    X_test, y_test = batch(test, batch_size)\n",
    "    X_val, y_val = batch(validation, batch_size)\n",
    "    \n",
    "    print(train.shape[0])\n",
    "\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(f'runs/{tail}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "\n",
    "    # LSTM parameters\n",
    "    input_size = X_train[0].shape[2]\n",
    "    output_size = 1\n",
    "    emb_size = round(input_size / 1)\n",
    "    number_layers = 3\n",
    "    dropout = 0\n",
    "    bi_directional = True\n",
    "\n",
    "    nn_model = Net(input_size, emb_size, output_size, bi_directional, number_layers, dropout).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "    \n",
    "    use_pretrained = False\n",
    "    \n",
    "    best_auc = 0\n",
    "    start_epoch = 0\n",
    "    \n",
    "    if use_pretrained:\n",
    "        model_path = f'data/models/{tail}/LSTM_best.pth'\n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path)\n",
    "            nn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            best_auc = checkpoint.get('best_auc', 0)  # Load best AUC or default to 0 if not found\n",
    "            start_epoch = checkpoint.get('epoch', 0)  # Load last epoch or default to 0 if not found\n",
    "            print(f\"Loaded pretrained model from {model_path} with AUC {best_auc} at epoch {start_epoch}.\")\n",
    "        else:\n",
    "            print(f\"No pretrained model found, starting training from scratch.\")\n",
    "\n",
    "    n_epochs = 200\n",
    "    best_auc = 0\n",
    "\n",
    "    for epoch in range(start_epoch + 1, n_epochs):\n",
    "        nn_model.train()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "\n",
    "        for i, (X_batch, y_batch) in enumerate(zip(X_train, y_train)):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            print(\"Shape X batch, y batch\", X_batch.shape, y_batch.shape)\n",
    "            print(X_batch[1][1][0].shape)\n",
    "            print(X_batch[1][1][0])\n",
    "            outputs = nn_model(X_batch)\n",
    "            print(\"Shape outputs\", outputs.shape)\n",
    "            # print(outputs)\n",
    "            outputs = torch.flatten(outputs)\n",
    "            y_batch = y_batch.type_as(outputs)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            train_accuracy = accuracy_score(y_batch.cpu().numpy(), predicted.cpu().numpy())\n",
    "            running_accuracy += train_accuracy\n",
    "\n",
    "            # Log training metrics to TensorBoard\n",
    "            writer.add_scalar('Training/Loss', loss.item(), epoch * len(X_train) + i)\n",
    "            writer.add_scalar('Training/Accuracy', train_accuracy, epoch * len(X_train) + i)\n",
    "\n",
    "        # Validation\n",
    "        nn_model.eval()\n",
    "        total_v_loss = 0\n",
    "        all_y_val = []\n",
    "        all_val_prob = []\n",
    "        all_accuracy = 0\n",
    "\n",
    "        for X_val_batch, y_val_batch in zip(X_val, y_val):\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                v_out = nn_model(X_val_batch)\n",
    "                v_out = torch.flatten(v_out)\n",
    "                y_val_batch = y_val_batch.type_as(v_out)\n",
    "                v_loss = criterion(v_out, y_val_batch)\n",
    "                val_prob = torch.sigmoid(v_out)\n",
    "                total_v_loss += v_loss.item()\n",
    "                all_y_val.extend(y_val_batch.cpu().numpy())\n",
    "                all_val_prob.extend(val_prob.cpu().numpy())\n",
    "                \n",
    "                predicted = torch.sigmoid(v_out) > 0.5\n",
    "                val_accuracy = accuracy_score(y_val_batch.cpu().numpy(), predicted.cpu().numpy())\n",
    "                all_accuracy += val_accuracy\n",
    "\n",
    "        avg_v_loss = total_v_loss / len(X_val)\n",
    "        roc_auc = roc_auc_score(all_y_val, all_val_prob)\n",
    "        avg_accuracy = all_accuracy / len(X_val)\n",
    "        \n",
    "        # Log validation metrics to TensorBoard\n",
    "        writer.add_scalar('Validation/Loss', avg_v_loss, epoch)\n",
    "        writer.add_scalar('Validation/AUC', roc_auc, epoch)\n",
    "        writer.add_scalar('Validation/Accuracy', avg_accuracy, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, \"\n",
    "            f\"Train Loss: {running_loss/len(X_train):.4f}, \"\n",
    "            f\"Train Accuracy: {running_accuracy/len(X_train):.4f}, \"\n",
    "            f\"Val Loss: {avg_v_loss:.4f}, \"\n",
    "            f\"Val AUC: {roc_auc:.4f},\"\n",
    "            f\"Val Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "        if roc_auc > best_auc:\n",
    "            best_auc = roc_auc\n",
    "            save_path = f'data/models/{tail}/LSTM_best.pth'\n",
    "            torch.save({\n",
    "                'model_state_dict': nn_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_auc': best_auc,\n",
    "                'epoch': epoch\n",
    "            }, save_path)\n",
    "\n",
    "    # Close the TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm adjusted\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "results = {}\n",
    "\n",
    "data_paths = [\n",
    "    \"data/preprocessed/preprocessed_data_6H.csv\",\n",
    "]\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')\n",
    "\n",
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch_data = data[i:i+batch_size]\n",
    "        \n",
    "        # Pad sequences to the same length within the batch\n",
    "        max_seq_length = max(len(seq) for seq in batch_data)\n",
    "        \n",
    "        X_batch = []\n",
    "        y_batch = []\n",
    "        \n",
    "        for seq in batch_data:\n",
    "            padded_seq = np.pad(seq, ((0, max_seq_length - len(seq)), (0, 0)), mode='constant')\n",
    "            X_batch.append(padded_seq[:, 1:-1])  # Exclude icustay_id and aki_stage\n",
    "            y_batch.append(padded_seq[0, -1])  # Take the aki_stage of the first row\n",
    "        \n",
    "        X_batches.append(torch.FloatTensor(X_batch))\n",
    "        y_batches.append(torch.LongTensor(y_batch))\n",
    "    \n",
    "    return X_batches, y_batches\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, emb_size, num_layers=number_layers, \n",
    "                            bidirectional=bi_directional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(emb_size * (2 if bi_directional else 1), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        \n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "        \n",
    "        out = self.fc(hidden)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "for data_path in data_paths:\n",
    "    tail = data_path.split(\"/\")[-1]\n",
    "    print(f\"Processing {tail}\")\n",
    "\n",
    "    X = pd.read_csv(data_path)\n",
    "    # X = X.head(10000)  # only take the first 10000 rows\n",
    "\n",
    "    numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_feat.remove('aki_stage')\n",
    "    numeric_feat.remove('icustay_id')\n",
    "\n",
    "    X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "    X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "    try:\n",
    "        X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    id_list = X['icustay_id'].unique()\n",
    "    id_train, id_test_val = train_test_split(id_list, test_size=SPLIT_SIZE, random_state=42)\n",
    "    id_valid, id_test = train_test_split(id_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "    X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "    train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id', 'charttime'])\n",
    "    test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id', 'charttime'])\n",
    "    validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "    train.drop(['charttime'], axis=1, inplace=True)\n",
    "    test.drop(['charttime'], axis=1, inplace=True)\n",
    "    validation.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "    train = list(train.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
    "    train.sort(key=len, reverse=True)  # Sort by sequence length in descending order\n",
    "    print(\"Number of sequences in train:\", len(train))\n",
    "    test = list(test.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
    "    validation = list(validation.groupby(['icustay_id']).apply(pd.DataFrame.to_numpy))\n",
    "\n",
    "    batch_size = 32  # You may need to adjust this\n",
    "    X_train, y_train = batch(train, batch_size)\n",
    "    X_test, y_test = batch(test, batch_size)\n",
    "    X_val, y_val = batch(validation, batch_size)\n",
    "\n",
    "    print(f\"Number of batches in train: {len(X_train)}\")\n",
    "\n",
    "    writer = SummaryWriter(f'runs/{tail}_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "\n",
    "    input_size = X_train[0].shape[2]\n",
    "    output_size = 1\n",
    "    emb_size = round(input_size / 1)\n",
    "    number_layers = 3\n",
    "    dropout = 0\n",
    "    bi_directional = True\n",
    "\n",
    "    nn_model = Net(input_size, emb_size, output_size, bi_directional, number_layers, dropout).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "    use_pretrained = False\n",
    "    best_auc = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    if use_pretrained:\n",
    "        model_path = f'data/models/{tail}/LSTM_best.pth'\n",
    "        if os.path.exists(model_path):\n",
    "            checkpoint = torch.load(model_path)\n",
    "            nn_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            best_auc = checkpoint.get('best_auc', 0)\n",
    "            start_epoch = checkpoint.get('epoch', 0)\n",
    "            print(f\"Loaded pretrained model from {model_path} with AUC {best_auc} at epoch {start_epoch}.\")\n",
    "        else:\n",
    "            print(f\"No pretrained model found, starting training from scratch.\")\n",
    "\n",
    "    n_epochs = 200\n",
    "\n",
    "    for epoch in range(start_epoch + 1, n_epochs):\n",
    "        nn_model.train()\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "\n",
    "        for i, (X_batch, y_batch) in enumerate(zip(X_train, y_train)):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = nn_model(X_batch)\n",
    "            loss = criterion(outputs, y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            predicted = torch.sigmoid(outputs) > 0.5\n",
    "            train_accuracy = accuracy_score(y_batch.cpu().numpy(), predicted.cpu().numpy())\n",
    "            running_accuracy += train_accuracy\n",
    "\n",
    "            writer.add_scalar('Training/Loss', loss.item(), epoch * len(X_train) + i)\n",
    "            writer.add_scalar('Training/Accuracy', train_accuracy, epoch * len(X_train) + i)\n",
    "\n",
    "        nn_model.eval()\n",
    "        total_v_loss = 0\n",
    "        all_y_val = []\n",
    "        all_val_prob = []\n",
    "        all_accuracy = 0\n",
    "\n",
    "        for X_val_batch, y_val_batch in zip(X_val, y_val):\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                v_out = nn_model(X_val_batch)\n",
    "                v_loss = criterion(v_out, y_val_batch.float())\n",
    "                val_prob = torch.sigmoid(v_out)\n",
    "                total_v_loss += v_loss.item()\n",
    "                all_y_val.extend(y_val_batch.cpu().numpy())\n",
    "                all_val_prob.extend(val_prob.cpu().numpy())\n",
    "                \n",
    "                predicted = val_prob > 0.5\n",
    "                val_accuracy = accuracy_score(y_val_batch.cpu().numpy(), predicted.cpu().numpy())\n",
    "                all_accuracy += val_accuracy\n",
    "\n",
    "        avg_v_loss = total_v_loss / len(X_val)\n",
    "        roc_auc = roc_auc_score(all_y_val, all_val_prob)\n",
    "        avg_accuracy = all_accuracy / len(X_val)\n",
    "        \n",
    "        writer.add_scalar('Validation/Loss', avg_v_loss, epoch)\n",
    "        writer.add_scalar('Validation/AUC', roc_auc, epoch)\n",
    "        writer.add_scalar('Validation/Accuracy', avg_accuracy, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, \"\n",
    "              f\"Train Loss: {running_loss/len(X_train):.4f}, \"\n",
    "              f\"Train Accuracy: {running_accuracy/len(X_train):.4f}, \"\n",
    "              f\"Val Loss: {avg_v_loss:.4f}, \"\n",
    "              f\"Val AUC: {roc_auc:.4f},\"\n",
    "              f\"Val Accuracy: {avg_accuracy:.4f}\")\n",
    "\n",
    "        if roc_auc > best_auc:\n",
    "            best_auc = roc_auc\n",
    "            save_path = f'data/models/{tail}/LSTM_best.pth'\n",
    "            torch.save({\n",
    "                'model_state_dict': nn_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_auc': best_auc,\n",
    "                'epoch': epoch\n",
    "            }, save_path)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate pretrained lstm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import collections\n",
    "from datetime import datetime\n",
    "import os\n",
    "import gc\n",
    "\n",
    "data_path = \"data/preprocessed/preprocessed_data_6H.csv\"\n",
    "tail = data_path.split(\"/\")[-1]\n",
    "print(f\"Processing {tail}\")\n",
    "\n",
    "X = pd.read_csv(data_path)\n",
    "\n",
    "# Preprocessing steps (similar to XGBoost)\n",
    "numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_feat.remove('aki_stage')\n",
    "numeric_feat.remove('icustay_id')\n",
    "\n",
    "X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "try:\n",
    "    X.drop(['hadm_id'], axis=1, inplace=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Split data (you may want to use the same splitting logic as in XGBoost)\n",
    "id_list = X['icustay_id'].unique()\n",
    "# id_list = common_id_list\n",
    "id_train, id_test_val = train_test_split(id_list, test_size=SPLIT_SIZE, random_state=42)\n",
    "id_valid, id_test = train_test_split(id_test_val, test_size=0.5, random_state=42)\n",
    "\n",
    "X = X.reindex(columns=[col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index=True)\n",
    "validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id'])\n",
    "\n",
    "train.drop(['charttime'], axis=1, inplace=True)\n",
    "test.drop(['charttime'], axis=1, inplace=True)\n",
    "validation.drop(['charttime'], axis=1, inplace=True)\n",
    "\n",
    "train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "# Prepare data for LSTM\n",
    "# X_train, y_train = batch(train.to_numpy(), batch_size)\n",
    "# X_test, y_test = batch(test.to_numpy(), test.shape[0])\n",
    "# X_val, y_val = batch(validation.to_numpy(), validation.shape[0])\n",
    "X_train, y_train = batch(train, batch_size)\n",
    "X_test, y_test = batch(test, batch_size)\n",
    "X_val, y_val = batch(validation, batch_size)\n",
    "\n",
    "# LSTM parameters\n",
    "input_size = X_train[0].shape[2]  # Subtract 2 for icustay_id and aki_stage\n",
    "output_size = 1\n",
    "emb_size = round(input_size / 1)\n",
    "number_layers = 3\n",
    "dropout = 0\n",
    "bi_directional = True\n",
    "\n",
    "\n",
    "# Assuming Net is defined elsewhere\n",
    "# Assuming X_train, y_train, X_val, y_val, X_test, y_test are defined and split into batches if necessary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "nn_model = Net(input_size, emb_size, output_size, bi_directional, number_layers, dropout).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Test evaluation with F1 score\n",
    "nn_model.load_state_dict(torch.load(f'data/models/{tail}/LSTM_best.pth')['model_state_dict'])\n",
    "nn_model.eval()\n",
    "total_test_loss = 0\n",
    "all_y_test = []\n",
    "all_test_prob = []\n",
    "all_test_f1 = 0\n",
    "\n",
    "for X_test_batch, y_test_batch in zip(X_test, y_test):\n",
    "    X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        t_out = nn_model(X_test_batch)\n",
    "        t_out = torch.flatten(t_out)\n",
    "        y_test_batch = y_test_batch.type_as(t_out)\n",
    "        test_loss = criterion(t_out, y_test_batch)\n",
    "        test_prob = torch.sigmoid(t_out)\n",
    "        total_test_loss += test_loss.item()\n",
    "        all_y_test.extend(y_test_batch.cpu().numpy())\n",
    "        all_test_prob.extend(test_prob.cpu().numpy())\n",
    "        \n",
    "        predicted = torch.sigmoid(t_out) > 0.08\n",
    "        test_f1 = f1_score(y_test_batch.cpu().numpy(), predicted.cpu().numpy(), zero_division=1)\n",
    "        all_test_f1 += test_f1\n",
    "        \n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(all_y_test, all_test_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "os.makedirs('data/plots', exist_ok=True)\n",
    "plt.savefig(f'data/plots/ROC_{tail}.png')  # Save ROC curve\n",
    "plt.close()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(all_y_test, all_test_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='PR curve (area = %0.2f)' % pr_auc)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "os.makedirs('data/plots', exist_ok=True)\n",
    "plt.savefig(f'data/plots/PR_{tail}.png')  # Save PR curve\n",
    "plt.close()\n",
    "\n",
    "print(f\"Test Loss: {total_test_loss / len(X_test):.4f}, \"\n",
    "    f\"Test AUC: {roc_auc:.4f}, \"\n",
    "    f\"Test F1: {all_test_f1 / len(X_test):.4f}, \"\n",
    "    f\"Test PR AUC: {pr_auc:.4f}\")\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.load(f'data/models/{tail}/LSTM_best.pth')['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import numpy as np\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(all_y_test, all_test_prob)\n",
    "# Find the optimal threshold\n",
    "optimal_idx = np.argmin(np.sqrt(np.square(1-tpr) + np.square(fpr)))\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(all_y_test, all_test_prob)\n",
    "# Add a last threshold corresponding to recall = 0.\n",
    "thresholds = np.append(thresholds, 1)\n",
    "\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "# Find the optimal threshold\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all_y_test and all_test_prob to numpy arrays for easier manipulation\n",
    "all_y_test = np.array(all_y_test)\n",
    "all_test_prob = np.array(all_test_prob)\n",
    "\n",
    "# Initialize variables to store the best threshold and its corresponding F1 score\n",
    "best_threshold = 0.0\n",
    "best_f1 = 0.0\n",
    "\n",
    "# Iterate over a range of possible threshold values (e.g., 0 to 1, step 0.01)\n",
    "for threshold in np.arange(0.0, 1.01, 0.01):\n",
    "    # Convert probabilities to binary predictions based on the current threshold\n",
    "    predictions = (all_test_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate F1 score for the current threshold\n",
    "    f1 = f1_score(all_y_test, predictions, zero_division=1)\n",
    "    \n",
    "    # Update best threshold and F1 score if the current F1 score is better\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "# Print the best threshold and its corresponding F1 score\n",
    "print(f\"Best Threshold: {best_threshold}\")\n",
    "print(f\"Best F1 Score: {best_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Training on CPU') # On mac book GPU is not possible =() \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(np.array(train)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    times = math.floor(data.shape[0]/batch_size)\n",
    "    remainder = data.shape[0]%times\n",
    "    a = 0\n",
    "    start = 0\n",
    "    end = start+batch_size\n",
    "    if remainder ==0:\n",
    "        a +=1\n",
    "    while a<times:\n",
    "        temp = pad(data[start:end,],0)\n",
    "        x = torch.from_numpy(temp[:,:,1:-1]).float() # without icustay_id and without aki_stage columns\n",
    "        y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "        X_batches.append(x)\n",
    "        y_batches.append(y)\n",
    "        start = end\n",
    "        end = start+batch_size\n",
    "        a +=1\n",
    "    temp = pad(data[start:data.shape[0]],0)\n",
    "    x = torch.from_numpy(temp[:,:,1:-1]).float()\n",
    "    y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "    X_batches.append(x)\n",
    "    y_batches.append(y)\n",
    "    if len(X_batches) != len(y_batches):\n",
    "        print(\"length error\")\n",
    "    return X_batches, y_batches # arrays\n",
    "\n",
    "# batching\n",
    "X_train, y_train = batch(train, batch_size) # to count weights\n",
    "\n",
    "# counting balance of the classes\n",
    "y = []\n",
    "for i in y_train:\n",
    "    for element in i:\n",
    "        y.append(element.item())\n",
    "\n",
    "#  weights\n",
    "counter=collections.Counter(y)\n",
    "print(counter)\n",
    "zeroes = counter[0]\n",
    "ones = counter[1]\n",
    "\n",
    "X_test, y_test = batch(test, test.shape[0]) \n",
    "X_val, y_val = batch(validation, validation.shape[0])\n",
    "X_val = X_val[0]\n",
    "y_val = y_val[0]\n",
    "X_test = X_test[0]\n",
    "y_test = y_test[0]\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# setup\n",
    "\n",
    "bi_directional = True\n",
    "n_epochs = 50\n",
    "lr = 0.001\n",
    "features = len(X_train[0][0][0])\n",
    "print(features)\n",
    "# features = \n",
    "emb_size = round(features/1)\n",
    "number_layers = 3\n",
    "dropout = 0 # dropout\n",
    "\n",
    "##########################\n",
    "input_size = features\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size \n",
    "        self.output_size = output_size\n",
    "        self.number_layers = number_layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "        # in bidirectional encoder we have  forward and backward hidden states\n",
    "        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "        # Create affine layer to project to the classes \n",
    "        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "        #dropout layer for regularizetion of a sequence\n",
    "        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "        h = self.relu(self.combination_layer(h))\n",
    "        h = self.dropout_layer(h)\n",
    "        h = self.projection(h) \n",
    "        return h\n",
    "\n",
    "#create a network \n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "#print(nn_model)\n",
    "#print(list(nn_model.parameters()))\n",
    "\n",
    "\n",
    "# BCE Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss() # class imbalance\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(round(zeroes/ones,0))) # class imbalance\n",
    "#print(round(zeroes/ones,0))\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=lr) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count unique values\n",
    "unique, counts = np.unique(y_val, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "# Count NaN values\n",
    "nan_count = np.isnan(y_val).sum()\n",
    "print(f\"Number of NaN values: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all nans in X_val with 0\n",
    "X_val[torch.isnan(X_val)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_original = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_original = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training loop (full data 3.5 hours)\n",
    "\n",
    "epochs = n_epochs\n",
    "starttime = datetime.now() # datetime object containing current date and time\n",
    "train_losses, validation_losses = [], []\n",
    "best = 0\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    print (\"\\n Epoch [%d] out of %d\" % (epoch + 1, epochs))\n",
    "    running_loss = 0.0\n",
    "    validation_loss = 0.0\n",
    "    roc_auc = 0.0\n",
    "    pr_auc = 0.0\n",
    "    m = 0\n",
    "    \n",
    "    #train\n",
    "    #print(list(nn_model.parameters())[0])\n",
    "    # pbar = tqdm(X_train, desc=f\"Epoch {epoch+1}\")\n",
    "    # for i in pbar:\n",
    "    #     # zero the parameter gradients\n",
    "    #     optimizer.zero_grad() # zero the gradient buffers not to consider gradients of previous iterations\n",
    "    #     X_batch = X_train[m]\n",
    "    #     y_batch = y_train[m]\n",
    "    #     # print(X_batch.shape)\n",
    "    #     # forward + backward + optimize\n",
    "    #     outputs = nn_model(X_batch)\n",
    "    #     outputs = torch.flatten(outputs)\n",
    "    #     y_batch = y_batch.type_as(outputs)\n",
    "    #     loss = criterion(outputs, y_batch)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step() # Does the update\n",
    "    #     running_loss += loss.item()\n",
    "    #     m +=1\n",
    "    #     pbar.set_postfix({\"Training Loss\": running_loss/len(X_train)})\n",
    "        \n",
    "   \n",
    "    #validation \n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        v_out = nn_model(X_val) \n",
    "        v_out = torch.flatten(v_out) \n",
    "        y_val = y_val.type_as(v_out)\n",
    "        v_loss = criterion(v_out, y_val)\n",
    "        validation_loss = v_loss.item()\n",
    "        # auc and pr auc\n",
    "        val_prob = torch.nn.Sigmoid() (v_out)\n",
    "        print(type(v_out))\n",
    "        print(v_out)\n",
    "        print(val_prob)\n",
    "        print(y_val)\n",
    "        roc_auc = roc_auc_score(y_val,val_prob) \n",
    "        \n",
    "    validation_losses.append(validation_loss) \n",
    "    train_losses.append(running_loss/len(X_train)) \n",
    "    print(f\"Training loss: {running_loss/len(X_train):.3f}.. \" f\"Validation loss: {validation_loss:.3f}.. \")\n",
    "    print(f\"AUC: {roc_auc:.2f}\")  \n",
    "    nn_model.train()\n",
    "    \n",
    "    \n",
    "    if roc_auc > best:\n",
    "        best = roc_auc\n",
    "        PATH = './LSTMbest.pth' \n",
    "        torch.save(nn_model.state_dict(), PATH) # save the model\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "       \n",
    "print('Finished Training')\n",
    "print(\"starttime =\", starttime)\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "PATH = './LSTM.pth' \n",
    "torch.save(nn_model.state_dict(), PATH) # save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "PATH = './LSTM.pth'\n",
    "nn_model.load_state_dict(torch.load(PATH))\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    t_out = nn_model(X_test)\n",
    "    t_out = torch.flatten(t_out)\n",
    "    y_test = y_test.type_as(t_out)\n",
    "    t_loss = criterion(t_out, y_test)\n",
    "    test_loss = t_loss.item()\n",
    "    # auc and pr auc\n",
    "    test_prob = torch.nn.Sigmoid() (t_out)\n",
    "    roc_auc = roc_auc_score(y_test,test_prob) \n",
    "    pr_auc = average_precision_score(y_test,test_prob)\n",
    "    # convert output probabilities to class labels\n",
    "    test_pred = (test_prob > 0.5).float()\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = accuracy_score(y_test.cpu().numpy(), test_pred.cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}.. \" f\"ROC AUC: {roc_auc:.2f}.. \" f\"PR AUC: {pr_auc:.2f}.. \")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a freshly initialized model on test\n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "# nn_model.load_state_dict(torch.load(PATH))\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    t_out = nn_model(X_test)\n",
    "    t_out = torch.flatten(t_out)\n",
    "    y_test = y_test.type_as(t_out)\n",
    "    t_loss = criterion(t_out, y_test)\n",
    "    test_loss = t_loss.item()\n",
    "    # auc and pr auc\n",
    "    test_prob = torch.nn.Sigmoid() (t_out)\n",
    "    roc_auc = roc_auc_score(y_test,test_prob) \n",
    "    pr_auc = average_precision_score(y_test,test_prob)\n",
    "    print(f\"Test loss: {test_loss:.3f}.. \" f\"ROC AUC: {roc_auc:.2f}.. \" f\"PR AUC: {pr_auc:.2f}.. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './i-Bidir_3_lr_0.001_nodropbest.pth'\n",
    "\n",
    "# save the model\n",
    "#torch.save(nn_model.state_dict(), PATH)\n",
    "\n",
    "# code to load saved model\n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "nn_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test) # single batch with zero padding to the max shape 635208"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = nn_model(X_test)\n",
    "pred = torch.nn.Sigmoid() (logits)\n",
    "pred = pred.detach().numpy()\n",
    "pred = pred.reshape(-1,1)\n",
    "print(\"Performance on full X_test where it has no batching: is padded to max dimentions. \\n\")\n",
    "print (\"Area Under ROC Curve: %0.2f\" % roc_auc_score(y_test, pred, average = 'micro')  )\n",
    "brier = round(metrics.brier_score_loss(y_test, pred, sample_weight=None, pos_label=None),3)\n",
    "print(\"Brier score : {:.3f}\".format(brier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('padded_lstm.npy', 'wb') as f:\n",
    "    np.save(f, y_test)\n",
    "    np.save(f, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = X_test.shape[1] #133\n",
    "icustays = X_test.shape[0]\n",
    "times = []\n",
    "auc_s = []\n",
    "t = 0\n",
    "\n",
    "while t < timestamps:\n",
    "    times.append(t+1)\n",
    "    row = t\n",
    "    i = 0\n",
    "    prob_t = []\n",
    "    y_t = []\n",
    "    while i < icustays:\n",
    "        prob_t.append(pred[row])\n",
    "        y_t.append(y_test[row])\n",
    "        row += timestamps\n",
    "        i +=1\n",
    "    prob_t = np.array(prob_t).reshape(-1,1)\n",
    "    y_t = np.array(y_t).reshape(-1,1)\n",
    "    auc_s.append(roc_auc_score(y_t, prob_t, average = 'micro'))\n",
    "    t +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.DataFrame(auc_s, columns = ['AUC'])\n",
    "df['Timestamps'] = times\n",
    "#df[120:133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "sns.lineplot(x=\"Timestamps\", y=\"AUC\", color = 'g',\n",
    "             data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing to LogR, XGB, RF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = batch(test, test.shape[0]) \n",
    "X_test = X_test[0]\n",
    "y_test = y_test[0]\n",
    "\n",
    "\n",
    "def to_one_label (model, label_list,X_test,index_list):\n",
    "    # evaluate on a test set\n",
    "    labels = np.array(label_list)\n",
    "    labels = labels.reshape(-1,1)\n",
    "    labels = labels.astype(int)\n",
    "    logits = model(X_test)\n",
    "    pred = torch.nn.Sigmoid() (logits)\n",
    "    max_rows = pred.shape[1]\n",
    "    predictions = pred.detach().numpy()\n",
    "    predictions = predictions.reshape(-1,1) \n",
    "    # select 1 per icu stay id by index\n",
    "    prob_1_label = []\n",
    "    row = 0\n",
    "    prev = 0\n",
    "    for i in index_list:\n",
    "        prob_1_label.append(predictions[row+i-prev])\n",
    "        row += pred.shape[1]\n",
    "        prev = i\n",
    "    prob_1_label = np.array(prob_1_label).reshape(-1,1)\n",
    "    \n",
    "    return labels, prob_1_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance (y_test, pred_probabilities):\n",
    "    # performance\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    # compute roc auc\n",
    "    roc_auc = roc_auc_score(y_test, pred_probabilities, average = 'micro')\n",
    "    # compute Precision_Recall curves\n",
    "    precision, recall, _ = precision_recall_curve(y_test, pred_probabilities)\n",
    "    # compute PR_AUC\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "       \n",
    "    # I add confusion matrix\n",
    "    optimal_cut_off = round(thresholds[np.argmax(tpr - fpr)],4)\n",
    "    a = np.where(pred_probabilities > optimal_cut_off, 1, 0)\n",
    "    brier = round(metrics.brier_score_loss(y_test, pred_probabilities, sample_weight=None, pos_label=None),3)\n",
    "    predictions = np.where(pred_probabilities > optimal_cut_off, 1, 0)  \n",
    "    \n",
    "    print (\"Area Under ROC Curve: %0.2f\" % roc_auc  )\n",
    "    #print (\"Area Under PR Curve(AP): %0.2f\" % pr_auc  ) \n",
    "    print(\"Brier score : {:.3f}\".format(brier))\n",
    "    #print('Accuracy for Classifier : {:.2f}'.format(accuracy_score(y_test, predictions)))\n",
    "    #print('Cut off: ' + str(optimal_cut_off))\n",
    "    matrix = metrics.confusion_matrix(y_test, a, labels=None, normalize=None)\n",
    "    #print(str(matrix))\n",
    "    \n",
    "    #f.write(\"\\n Area Under ROC Curve: \" +str(roc_auc))\n",
    "    #f.write(\"\\n Area Under PR Curve(AP): \" + str(pr_auc))\n",
    "    #f.write(\"\\n Brier score: \" +str(brier))\n",
    "    #f.write('\\n Accuracy for Classifier '+str(round((accuracy_score(labels, predictions)),3)))\n",
    "    #f.write(\"\\n Cut off: \" +str(optimal_cut_off))\n",
    "    #f.write(str(matrix))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels, prob_1_label = to_one_label (nn_model, label_list,X_test,index_list)\n",
    "performance(labels,prob_1_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save labels, prob_1_label\n",
    "\n",
    "with open('test.npy', 'wb') as f:\n",
    "    #np.save(f, labels)\n",
    "    np.save(f, prob_1_label)\n",
    "with open('test.npy', 'rb') as f:\n",
    "    #lstm_labels = np.load(f)\n",
    "    lstm_prob = np.load(f)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply integrated gradients, we first create an IntegratedGradients object, providing the model object.\n",
    "ig = IntegratedGradients(nn_model)\n",
    "# To compute the integrated gradients, we use the attribute method of the IntegratedGradients object. The method takes\n",
    "# tensor(s) of input examples (matching the forward function of the model), and returns the input attributions for the\n",
    "# given examples. A target index, defining the index of the output for which gradients are computed is 1, \n",
    "# corresponding to AKI (1/0).\n",
    "\n",
    "#The input tensor provided should require grad, so we call requires_grad_ on the tensor. The attribute method also \n",
    "# takes a baseline, which is the starting point from which gradients are integrated. The default value is just the \n",
    "# 0 tensor, which is a reasonable baseline / default for this task.\n",
    "\n",
    "#The returned values of the attribute method are the attributions, which match the size of the given inputs, and delta,\n",
    "# which approximates the error between the approximated integral and true integral.\n",
    "print(datetime.now())\n",
    "X_test.requires_grad_()\n",
    "attr, delta = ig.attribute(X_test,target=1, return_convergence_delta=True)\n",
    "attr = attr.detach().numpy()\n",
    "attr= np.reshape(attr,(-1,35))\n",
    "importances = np.mean(attr, axis=0)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[:,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[:,4].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_importances(feature_names, importances, title=\"LSTM Average Feature Importances\", axis_title=\"Features\"):\n",
    "    print(title)\n",
    "    i = 0\n",
    "    while i < features:\n",
    "        print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "        i +=1\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    \n",
    "visualize_feature_importances(feature_names, importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df =  pd.DataFrame(importances, columns = ['Feature Importance'])\n",
    "lstm_df['Features'] = feature_names\n",
    "lstm_df = lstm_df.sort_values(by = ['Feature Importance'], ascending = False, ignore_index = True)\n",
    "#lstm_df[\"Feature Importance\"] =  lstm_df[\"Feature Importance\"]\n",
    "#lstm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df[\"Feature Importance\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df)\n",
    "ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df, color = 'grey')\n",
    "ax.set_xlabel('Feature Importance', fontsize = 15)\n",
    "ax.set_ylabel(\"Features\",fontsize=15)\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 6)\n",
    "plt.title('LSTM feature Importances')\n",
    "plt.savefig('LSTM_feature_importance_grey.png', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df['abs'] = abs(lstm_df['Feature Importance'])\n",
    "lstm_df = lstm_df.sort_values(by = ['abs'], ascending = False, ignore_index = True)\n",
    "lstm_df_10 = lstm_df.head(10)\n",
    "#lstm_df_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df_10, palette=\"mako\")\n",
    "\n",
    "ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df_10, color = 'darkgreen')\n",
    "ax.set_xlabel('Feature Importance', fontsize = 15)\n",
    "ax.set_ylabel(\"Features\",fontsize=15)\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 10)\n",
    "plt.title('LSTM top 10 features by feature importance')\n",
    "plt.savefig('LSTM_top10_feature_importance_darkgreen.png', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graphs (y_test,pred_probabilities, classifier_name, plot_name, algorithm):\n",
    "    \n",
    "    def bin_total(y_true, y_prob, n_bins):\n",
    "        bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "        # In sklearn.calibration.calibration_curve, the last value in the array is always 0.\n",
    "        binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "        return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "    def missing_bin(bin_array):\n",
    "        midpoint = \" \"    \n",
    "        if bin_array[0]==0:\n",
    "            midpoint = \"5%, \"\n",
    "        if bin_array[1]==0:\n",
    "            midpoint = midpoint + \"15%, \"\n",
    "        if bin_array[2]==0:\n",
    "            midpoint = midpoint + \"25%, \"\n",
    "        if bin_array[3]==0:\n",
    "            midpoint = midpoint + \"35%, \" \n",
    "        if bin_array[4]==0:\n",
    "            midpoint = midpoint + \"45%, \"\n",
    "        if bin_array[5]==0:\n",
    "            midpoint = midpoint + \"55%, \"\n",
    "        if bin_array[6]==0:\n",
    "            midpoint = midpoint + \"65%, \"\n",
    "        if bin_array[7]==0:\n",
    "            midpoint = midpoint + \"75%, \"\n",
    "        if bin_array[8]==0:\n",
    "            midpoint = midpoint + \"85%, \"\n",
    "        if bin_array[9]==0:\n",
    "            midpoint = midpoint + \"95%, \"\n",
    "        return \"The missing bins have midpoint values of \"+ str(midpoint)\n",
    "    \n",
    "    # performance\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    # compute roc auc\n",
    "    roc_auc = roc_auc_score(y_test, pred_probabilities, average = 'micro')\n",
    "    # compute Precision_Recall curves\n",
    "    precision, recall, _ = precision_recall_curve(y_test, pred_probabilities)\n",
    "    # compute PR_AUC\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "\n",
    "    # compute calibration curve\n",
    "    LR_y, LR_x = calibration_curve(y_test, pred_probabilities, n_bins=10)\n",
    "    #find out which one are the missing bins\n",
    "    bin_array = bin_total(y_test, pred_probabilities , n_bins=10)\n",
    "    print(missing_bin(bin_array))\n",
    "\n",
    "    print(\"plot curves and save in one png file\")\n",
    "    #save three plots in one png file\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, figsize=(7, 24))\n",
    "    fig.subplots_adjust(wspace=0.3, hspace= 0.3)\n",
    "    fig.suptitle('Evaluation of '+ plot_name)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    \n",
    "    # plot roc curve\n",
    "    ax1.plot(fpr, tpr,'C2', label=algorithm+\" \"+\"Classifier \" + str(classifier_name) + \", auc=\" +str(round(roc_auc,2)))\n",
    "    ax1.title.set_text('ROC AUC')\n",
    "    ax1.set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot PR curve\n",
    "    ax2.plot(recall, precision,'C2', label=algorithm+\" \"+\"Classifier \" + str(classifier_name) + \", auc=\"+str(round(pr_auc,2)))\n",
    "    ax2.title.set_text('PR AUC')\n",
    "    ax2.set(xlabel='Recall', ylabel='Precision')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot calibration curve\n",
    "    ax3.plot(LR_x, LR_y, 'C2',marker='o', linewidth=1, label='LR')\n",
    "    line = mlines.Line2D([0, 1], [0, 1], color='black')\n",
    "    transform = ax3.transAxes\n",
    "    line.set_transform(transform)\n",
    "    ax3.add_line(line)\n",
    "    ax3.title.set_text('Calibration plot for '+str(plot_name))\n",
    "    ax3.set(xlabel= 'Predicted probability', ylabel= 'True probability in each bin')\n",
    "    ax3.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.savefig(plot_name+\".png\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(pred_probabilities, y_test, dist_name):\n",
    "    #probabilities distributions graphs\n",
    "    true_1 = pd.DataFrame(pred_probabilities, columns=['Predicted probabilities'])\n",
    "    true_1['labels'] = y_test.tolist()\n",
    "    true_0 = true_1.copy(deep = True) \n",
    "    indexNames = true_1[true_1['labels'] == 0].index\n",
    "    true_1.drop(indexNames , inplace=True)\n",
    "    indexNames = true_0[ true_0['labels'] == 1 ].index\n",
    "    true_0.drop(indexNames , inplace=True)\n",
    "    true_1.drop(columns=['labels'], inplace = True)\n",
    "    true_0.drop(columns=['labels'], inplace = True)\n",
    "    \n",
    "    sns.distplot(true_1['Predicted probabilities'], hist = False, kde = True,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3,\"color\": \"g\"}, label = 'Class 1')\n",
    "    plt.ylabel('Density')\n",
    "    sns.distplot(true_0['Predicted probabilities'], hist = False, kde = True,\n",
    "                     kde_kws = {'shade': True, 'linewidth': 3}, label = 'Class 0')\n",
    "    plt.title('Density Plot'+ dist_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distribution(prob_1_label, labels.flatten(), \" Bidirectional LSTM no imputation \")\n",
    "plt.savefig('dist_LSTM_bi_NOimp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"None vs. Any AKI\"    ###change every time #Moderate vs. Severe #None vs. Any #Others vs. Severe\n",
    "plot_name = \"LSTM NO imputation\"\n",
    "build_graphs(labels.flatten(), prob_1_label.flatten(), classifier_name, plot_name, \"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(labels, prob_1_label)\n",
    "fpr, tpr, thresholds = roc_curve(labels, prob_1_label)\n",
    "optimal_cut_off = round(thresholds[np.argmax(tpr - fpr)],2)\n",
    "prediction = np.where(prob_1_label > optimal_cut_off, 1, 0)\n",
    "f1 = f1_score(labels,prediction)\n",
    "prauc =auc(recall, precision)\n",
    "print('F1 = %.3f, PR auc =%.3f' % (f1,prauc))\n",
    "\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(labels[labels==1]) / len(labels)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall,precision, marker='.', label='LSTM')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search grid \n",
    "layers = [1,2,3]\n",
    "l_rate = [0.001, 0.0001]\n",
    "drop = [0,0.2]\n",
    "bidirectionality = [True,False]\n",
    "#loops count\n",
    "hypercount = 0\n",
    "# static parameters\n",
    "n_epochs = 80\n",
    "emb_size = round(features/1)\n",
    "input_size = features\n",
    "output_size = 1\n",
    "###############################\n",
    "\n",
    "f = open('lstm_no_imp_uni.txt', 'w+') #change with or without imp\n",
    "\n",
    "for q1 in bidirectionality:\n",
    "    for q2 in layers:\n",
    "        for q3 in drop:\n",
    "            for q4 in l_rate:\n",
    "                hypercount +=1\n",
    "                name = \"i-Bidir_\" if q1 else \"i-Onedir_\"\n",
    "                name = name+str(q2) + \"_lr_\"+str(q4)\n",
    "                name = name+\"_drop\"+str(q3) if q3 == 0.2 else name+\"_nodrop\"\n",
    "                #set parameters\n",
    "                bi_directional = q1\n",
    "                lr = q4\n",
    "                number_layers = q2\n",
    "                dropout = q3 # dropout\n",
    "                print('hypercount: %d' % hypercount)\n",
    "                print('\\n')\n",
    "                print(name)\n",
    "                f.write('\\n\\n' + str(name)+ '\\n\\n')\n",
    "                    \n",
    "                # create the NN\n",
    "                class Net(nn.Module):\n",
    "                    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "                        super(Net, self).__init__()\n",
    "                        self.input_size = input_size\n",
    "                        self.emb_size = emb_size \n",
    "                        self.output_size = output_size\n",
    "                        self.number_layers = number_layers\n",
    "                        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "                        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "                        # in bidirectional encoder we have  forward and backward hidden states\n",
    "                        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "                        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "                        # Create affine layer to project to the classes \n",
    "                        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "                        #dropout layer for regularizetion of a sequence\n",
    "                        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "                        self.relu = nn.ReLU()\n",
    "\n",
    "                    def forward(self, x):\n",
    "                        h = self.relu(self.fc1(x))\n",
    "                        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "                        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "                        h = self.relu(self.combination_layer(h))\n",
    "                        h = self.dropout_layer(h)\n",
    "                        h = self.projection(h) \n",
    "                        return h\n",
    "\n",
    "                #create a network \n",
    "                nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "                print(nn_model)\n",
    "                #print(list(nn_model.parameters()))\n",
    "                \n",
    "                # BCE Loss and optimizer\n",
    "                criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(round(zeroes/ones,0))) # class imbalance\n",
    "                #print(round(zeroes/ones,0))\n",
    "                optimizer = optim.Adam(nn_model.parameters(), lr=lr) \n",
    "    \n",
    "    \n",
    "                # TRAINING LOOP \n",
    "                epochs = n_epochs\n",
    "                starttime = datetime.now() # datetime object containing current date and time\n",
    "                train_losses, validation_losses = [], []\n",
    "                best = 0\n",
    "                patience = 0\n",
    "                old_auc = 0\n",
    "                old_pr = 0\n",
    "\n",
    "                for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "                    print (\"\\n Epoch [%d] out of %d\" % (epoch + 1, epochs))\n",
    "                    running_loss = 0.0\n",
    "                    validation_loss = 0.0\n",
    "                    roc_auc = 0.0\n",
    "                    pr_auc = 0.0\n",
    "                    m = 0\n",
    "                    \n",
    "                    #train\n",
    "                    #print(list(nn_model.parameters())[0])\n",
    "                    for i in X_train:\n",
    "                        # zero the parameter gradients\n",
    "                        optimizer.zero_grad() # zero the gradient buffers not to consider gradients of previous iterations\n",
    "                        X_batch = X_train[m]\n",
    "                        y_batch = y_train[m]\n",
    "                        # forward + backward + optimize\n",
    "                        outputs = nn_model(X_batch)\n",
    "                        outputs = torch.flatten(outputs)\n",
    "                        y_batch = y_batch.type_as(outputs)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                        loss.backward()\n",
    "                        optimizer.step() # Does the update\n",
    "                        running_loss += loss.item()\n",
    "                        m +=1\n",
    "                    #validation \n",
    "                    nn_model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        v_out = nn_model(X_val) \n",
    "                        v_out = torch.flatten(v_out) \n",
    "                        y_val = y_val.type_as(v_out)\n",
    "                        v_loss = criterion(v_out, y_val)\n",
    "                        validation_loss = v_loss.item()\n",
    "                        # auc and pr auc\n",
    "                        val_prob = torch.nn.Sigmoid() (v_out)\n",
    "                        precision, recall, thresholds = precision_recall_curve(y_val, val_prob)\n",
    "                        pr_auc = auc(recall, precision)\n",
    "                        roc_auc = roc_auc_score(y_val,val_prob) \n",
    "\n",
    "                    validation_losses.append(validation_loss) \n",
    "                    train_losses.append(running_loss/len(X_train)) \n",
    "                    print(f\"Training loss: {running_loss/len(X_train):.3f}.. \" f\"Validation loss: {validation_loss:.3f}.. \")\n",
    "                    print(f\"AUC: {roc_auc:.2f} \" f\"PR AUC: {pr_auc:.2f} \")  \n",
    "                    nn_model.train()\n",
    "\n",
    "                    \n",
    "                    if roc_auc > best:\n",
    "                        best = roc_auc\n",
    "                        PATH1 = './'+str(name)+'best.pth' \n",
    "                        torch.save(nn_model.state_dict(), PATH1) # save the model\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    if roc_auc == old_auc and pr_auc==old_pr:\n",
    "                        patience +=1\n",
    "                    old_auc = roc_auc\n",
    "                    old_pr = pr_auc\n",
    "                    if patience ==10:\n",
    "                        print(\"out of patience\")\n",
    "                        break\n",
    "\n",
    "                print('\\n Finished Training')\n",
    "                print(\"starttime =\", starttime)\n",
    "                now = datetime.now()\n",
    "                print(\"endtime =\", now)\n",
    "                # end of training loop\n",
    "                \n",
    "                PATH2 = './'+str(name)+'last.pth' \n",
    "                torch.save(nn_model.state_dict(), PATH2) # save the model\n",
    "                print('\\n Last model \\n')\n",
    "                labels, probs = to_one_label(nn_model,label_list,X_test,index_list)\n",
    "                performance (nn_model, labels, probs)\n",
    "                \n",
    "                #load the best model\n",
    "                best_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "                best_model.load_state_dict(torch.load(PATH1))\n",
    "                print('\\n Best model \\n')\n",
    "                labels, probs = to_one_label(best_model,label_list,X_test,index_list)\n",
    "                performance (best_model, labels, probs)\n",
    "f.close() \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
