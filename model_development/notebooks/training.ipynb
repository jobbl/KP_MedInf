{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/jori152b-medinf/KP_MedInf\n",
      "/data/horse/ws/jori152b-medinf/KP_MedInf/model_development\n"
     ]
    }
   ],
   "source": [
    "# only for taurus\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"/home/jori152b/DIR/horse/jori152b-medinf/KP_MedInf/model_development\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-02 11:41:51.811070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "#data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "# NN\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score, roc_auc_score, auc, accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from captum.attr import IntegratedGradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Training on CPU') # On mac book GPU is not possible =() \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter as constant \n",
    "\n",
    "TESTING = False \n",
    "TEST_SIZE = 0.05\n",
    "\n",
    "SPLIT_SIZE = 0.2 \n",
    "\n",
    "NORMALIZATION = 'min-max' \n",
    "\n",
    "CAPPING_THRESHOLD_UPPER = 0.99\n",
    "CAPPING_THRESHOLD_LOWER = 0.01\n",
    "\n",
    "# How much time the prediction should occur (hours)\n",
    "HOURS_AHEAD = 48\n",
    "\n",
    "NORM_TYPE = 'min_max'\n",
    "\n",
    "RANDOM = 42\n",
    "\n",
    "# LSTM\n",
    "batch_size = 5\n",
    "\n",
    "# naming model and plot\n",
    "classifier_name = \"None vs. Any AKI\"    ###change every time #Moderate vs. Severe #None vs. Any #Others vs. Severe\n",
    "plot_name = \"adult_AnyAKI_LR\"    ###change every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions used later\n",
    "\n",
    "def cap_data(df):\n",
    "    print(\"Capping between the {} and {} quantile\".format(CAPPING_THRESHOLD_LOWER, CAPPING_THRESHOLD_UPPER))\n",
    "    cap_mask = df.columns.difference(['icustay_id', 'charttime', 'aki_stage'])\n",
    "    df[cap_mask] = df[cap_mask].clip(df[cap_mask].quantile(CAPPING_THRESHOLD_LOWER),\n",
    "                                     df[cap_mask].quantile(CAPPING_THRESHOLD_UPPER),\n",
    "                                     axis=1)\n",
    "\n",
    "    return df\n",
    " \n",
    "    \n",
    "def normalise_data(df, norm_mask):\n",
    "    print(\"Normalizing in [0,1] with {} normalization\".format(NORMALIZATION))\n",
    "    \n",
    "    min_values = df[norm_mask].min()\n",
    "    max_values = df[norm_mask].max()\n",
    "    \n",
    "    # Skip normalization for constant columns\n",
    "    for column in norm_mask:\n",
    "        if min_values[column] != max_values[column]:\n",
    "            df[column] = (df[column] - min_values[column]) / (max_values[column] - min_values[column])\n",
    "    \n",
    "    normalization_parameters = {column: {'min': min_values[column], 'max': max_values[column]} for column in norm_mask}\n",
    "    \n",
    "    return df, normalization_parameters\n",
    "\n",
    "\n",
    "# impute missing value in resampleing data with most common based on each id\n",
    "def fast_mode(df, key_cols, value_col):\n",
    "    \"\"\" Calculate a column mode, by group, ignoring null values. \n",
    "    \n",
    "    key_cols : list of str - Columns to groupby for calculation of mode.\n",
    "    value_col : str - Column for which to calculate the mode. \n",
    "\n",
    "    Return\n",
    "    pandas.DataFrame\n",
    "        One row for the mode of value_col per key_cols group. If ties, returns the one which is sorted first. \"\"\"\n",
    "    return (df.groupby(key_cols + [value_col]).size() \n",
    "              .to_frame('counts').reset_index() \n",
    "              .sort_values('counts', ascending=False) \n",
    "              .drop_duplicates(subset=key_cols)).drop('counts',axis=1)\n",
    "\n",
    "\n",
    "#get max shape of 3d array\n",
    "def get_dimensions(array, level=0):   \n",
    "    yield level, len(array)\n",
    "    try:\n",
    "        for row in array:\n",
    "            yield from get_dimensions(row, level + 1)\n",
    "    except TypeError: #not an iterable\n",
    "        pass\n",
    "\n",
    "def get_max_shape(array):\n",
    "    dimensions = defaultdict(int)\n",
    "    for level, length in get_dimensions(array):\n",
    "        dimensions[level] = max(dimensions[level], length)\n",
    "    return [value for _, value in sorted(dimensions.items())]\n",
    "\n",
    "#pad the ragged 3d array to rectangular shape based on max size\n",
    "def iterate_nested_array(array, index=()):\n",
    "    try:\n",
    "        for idx, row in enumerate(array):\n",
    "            yield from iterate_nested_array(row, (*index, idx)) \n",
    "    except TypeError: # final level            \n",
    "        yield (*index, slice(len(array))), array # think of the types\n",
    "\n",
    "def pad(array, fill_value):\n",
    "    dimensions = get_max_shape(array)\n",
    "    result = np.full(dimensions, fill_value, dtype = np.float64)  \n",
    "    for index, value in iterate_nested_array(array):\n",
    "        result[index] = value \n",
    "    return result\n",
    "\n",
    "def bin_total(y_true, y_prob, n_bins):\n",
    "    bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "    # In sklearn.calibration.calibration_curve,\n",
    "    # the last value in the array is always 0.\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "def missing_bin(bin_array):\n",
    "    midpoint = \" \"    \n",
    "    if bin_array[0]==0:\n",
    "        midpoint = \"5%, \"\n",
    "    if bin_array[1]==0:\n",
    "        midpoint = midpoint + \"15%, \"\n",
    "    if bin_array[2]==0:\n",
    "        midpoint = midpoint + \"25%, \"\n",
    "    if bin_array[3]==0:\n",
    "        midpoint = midpoint + \"35%, \" \n",
    "    if bin_array[4]==0:\n",
    "        midpoint = midpoint + \"45%, \"\n",
    "    if bin_array[5]==0:\n",
    "        midpoint = midpoint + \"55%, \"\n",
    "    if bin_array[6]==0:\n",
    "        midpoint = midpoint + \"65%, \"\n",
    "    if bin_array[7]==0:\n",
    "        midpoint = midpoint + \"75%, \"\n",
    "    if bin_array[8]==0:\n",
    "        midpoint = midpoint + \"85%, \"\n",
    "    if bin_array[9]==0:\n",
    "        midpoint = midpoint + \"95%, \"\n",
    "    return \"The missing bins have midpoint values of \"+ str(midpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "X = pd.read_csv(f'data/preprocessed/preprocessed_data_extended.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing in [0,1] with min-max normalization\n",
      "icustay_id    299999\n",
      "size             133\n",
      "dtype: int64\n",
      "train is 37797\n",
      "val and test are 4725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3371261/2904364912.py:50: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3371261/2904364912.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
      "/tmp/ipykernel_3371261/2904364912.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numeric_feat = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_feat.remove('aki_stage',)\n",
    "numeric_feat.remove('icustay_id',)\n",
    "\n",
    "\n",
    "# normalize data and cap features\n",
    "# X = cap_data(X)\n",
    "X, normalization_parameters = normalise_data(X, numeric_feat)\n",
    "\n",
    "# X = X.sort_values(by=['icustay_id', 'charttime'])\n",
    "X = X.sort_values(by=['icustay_id'])\n",
    "\n",
    "seq_lengths = X.groupby(['icustay_id'],as_index=False).size().sort_values(by = ['size'],ascending=False)\n",
    "sequence_length = seq_lengths.max() # the longest sequence per icustay-id\n",
    "print(sequence_length)\n",
    "\n",
    "#AL re-write as try except to make it work as hadm_id is not used if only one csv file is used and none are merged\n",
    "try:\n",
    "    X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "id_list = X['icustay_id'].unique()\n",
    "id_train, id_test_val = train_test_split(id_list, test_size = SPLIT_SIZE, random_state = 42) # train set is 80%)\n",
    "print(\"train is %d\" % len(id_train))\n",
    "# remaining 20% split in halves as test and validation 10% and 10%\n",
    "id_valid, id_test = train_test_split(id_test_val, test_size = 0.5, random_state = 42) # test 10% valid 10%\n",
    "print(\"val and test are %d\" %len(id_test))\n",
    "\n",
    "# move (\"aki_stage\") to last column\n",
    "X = X.reindex(columns = [col for col in X.columns if col != 'aki_stage'] + ['aki_stage'])\n",
    "\n",
    "train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index = True) \n",
    "validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id']) \n",
    "\n",
    "test = test.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "train = train.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "validation = validation.sort_values(by=['icustay_id'], ignore_index = True)\n",
    "\n",
    "train.drop(['charttime'], axis=1, inplace = True)  \n",
    "test.drop(['charttime'], axis=1, inplace = True)\n",
    "validation.drop(['charttime'], axis=1, inplace = True)\n",
    "\n",
    "try:\n",
    "    X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "train = train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "test = test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)\n",
    "validation = validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# implement random forest classifier for this tabular data to predict attribute aki_stage\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# flatten the train, test and validation data\n",
    "train_flat = np.concatenate(train, axis=0)\n",
    "test_flat = np.concatenate(test, axis=0)\n",
    "validation_flat = np.concatenate(validation, axis=0)\n",
    "\n",
    "# get the labels\n",
    "train_labels = np.array([x[-1] for x in train_flat])\n",
    "test_labels = np.array([x[-1] for x in test_flat])\n",
    "validation_labels = np.array([x[-1] for x in validation_flat])\n",
    "\n",
    "# get the features\n",
    "train_features = np.array([x[1:-1] for x in train_flat])\n",
    "\n",
    "# create the random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=50, random_state=RANDOM)\n",
    "\n",
    "# train the classifier\n",
    "rf.fit(train_features, train_labels)\n",
    "\n",
    "# get the predictions\n",
    "test_predictions = rf.predict([x[1:-1] for x in test_flat])\n",
    "validation_predictions = rf.predict([x[1:-1] for x in validation_flat])\n",
    "\n",
    "# get the accuracy\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "validation_accuracy = accuracy_score(validation_labels, validation_predictions)\n",
    "\n",
    "print(f'Test accuracy: {test_accuracy}')\n",
    "print(f'Validation accuracy: {validation_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# get the probabilities of the positive class\n",
    "test_prob = rf.predict_proba([x[1:-1] for x in test_flat])[:, 1]\n",
    "validation_prob = rf.predict_proba([x[1:-1] for x in validation_flat])[:, 1]\n",
    "# calculate ROC AUC and PR AUC for the test set\n",
    "test_roc_auc = roc_auc_score(test_labels, test_prob)\n",
    "test_pr_auc = average_precision_score(test_labels, test_prob)\n",
    "\n",
    "# calculate ROC AUC and PR AUC for the validation set\n",
    "validation_roc_auc = roc_auc_score(validation_labels, validation_prob)\n",
    "validation_pr_auc = average_precision_score(validation_labels, validation_prob)\n",
    "\n",
    "print(f\"Test accuracy: {test_accuracy:.3f}.. Test ROC AUC: {test_roc_auc:.2f}.. Test PR AUC: {test_pr_auc:.2f}..\")\n",
    "print(f\"Validation accuracy: {validation_accuracy:.3f}.. Validation ROC AUC: {validation_roc_auc:.2f}.. Validation PR AUC: {validation_pr_auc:.2f}..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_flat[-1][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.896602943686858\n",
      "Test accuracy: 0.8516700515729452\n",
      "Validation accuracy: 0.849157173532733\n",
      "Train accuracy: 0.897.. Train ROC AUC: 0.90.. Train PR AUC: 0.73..\n",
      "Test accuracy: 0.852.. Test ROC AUC: 0.73.. Test PR AUC: 0.41..\n",
      "Validation accuracy: 0.849.. Validation ROC AUC: 0.73.. Validation PR AUC: 0.41..\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# flatten the train, test and validation data\n",
    "train_flat = np.concatenate(train, axis=0)\n",
    "test_flat = np.concatenate(test, axis=0)\n",
    "validation_flat = np.concatenate(validation, axis=0)\n",
    "\n",
    "# get the labels\n",
    "train_labels = np.array([x[-1] for x in train_flat])\n",
    "test_labels = np.array([x[-1] for x in test_flat])\n",
    "validation_labels = np.array([x[-1] for x in validation_flat])\n",
    "\n",
    "# get the features\n",
    "train_features = np.array([x[1:-1] for x in train_flat])\n",
    "\n",
    "# create the XGBoost classifier\n",
    "xgb = XGBClassifier(n_estimators=1000, use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM)\n",
    "\n",
    "# train the classifier\n",
    "xgb.fit(train_features, train_labels)\n",
    "\n",
    "# get the predictions\n",
    "train_predictions = xgb.predict(np.array([x[1:-1] for x in train_flat]))\n",
    "test_predictions = xgb.predict(np.array([x[1:-1] for x in test_flat]))\n",
    "validation_predictions = xgb.predict(np.array([x[1:-1] for x in validation_flat]))\n",
    "\n",
    "# get the accuracy\n",
    "train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "validation_accuracy = accuracy_score(validation_labels, validation_predictions)\n",
    "\n",
    "print(f'Train accuracy: {train_accuracy}')\n",
    "print(f'Test accuracy: {test_accuracy}')\n",
    "print(f'Validation accuracy: {validation_accuracy}')\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# get the probabilities of the positive class\n",
    "training_prob = xgb.predict_proba([x[1:-1] for x in train_flat])[:, 1]\n",
    "test_prob = xgb.predict_proba([x[1:-1] for x in test_flat])[:, 1]\n",
    "validation_prob = xgb.predict_proba([x[1:-1] for x in validation_flat])[:, 1]\n",
    "\n",
    "# calculate ROC AUC and PR AUC for the training set\n",
    "training_roc_auc = roc_auc_score(train_labels, training_prob)\n",
    "training_pr_auc = average_precision_score(train_labels, training_prob)\n",
    "\n",
    "# calculate ROC AUC and PR AUC for the test set\n",
    "test_roc_auc = roc_auc_score(test_labels, test_prob)\n",
    "test_pr_auc = average_precision_score(test_labels, test_prob)\n",
    "\n",
    "# calculate ROC AUC and PR AUC for the validation set\n",
    "validation_roc_auc = roc_auc_score(validation_labels, validation_prob)\n",
    "validation_pr_auc = average_precision_score(validation_labels, validation_prob)\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy:.3f}.. Train ROC AUC: {training_roc_auc:.2f}.. Train PR AUC: {training_pr_auc:.2f}..\")\n",
    "print(f\"Test accuracy: {test_accuracy:.3f}.. Test ROC AUC: {test_roc_auc:.2f}.. Test PR AUC: {test_pr_auc:.2f}..\")\n",
    "print(f\"Validation accuracy: {validation_accuracy:.3f}.. Validation ROC AUC: {validation_roc_auc:.2f}.. Validation PR AUC: {validation_pr_auc:.2f}..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the train, test and validation data\n",
    "train_flat = np.concatenate(train, axis=0)\n",
    "test_flat = np.concatenate(test, axis=0)\n",
    "validation_flat = np.concatenate(validation, axis=0)\n",
    "\n",
    "# get the labels\n",
    "train_labels = np.array([x[-1] for x in train_flat])\n",
    "test_labels = np.array([x[-1] for x in test_flat])\n",
    "validation_labels = np.array([x[-1] for x in validation_flat])\n",
    "\n",
    "# get the features\n",
    "train_features = np.array([x[1:-1] for x in train_flat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[CV] END colsample_bytree=0.749816047538945, gamma=0.4753571532049581, learning_rate=0.22959818254342154, max_depth=7, min_child_weight=5, n_estimators=714, subsample=0.7783331011414365; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.9329770563201687, gamma=0.10616955533913808, learning_rate=0.06454749016213018, max_depth=7, min_child_weight=1, n_estimators=559, subsample=0.8446612641953124; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.6028265220878869, gamma=0.011531212520707879, learning_rate=0.16743239807751675, max_depth=9, min_child_weight=3, n_estimators=975, subsample=0.8056937753654446; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.7760609974958406, gamma=0.06101911742238941, learning_rate=0.15855307303338106, max_depth=9, min_child_weight=3, n_estimators=305, subsample=0.7564242430292963; total time=  47.8s\n",
      "[CV] END colsample_bytree=0.7760609974958406, gamma=0.06101911742238941, learning_rate=0.15855307303338106, max_depth=9, min_child_weight=3, n_estimators=305, subsample=0.7564242430292963; total time=  49.1s\n",
      "[CV] END colsample_bytree=0.9369139098379994, gamma=0.22487706668488283, learning_rate=0.12854507080054434, max_depth=6, min_child_weight=8, n_estimators=561, subsample=0.6783931449676581; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.9369139098379994, gamma=0.22487706668488283, learning_rate=0.12854507080054434, max_depth=6, min_child_weight=8, n_estimators=561, subsample=0.6783931449676581; total time=  59.0s\n",
      "[CV] END colsample_bytree=0.6180909155642152, gamma=0.16266516538163217, learning_rate=0.1266031869068446, max_depth=4, min_child_weight=5, n_estimators=891, subsample=0.7427013306774357; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.908897907718663, gamma=0.0993578407670862, learning_rate=0.01165663513708072, max_depth=5, min_child_weight=3, n_estimators=692, subsample=0.88453678109946; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.908897907718663, gamma=0.0993578407670862, learning_rate=0.01165663513708072, max_depth=5, min_child_weight=3, n_estimators=692, subsample=0.88453678109946; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8550229885420852, gamma=0.44360637128816327, learning_rate=0.15166447754858478, max_depth=7, min_child_weight=3, n_estimators=868, subsample=0.8886918084659492; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.8550229885420852, gamma=0.44360637128816327, learning_rate=0.15166447754858478, max_depth=7, min_child_weight=3, n_estimators=868, subsample=0.8886918084659492; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.7901480892728447, gamma=0.28163778598819184, learning_rate=0.21865482592783825, max_depth=7, min_child_weight=7, n_estimators=330, subsample=0.7641531692142519; total time=  40.3s\n",
      "[CV] END colsample_bytree=0.9022204554172195, gamma=0.11439908274581123, learning_rate=0.0330939729486379, max_depth=5, min_child_weight=3, n_estimators=185, subsample=0.9521871356061031; total time=  24.7s\n",
      "[CV] END colsample_bytree=0.8497416192535173, gamma=0.147816842918857, learning_rate=0.041648277949081186, max_depth=6, min_child_weight=4, n_estimators=897, subsample=0.9533121035675474; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.7297380084021096, gamma=0.061043977350336676, learning_rate=0.11688935142309247, max_depth=8, min_child_weight=1, n_estimators=484, subsample=0.6911740650167767; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.7939319885435933, gamma=0.34621801644513517, learning_rate=0.09082370013955643, max_depth=9, min_child_weight=6, n_estimators=771, subsample=0.8075162486973464; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.881207583558071, gamma=0.181814801189647, learning_rate=0.3015346248162882, max_depth=6, min_child_weight=7, n_estimators=485, subsample=0.8849082359697769; total time=  47.9s\n",
      "[CV] END colsample_bytree=0.7114585856946446, gamma=0.45413294298332685, learning_rate=0.08186856720009172, max_depth=4, min_child_weight=5, n_estimators=783, subsample=0.7791132658292367; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.8211572356285312, gamma=0.29634836193969677, learning_rate=0.03425599789981457, max_depth=5, min_child_weight=1, n_estimators=866, subsample=0.9212559025519583; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.6746074041599417, gamma=0.020387570777381958, learning_rate=0.18726788295647254, max_depth=5, min_child_weight=4, n_estimators=502, subsample=0.884459812975207; total time=  45.3s\n",
      "[CV] END colsample_bytree=0.9238004184558861, gamma=0.1743329936458647, learning_rate=0.03885296532742623, max_depth=3, min_child_weight=1, n_estimators=739, subsample=0.8071005402109921; total time=  57.2s\n",
      "[CV] END colsample_bytree=0.7031766510860622, gamma=0.3299920230170895, learning_rate=0.25516666006036476, max_depth=3, min_child_weight=4, n_estimators=671, subsample=0.7043316699321636; total time=  49.1s\n",
      "[CV] END colsample_bytree=0.8801431319891084, gamma=0.42333057111915295, learning_rate=0.26689728756342773, max_depth=8, min_child_weight=8, n_estimators=772, subsample=0.8568126584617151; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.8654007076432223, gamma=0.0025307919231093434, learning_rate=0.0582424154252496, max_depth=4, min_child_weight=2, n_estimators=921, subsample=0.7793696571944989; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.8598531596188859, gamma=0.42461170524708897, learning_rate=0.207283867690103, max_depth=3, min_child_weight=3, n_estimators=569, subsample=0.7060809470726902; total time=  43.0s\n",
      "[CV] END colsample_bytree=0.8598531596188859, gamma=0.42461170524708897, learning_rate=0.207283867690103, max_depth=3, min_child_weight=3, n_estimators=569, subsample=0.7060809470726902; total time=  42.4s\n",
      "[CV] END colsample_bytree=0.6975958573516334, gamma=0.4865052773762228, learning_rate=0.1279293174000281, max_depth=8, min_child_weight=8, n_estimators=382, subsample=0.7737577462041715; total time=  51.2s\n",
      "[CV] END colsample_bytree=0.8288016796836732, gamma=0.3842770071531545, learning_rate=0.023081131526330123, max_depth=6, min_child_weight=5, n_estimators=617, subsample=0.7480634801021777; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.606182646611547, gamma=0.4641592812938627, learning_rate=0.1384552444951943, max_depth=7, min_child_weight=3, n_estimators=799, subsample=0.941203782186944; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.7177795568278342, gamma=0.19254886430096263, learning_rate=0.26534100145505707, max_depth=5, min_child_weight=2, n_estimators=892, subsample=0.7578765867237889; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.8073318609454947, gamma=0.4386865359639777, learning_rate=0.23223058532626134, max_depth=3, min_child_weight=9, n_estimators=910, subsample=0.8835643987640474; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8211279907631631, gamma=0.14825507182389924, learning_rate=0.13593425693388295, max_depth=4, min_child_weight=9, n_estimators=507, subsample=0.9652962210225885; total time=  39.5s\n",
      "[CV] END colsample_bytree=0.6203074124157587, gamma=0.44330857447532995, learning_rate=0.018285031562111413, max_depth=4, min_child_weight=2, n_estimators=899, subsample=0.8313120563984696; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.7001005442063456, gamma=0.0194173672147116, learning_rate=0.10097965440196684, max_depth=5, min_child_weight=5, n_estimators=363, subsample=0.9079974212394444; total time=  35.1s\n",
      "[CV] END colsample_bytree=0.6863284109987373, gamma=0.31144523790950013, learning_rate=0.0356042394981304, max_depth=7, min_child_weight=4, n_estimators=449, subsample=0.8162540486440426; total time=  57.9s\n",
      "[CV] END colsample_bytree=0.8549719605992826, gamma=0.3630456668613308, learning_rate=0.30275562383876037, max_depth=6, min_child_weight=3, n_estimators=756, subsample=0.7595283605779178; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.8783896824374792, gamma=0.20447647220713494, learning_rate=0.06198829602125373, max_depth=3, min_child_weight=2, n_estimators=195, subsample=0.8196906658824482; total time=  18.5s\n",
      "[CV] END colsample_bytree=0.8783896824374792, gamma=0.20447647220713494, learning_rate=0.06198829602125373, max_depth=3, min_child_weight=2, n_estimators=195, subsample=0.8196906658824482; total time=  18.1s\n",
      "[CV] END colsample_bytree=0.749816047538945, gamma=0.4753571532049581, learning_rate=0.22959818254342154, max_depth=7, min_child_weight=5, n_estimators=714, subsample=0.7783331011414365; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.9329770563201687, gamma=0.10616955533913808, learning_rate=0.06454749016213018, max_depth=7, min_child_weight=1, n_estimators=559, subsample=0.8446612641953124; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.836965827544817, gamma=0.023225206359998862, learning_rate=0.1922634555704315, max_depth=7, min_child_weight=9, n_estimators=266, subsample=0.6053059844639466; total time=  39.1s\n",
      "[CV] END colsample_bytree=0.836965827544817, gamma=0.023225206359998862, learning_rate=0.1922634555704315, max_depth=7, min_child_weight=9, n_estimators=266, subsample=0.6053059844639466; total time=  40.5s\n",
      "[CV] END colsample_bytree=0.9768807022739411, gamma=0.28164410892276964, learning_rate=0.12562495076197483, max_depth=4, min_child_weight=5, n_estimators=997, subsample=0.8736932106048627; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.672894435115225, gamma=0.37768070515882624, learning_rate=0.13754676234737342, max_depth=8, min_child_weight=4, n_estimators=921, subsample=0.6125253169822235; total time= 2.5min\n",
      "[CV] END colsample_bytree=0.6180909155642152, gamma=0.16266516538163217, learning_rate=0.1266031869068446, max_depth=4, min_child_weight=5, n_estimators=891, subsample=0.7427013306774357; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.908897907718663, gamma=0.0993578407670862, learning_rate=0.01165663513708072, max_depth=5, min_child_weight=3, n_estimators=692, subsample=0.88453678109946; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.9160702162124823, gamma=0.3029799873905057, learning_rate=0.2878902635540047, max_depth=4, min_child_weight=9, n_estimators=127, subsample=0.9452413703502374; total time=  15.6s\n",
      "[CV] END colsample_bytree=0.8493192507310232, gamma=0.1654490124263246, learning_rate=0.02906750508580709, max_depth=9, min_child_weight=8, n_estimators=904, subsample=0.8918424713352255; total time= 2.9min\n",
      "[CV] END colsample_bytree=0.7901480892728447, gamma=0.28163778598819184, learning_rate=0.21865482592783825, max_depth=7, min_child_weight=7, n_estimators=330, subsample=0.7641531692142519; total time=  47.2s\n",
      "[CV] END colsample_bytree=0.7901480892728447, gamma=0.28163778598819184, learning_rate=0.21865482592783825, max_depth=7, min_child_weight=7, n_estimators=330, subsample=0.7641531692142519; total time=  47.4s\n",
      "[CV] END colsample_bytree=0.7901480892728447, gamma=0.28163778598819184, learning_rate=0.21865482592783825, max_depth=7, min_child_weight=7, n_estimators=330, subsample=0.7641531692142519; total time=  45.4s\n",
      "[CV] END colsample_bytree=0.7297380084021096, gamma=0.061043977350336676, learning_rate=0.11688935142309247, max_depth=8, min_child_weight=1, n_estimators=484, subsample=0.6911740650167767; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.7708431154505025, gamma=0.40900738296124656, learning_rate=0.268219174976903, max_depth=9, min_child_weight=6, n_estimators=555, subsample=0.8136357677501768; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.881207583558071, gamma=0.181814801189647, learning_rate=0.3015346248162882, max_depth=6, min_child_weight=7, n_estimators=485, subsample=0.8849082359697769; total time=  57.0s\n",
      "[CV] END colsample_bytree=0.6592347719813599, gamma=0.49887024252447093, learning_rate=0.09003430428258549, max_depth=4, min_child_weight=2, n_estimators=319, subsample=0.6205915004999957; total time=  37.0s\n",
      "[CV] END colsample_bytree=0.7114585856946446, gamma=0.45413294298332685, learning_rate=0.08186856720009172, max_depth=4, min_child_weight=5, n_estimators=783, subsample=0.7791132658292367; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.7881202537784153, gamma=0.4917115704474215, learning_rate=0.12964733273336593, max_depth=9, min_child_weight=1, n_estimators=300, subsample=0.7283120259886944; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.9238004184558861, gamma=0.1743329936458647, learning_rate=0.03885296532742623, max_depth=3, min_child_weight=1, n_estimators=739, subsample=0.8071005402109921; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.9350840423629312, gamma=0.33784505851964036, learning_rate=0.23056483577223164, max_depth=4, min_child_weight=7, n_estimators=514, subsample=0.9509357413523924; total time=  45.8s\n",
      "[CV] END colsample_bytree=0.9985014799031697, gamma=0.4827096756443968, learning_rate=0.1774880360821293, max_depth=5, min_child_weight=7, n_estimators=108, subsample=0.7115485410368727; total time=  16.4s\n",
      "[CV] END colsample_bytree=0.9985014799031697, gamma=0.4827096756443968, learning_rate=0.1774880360821293, max_depth=5, min_child_weight=7, n_estimators=108, subsample=0.7115485410368727; total time=  15.4s\n",
      "[CV] END colsample_bytree=0.6336559859980195, gamma=0.08081435704730688, learning_rate=0.2795662565581238, max_depth=6, min_child_weight=3, n_estimators=211, subsample=0.6405886171464128; total time=  28.2s\n",
      "[CV] END colsample_bytree=0.8654007076432223, gamma=0.0025307919231093434, learning_rate=0.0582424154252496, max_depth=4, min_child_weight=2, n_estimators=921, subsample=0.7793696571944989; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.9977829850443283, gamma=0.08796262633867269, learning_rate=0.015422609084656261, max_depth=8, min_child_weight=5, n_estimators=915, subsample=0.8985965620472096; total time= 2.6min\n",
      "[CV] END colsample_bytree=0.7400313630778703, gamma=0.3225516810152824, learning_rate=0.2106772178989299, max_depth=3, min_child_weight=5, n_estimators=928, subsample=0.7996773519539009; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8288016796836732, gamma=0.3842770071531545, learning_rate=0.023081131526330123, max_depth=6, min_child_weight=5, n_estimators=617, subsample=0.7480634801021777; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.7177795568278342, gamma=0.19254886430096263, learning_rate=0.26534100145505707, max_depth=5, min_child_weight=2, n_estimators=892, subsample=0.7578765867237889; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8073318609454947, gamma=0.4386865359639777, learning_rate=0.23223058532626134, max_depth=3, min_child_weight=9, n_estimators=910, subsample=0.8835643987640474; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8211279907631631, gamma=0.14825507182389924, learning_rate=0.13593425693388295, max_depth=4, min_child_weight=9, n_estimators=507, subsample=0.9652962210225885; total time=  50.7s\n",
      "[CV] END colsample_bytree=0.6203074124157587, gamma=0.44330857447532995, learning_rate=0.018285031562111413, max_depth=4, min_child_weight=2, n_estimators=899, subsample=0.8313120563984696; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.6143769095186968, gamma=0.23279900906623008, learning_rate=0.17279339041227298, max_depth=7, min_child_weight=6, n_estimators=413, subsample=0.9441618473246701; total time=  53.9s\n",
      "[CV] END colsample_bytree=0.8549719605992826, gamma=0.3630456668613308, learning_rate=0.30275562383876037, max_depth=6, min_child_weight=3, n_estimators=756, subsample=0.7595283605779178; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.7734082950322968, gamma=0.3720213214995577, learning_rate=0.08525815820399835, max_depth=4, min_child_weight=5, n_estimators=929, subsample=0.9343920482048823; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8446882984937409, gamma=0.20980003121389496, learning_rate=0.08431929685034723, max_depth=5, min_child_weight=5, n_estimators=185, subsample=0.6057573954519023; total time=  25.2s\n",
      "[CV] END colsample_bytree=0.8446882984937409, gamma=0.20980003121389496, learning_rate=0.08431929685034723, max_depth=5, min_child_weight=5, n_estimators=185, subsample=0.6057573954519023; total time=  25.5s\n",
      "[CV] END colsample_bytree=0.6464290562027665, gamma=0.023001321010876374, learning_rate=0.02221864069569104, max_depth=8, min_child_weight=4, n_estimators=552, subsample=0.78966953163493; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.815750979360025, gamma=0.34198188469907054, learning_rate=0.19475534931697416, max_depth=9, min_child_weight=2, n_estimators=596, subsample=0.9468795734220015; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.749816047538945, gamma=0.4753571532049581, learning_rate=0.22959818254342154, max_depth=7, min_child_weight=5, n_estimators=714, subsample=0.7783331011414365; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.9329770563201687, gamma=0.10616955533913808, learning_rate=0.06454749016213018, max_depth=7, min_child_weight=1, n_estimators=559, subsample=0.8446612641953124; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.836965827544817, gamma=0.023225206359998862, learning_rate=0.1922634555704315, max_depth=7, min_child_weight=9, n_estimators=266, subsample=0.6053059844639466; total time=  36.1s\n",
      "[CV] END colsample_bytree=0.9768807022739411, gamma=0.28164410892276964, learning_rate=0.12562495076197483, max_depth=4, min_child_weight=5, n_estimators=997, subsample=0.8736932106048627; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.7760609974958406, gamma=0.06101911742238941, learning_rate=0.15855307303338106, max_depth=9, min_child_weight=3, n_estimators=305, subsample=0.7564242430292963; total time=  51.5s\n",
      "[CV] END colsample_bytree=0.9369139098379994, gamma=0.22487706668488283, learning_rate=0.12854507080054434, max_depth=6, min_child_weight=8, n_estimators=561, subsample=0.6783931449676581; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.6180909155642152, gamma=0.16266516538163217, learning_rate=0.1266031869068446, max_depth=4, min_child_weight=5, n_estimators=891, subsample=0.7427013306774357; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.7123738038749523, gamma=0.27134804157912423, learning_rate=0.052277267492428794, max_depth=9, min_child_weight=1, n_estimators=956, subsample=0.9947547746402069; total time= 2.7min\n",
      "[CV] END colsample_bytree=0.8493192507310232, gamma=0.1654490124263246, learning_rate=0.02906750508580709, max_depth=9, min_child_weight=8, n_estimators=904, subsample=0.8918424713352255; total time= 2.7min\n",
      "[CV] END colsample_bytree=0.6943939678995823, gamma=0.12803416138066198, learning_rate=0.022130076861529402, max_depth=9, min_child_weight=3, n_estimators=674, subsample=0.9583054382694077; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.9022204554172195, gamma=0.11439908274581123, learning_rate=0.0330939729486379, max_depth=5, min_child_weight=3, n_estimators=185, subsample=0.9521871356061031; total time=  24.9s\n",
      "[CV] END colsample_bytree=0.8497416192535173, gamma=0.147816842918857, learning_rate=0.041648277949081186, max_depth=6, min_child_weight=4, n_estimators=897, subsample=0.9533121035675474; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.7708431154505025, gamma=0.40900738296124656, learning_rate=0.268219174976903, max_depth=9, min_child_weight=6, n_estimators=555, subsample=0.8136357677501768; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.881207583558071, gamma=0.181814801189647, learning_rate=0.3015346248162882, max_depth=6, min_child_weight=7, n_estimators=485, subsample=0.8849082359697769; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.6592347719813599, gamma=0.49887024252447093, learning_rate=0.09003430428258549, max_depth=4, min_child_weight=2, n_estimators=319, subsample=0.6205915004999957; total time=  33.5s\n",
      "[CV] END colsample_bytree=0.6592347719813599, gamma=0.49887024252447093, learning_rate=0.09003430428258549, max_depth=4, min_child_weight=2, n_estimators=319, subsample=0.6205915004999957; total time=  32.3s\n",
      "[CV] END colsample_bytree=0.8211572356285312, gamma=0.29634836193969677, learning_rate=0.03425599789981457, max_depth=5, min_child_weight=1, n_estimators=866, subsample=0.9212559025519583; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.7881202537784153, gamma=0.4917115704474215, learning_rate=0.12964733273336593, max_depth=9, min_child_weight=1, n_estimators=300, subsample=0.7283120259886944; total time=  55.9s\n",
      "[CV] END colsample_bytree=0.9238004184558861, gamma=0.1743329936458647, learning_rate=0.03885296532742623, max_depth=3, min_child_weight=1, n_estimators=739, subsample=0.8071005402109921; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.7031766510860622, gamma=0.3299920230170895, learning_rate=0.25516666006036476, max_depth=3, min_child_weight=4, n_estimators=671, subsample=0.7043316699321636; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.6336559859980195, gamma=0.08081435704730688, learning_rate=0.2795662565581238, max_depth=6, min_child_weight=3, n_estimators=211, subsample=0.6405886171464128; total time=  32.3s\n",
      "[CV] END colsample_bytree=0.6336559859980195, gamma=0.08081435704730688, learning_rate=0.2795662565581238, max_depth=6, min_child_weight=3, n_estimators=211, subsample=0.6405886171464128; total time=  29.4s\n",
      "[CV] END colsample_bytree=0.8654007076432223, gamma=0.0025307919231093434, learning_rate=0.0582424154252496, max_depth=4, min_child_weight=2, n_estimators=921, subsample=0.7793696571944989; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.9977829850443283, gamma=0.08796262633867269, learning_rate=0.015422609084656261, max_depth=8, min_child_weight=5, n_estimators=915, subsample=0.8985965620472096; total time= 2.6min\n",
      "[CV] END colsample_bytree=0.7400313630778703, gamma=0.3225516810152824, learning_rate=0.2106772178989299, max_depth=3, min_child_weight=5, n_estimators=928, subsample=0.7996773519539009; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.606182646611547, gamma=0.4641592812938627, learning_rate=0.1384552444951943, max_depth=7, min_child_weight=3, n_estimators=799, subsample=0.941203782186944; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.7177795568278342, gamma=0.19254886430096263, learning_rate=0.26534100145505707, max_depth=5, min_child_weight=2, n_estimators=892, subsample=0.7578765867237889; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8073318609454947, gamma=0.4386865359639777, learning_rate=0.23223058532626134, max_depth=3, min_child_weight=9, n_estimators=910, subsample=0.8835643987640474; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8045369595443751, gamma=0.2507581473435998, learning_rate=0.24948855369003253, max_depth=3, min_child_weight=8, n_estimators=611, subsample=0.7587135308855554; total time=  50.7s\n",
      "[CV] END colsample_bytree=0.6143769095186968, gamma=0.23279900906623008, learning_rate=0.17279339041227298, max_depth=7, min_child_weight=6, n_estimators=413, subsample=0.9441618473246701; total time=  52.6s\n",
      "[CV] END colsample_bytree=0.7001005442063456, gamma=0.0194173672147116, learning_rate=0.10097965440196684, max_depth=5, min_child_weight=5, n_estimators=363, subsample=0.9079974212394444; total time=  40.0s\n",
      "[CV] END colsample_bytree=0.6863284109987373, gamma=0.31144523790950013, learning_rate=0.0356042394981304, max_depth=7, min_child_weight=4, n_estimators=449, subsample=0.8162540486440426; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.7734082950322968, gamma=0.3720213214995577, learning_rate=0.08525815820399835, max_depth=4, min_child_weight=5, n_estimators=929, subsample=0.9343920482048823; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.8783896824374792, gamma=0.20447647220713494, learning_rate=0.06198829602125373, max_depth=3, min_child_weight=2, n_estimators=195, subsample=0.8196906658824482; total time=  19.6s\n",
      "[CV] END colsample_bytree=0.8446882984937409, gamma=0.20980003121389496, learning_rate=0.08431929685034723, max_depth=5, min_child_weight=5, n_estimators=185, subsample=0.6057573954519023; total time=  24.4s\n",
      "[CV] END colsample_bytree=0.8446882984937409, gamma=0.20980003121389496, learning_rate=0.08431929685034723, max_depth=5, min_child_weight=5, n_estimators=185, subsample=0.6057573954519023; total time=  25.1s\n",
      "[CV] END colsample_bytree=0.6464290562027665, gamma=0.023001321010876374, learning_rate=0.02221864069569104, max_depth=8, min_child_weight=4, n_estimators=552, subsample=0.78966953163493; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.9796582926365544, gamma=0.4433401936490238, learning_rate=0.08826808700251418, max_depth=4, min_child_weight=3, n_estimators=541, subsample=0.8004159535661037; total time=  51.6s\n",
      "[CV] END colsample_bytree=0.8545614389784044, gamma=0.40047464734119986, learning_rate=0.21315050271489452, max_depth=5, min_child_weight=1, n_estimators=790, subsample=0.9282557902975821; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.6399899663272012, gamma=0.22962444598293358, learning_rate=0.11011258334170654, max_depth=5, min_child_weight=6, n_estimators=408, subsample=0.9879639408647978; total time=  42.3s\n",
      "[CV] END colsample_bytree=0.6399899663272012, gamma=0.22962444598293358, learning_rate=0.11011258334170654, max_depth=5, min_child_weight=6, n_estimators=408, subsample=0.9879639408647978; total time=  43.1s\n",
      "[CV] END colsample_bytree=0.6028265220878869, gamma=0.011531212520707879, learning_rate=0.16743239807751675, max_depth=9, min_child_weight=3, n_estimators=975, subsample=0.8056937753654446; total time= 2.9min\n",
      "[CV] END colsample_bytree=0.7760609974958406, gamma=0.06101911742238941, learning_rate=0.15855307303338106, max_depth=9, min_child_weight=3, n_estimators=305, subsample=0.7564242430292963; total time=  57.7s\n",
      "[CV] END colsample_bytree=0.672894435115225, gamma=0.37768070515882624, learning_rate=0.13754676234737342, max_depth=8, min_child_weight=4, n_estimators=921, subsample=0.6125253169822235; total time= 2.5min\n",
      "[CV] END colsample_bytree=0.6180909155642152, gamma=0.16266516538163217, learning_rate=0.1266031869068446, max_depth=4, min_child_weight=5, n_estimators=891, subsample=0.7427013306774357; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.7123738038749523, gamma=0.27134804157912423, learning_rate=0.052277267492428794, max_depth=9, min_child_weight=1, n_estimators=956, subsample=0.9947547746402069; total time= 2.6min\n",
      "[CV] END colsample_bytree=0.8550229885420852, gamma=0.44360637128816327, learning_rate=0.15166447754858478, max_depth=7, min_child_weight=3, n_estimators=868, subsample=0.8886918084659492; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.8550229885420852, gamma=0.44360637128816327, learning_rate=0.15166447754858478, max_depth=7, min_child_weight=3, n_estimators=868, subsample=0.8886918084659492; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.7901480892728447, gamma=0.28163778598819184, learning_rate=0.21865482592783825, max_depth=7, min_child_weight=7, n_estimators=330, subsample=0.7641531692142519; total time=  44.8s\n",
      "[CV] END colsample_bytree=0.8497416192535173, gamma=0.147816842918857, learning_rate=0.041648277949081186, max_depth=6, min_child_weight=4, n_estimators=897, subsample=0.9533121035675474; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.7708431154505025, gamma=0.40900738296124656, learning_rate=0.268219174976903, max_depth=9, min_child_weight=6, n_estimators=555, subsample=0.8136357677501768; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.881207583558071, gamma=0.181814801189647, learning_rate=0.3015346248162882, max_depth=6, min_child_weight=7, n_estimators=485, subsample=0.8849082359697769; total time=  58.3s\n",
      "[CV] END colsample_bytree=0.881207583558071, gamma=0.181814801189647, learning_rate=0.3015346248162882, max_depth=6, min_child_weight=7, n_estimators=485, subsample=0.8849082359697769; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.7114585856946446, gamma=0.45413294298332685, learning_rate=0.08186856720009172, max_depth=4, min_child_weight=5, n_estimators=783, subsample=0.7791132658292367; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.7881202537784153, gamma=0.4917115704474215, learning_rate=0.12964733273336593, max_depth=9, min_child_weight=1, n_estimators=300, subsample=0.7283120259886944; total time=  54.9s\n",
      "[CV] END colsample_bytree=0.6746074041599417, gamma=0.020387570777381958, learning_rate=0.18726788295647254, max_depth=5, min_child_weight=4, n_estimators=502, subsample=0.884459812975207; total time=  50.4s\n",
      "[CV] END colsample_bytree=0.9350840423629312, gamma=0.33784505851964036, learning_rate=0.23056483577223164, max_depth=4, min_child_weight=7, n_estimators=514, subsample=0.9509357413523924; total time=  46.3s\n",
      "[CV] END colsample_bytree=0.7031766510860622, gamma=0.3299920230170895, learning_rate=0.25516666006036476, max_depth=3, min_child_weight=4, n_estimators=671, subsample=0.7043316699321636; total time=  59.1s\n",
      "[CV] END colsample_bytree=0.6336559859980195, gamma=0.08081435704730688, learning_rate=0.2795662565581238, max_depth=6, min_child_weight=3, n_estimators=211, subsample=0.6405886171464128; total time=  29.1s\n",
      "[CV] END colsample_bytree=0.6336559859980195, gamma=0.08081435704730688, learning_rate=0.2795662565581238, max_depth=6, min_child_weight=3, n_estimators=211, subsample=0.6405886171464128; total time=  29.3s\n",
      "[CV] END colsample_bytree=0.8654007076432223, gamma=0.0025307919231093434, learning_rate=0.0582424154252496, max_depth=4, min_child_weight=2, n_estimators=921, subsample=0.7793696571944989; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.8598531596188859, gamma=0.42461170524708897, learning_rate=0.207283867690103, max_depth=3, min_child_weight=3, n_estimators=569, subsample=0.7060809470726902; total time=  56.2s\n",
      "[CV] END colsample_bytree=0.8598531596188859, gamma=0.42461170524708897, learning_rate=0.207283867690103, max_depth=3, min_child_weight=3, n_estimators=569, subsample=0.7060809470726902; total time=  51.5s\n",
      "[CV] END colsample_bytree=0.7400313630778703, gamma=0.3225516810152824, learning_rate=0.2106772178989299, max_depth=3, min_child_weight=5, n_estimators=928, subsample=0.7996773519539009; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8288016796836732, gamma=0.3842770071531545, learning_rate=0.023081131526330123, max_depth=6, min_child_weight=5, n_estimators=617, subsample=0.7480634801021777; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.7177795568278342, gamma=0.19254886430096263, learning_rate=0.26534100145505707, max_depth=5, min_child_weight=2, n_estimators=892, subsample=0.7578765867237889; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.9376852562905246, gamma=0.46500841740541593, learning_rate=0.031124839254863167, max_depth=4, min_child_weight=6, n_estimators=762, subsample=0.6560336060946096; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.8211279907631631, gamma=0.14825507182389924, learning_rate=0.13593425693388295, max_depth=4, min_child_weight=9, n_estimators=507, subsample=0.9652962210225885; total time=  52.1s\n",
      "[CV] END colsample_bytree=0.6203074124157587, gamma=0.44330857447532995, learning_rate=0.018285031562111413, max_depth=4, min_child_weight=2, n_estimators=899, subsample=0.8313120563984696; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.6143769095186968, gamma=0.23279900906623008, learning_rate=0.17279339041227298, max_depth=7, min_child_weight=6, n_estimators=413, subsample=0.9441618473246701; total time=  52.9s\n",
      "[CV] END colsample_bytree=0.6863284109987373, gamma=0.31144523790950013, learning_rate=0.0356042394981304, max_depth=7, min_child_weight=4, n_estimators=449, subsample=0.8162540486440426; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.7734082950322968, gamma=0.3720213214995577, learning_rate=0.08525815820399835, max_depth=4, min_child_weight=5, n_estimators=929, subsample=0.9343920482048823; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.8858383690800249, gamma=0.33009868835886563, learning_rate=0.09398016908378284, max_depth=7, min_child_weight=7, n_estimators=803, subsample=0.8217416210045603; total time= 1.8min\n",
      "[CV] END colsample_bytree=0.6464290562027665, gamma=0.023001321010876374, learning_rate=0.02221864069569104, max_depth=8, min_child_weight=4, n_estimators=552, subsample=0.78966953163493; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.8545614389784044, gamma=0.40047464734119986, learning_rate=0.21315050271489452, max_depth=5, min_child_weight=1, n_estimators=790, subsample=0.9282557902975821; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.8545614389784044, gamma=0.40047464734119986, learning_rate=0.21315050271489452, max_depth=5, min_child_weight=1, n_estimators=790, subsample=0.9282557902975821; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.6472659310486625, gamma=0.3483685826820753, learning_rate=0.1986828540339652, max_depth=7, min_child_weight=8, n_estimators=519, subsample=0.7671784126862315; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.9003484271165989, gamma=0.3772714370423412, learning_rate=0.040937160650779784, max_depth=3, min_child_weight=1, n_estimators=161, subsample=0.7986244762732311; total time=  19.7s\n",
      "[CV] END colsample_bytree=0.749816047538945, gamma=0.4753571532049581, learning_rate=0.22959818254342154, max_depth=7, min_child_weight=5, n_estimators=714, subsample=0.7783331011414365; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.9329770563201687, gamma=0.10616955533913808, learning_rate=0.06454749016213018, max_depth=7, min_child_weight=1, n_estimators=559, subsample=0.8446612641953124; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.836965827544817, gamma=0.023225206359998862, learning_rate=0.1922634555704315, max_depth=7, min_child_weight=9, n_estimators=266, subsample=0.6053059844639466; total time=  38.9s\n",
      "[CV] END colsample_bytree=0.836965827544817, gamma=0.023225206359998862, learning_rate=0.1922634555704315, max_depth=7, min_child_weight=9, n_estimators=266, subsample=0.6053059844639466; total time=  39.2s\n",
      "[CV] END colsample_bytree=0.9768807022739411, gamma=0.28164410892276964, learning_rate=0.12562495076197483, max_depth=4, min_child_weight=5, n_estimators=997, subsample=0.8736932106048627; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.672894435115225, gamma=0.37768070515882624, learning_rate=0.13754676234737342, max_depth=8, min_child_weight=4, n_estimators=921, subsample=0.6125253169822235; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.6180909155642152, gamma=0.16266516538163217, learning_rate=0.1266031869068446, max_depth=4, min_child_weight=5, n_estimators=891, subsample=0.7427013306774357; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.908897907718663, gamma=0.0993578407670862, learning_rate=0.01165663513708072, max_depth=5, min_child_weight=3, n_estimators=692, subsample=0.88453678109946; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.908897907718663, gamma=0.0993578407670862, learning_rate=0.01165663513708072, max_depth=5, min_child_weight=3, n_estimators=692, subsample=0.88453678109946; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.8550229885420852, gamma=0.44360637128816327, learning_rate=0.15166447754858478, max_depth=7, min_child_weight=3, n_estimators=868, subsample=0.8886918084659492; total time= 1.8min\n",
      "[CV] END colsample_bytree=0.6943939678995823, gamma=0.12803416138066198, learning_rate=0.022130076861529402, max_depth=9, min_child_weight=3, n_estimators=674, subsample=0.9583054382694077; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.9022204554172195, gamma=0.11439908274581123, learning_rate=0.0330939729486379, max_depth=5, min_child_weight=3, n_estimators=185, subsample=0.9521871356061031; total time=  23.3s\n",
      "[CV] END colsample_bytree=0.8497416192535173, gamma=0.147816842918857, learning_rate=0.041648277949081186, max_depth=6, min_child_weight=4, n_estimators=897, subsample=0.9533121035675474; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.7708431154505025, gamma=0.40900738296124656, learning_rate=0.268219174976903, max_depth=9, min_child_weight=6, n_estimators=555, subsample=0.8136357677501768; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.7939319885435933, gamma=0.34621801644513517, learning_rate=0.09082370013955643, max_depth=9, min_child_weight=6, n_estimators=771, subsample=0.8075162486973464; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.6592347719813599, gamma=0.49887024252447093, learning_rate=0.09003430428258549, max_depth=4, min_child_weight=2, n_estimators=319, subsample=0.6205915004999957; total time=  49.5s\n",
      "[CV] END colsample_bytree=0.8211572356285312, gamma=0.29634836193969677, learning_rate=0.03425599789981457, max_depth=5, min_child_weight=1, n_estimators=866, subsample=0.9212559025519583; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.6746074041599417, gamma=0.020387570777381958, learning_rate=0.18726788295647254, max_depth=5, min_child_weight=4, n_estimators=502, subsample=0.884459812975207; total time=  49.7s\n",
      "[CV] END colsample_bytree=0.9238004184558861, gamma=0.1743329936458647, learning_rate=0.03885296532742623, max_depth=3, min_child_weight=1, n_estimators=739, subsample=0.8071005402109921; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.7031766510860622, gamma=0.3299920230170895, learning_rate=0.25516666006036476, max_depth=3, min_child_weight=4, n_estimators=671, subsample=0.7043316699321636; total time=  54.1s\n",
      "[CV] END colsample_bytree=0.8801431319891084, gamma=0.42333057111915295, learning_rate=0.26689728756342773, max_depth=8, min_child_weight=8, n_estimators=772, subsample=0.8568126584617151; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.9977829850443283, gamma=0.08796262633867269, learning_rate=0.015422609084656261, max_depth=8, min_child_weight=5, n_estimators=915, subsample=0.8985965620472096; total time= 2.5min\n",
      "[CV] END colsample_bytree=0.6975958573516334, gamma=0.4865052773762228, learning_rate=0.1279293174000281, max_depth=8, min_child_weight=8, n_estimators=382, subsample=0.7737577462041715; total time=  57.9s\n",
      "[CV] END colsample_bytree=0.7400313630778703, gamma=0.3225516810152824, learning_rate=0.2106772178989299, max_depth=3, min_child_weight=5, n_estimators=928, subsample=0.7996773519539009; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.606182646611547, gamma=0.4641592812938627, learning_rate=0.1384552444951943, max_depth=7, min_child_weight=3, n_estimators=799, subsample=0.941203782186944; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.9376852562905246, gamma=0.46500841740541593, learning_rate=0.031124839254863167, max_depth=4, min_child_weight=6, n_estimators=762, subsample=0.6560336060946096; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.8211279907631631, gamma=0.14825507182389924, learning_rate=0.13593425693388295, max_depth=4, min_child_weight=9, n_estimators=507, subsample=0.9652962210225885; total time=  45.3s\n",
      "[CV] END colsample_bytree=0.8045369595443751, gamma=0.2507581473435998, learning_rate=0.24948855369003253, max_depth=3, min_child_weight=8, n_estimators=611, subsample=0.7587135308855554; total time=  52.7s\n",
      "[CV] END colsample_bytree=0.6143769095186968, gamma=0.23279900906623008, learning_rate=0.17279339041227298, max_depth=7, min_child_weight=6, n_estimators=413, subsample=0.9441618473246701; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.6863284109987373, gamma=0.31144523790950013, learning_rate=0.0356042394981304, max_depth=7, min_child_weight=4, n_estimators=449, subsample=0.8162540486440426; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8549719605992826, gamma=0.3630456668613308, learning_rate=0.30275562383876037, max_depth=6, min_child_weight=3, n_estimators=756, subsample=0.7595283605779178; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8858383690800249, gamma=0.33009868835886563, learning_rate=0.09398016908378284, max_depth=7, min_child_weight=7, n_estimators=803, subsample=0.8217416210045603; total time= 1.8min\n",
      "[CV] END colsample_bytree=0.6391336642604005, gamma=0.24580793755841618, learning_rate=0.15204153123416972, max_depth=3, min_child_weight=5, n_estimators=122, subsample=0.6192235696788134; total time=  14.3s\n",
      "[CV] END colsample_bytree=0.6391336642604005, gamma=0.24580793755841618, learning_rate=0.15204153123416972, max_depth=3, min_child_weight=5, n_estimators=122, subsample=0.6192235696788134; total time=  14.2s\n",
      "[CV] END colsample_bytree=0.9796582926365544, gamma=0.4433401936490238, learning_rate=0.08826808700251418, max_depth=4, min_child_weight=3, n_estimators=541, subsample=0.8004159535661037; total time=  54.6s\n",
      "[CV] END colsample_bytree=0.815750979360025, gamma=0.34198188469907054, learning_rate=0.19475534931697416, max_depth=9, min_child_weight=2, n_estimators=596, subsample=0.9468795734220015; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.850375868040629, gamma=0.41021342188563403, learning_rate=0.20544543155733638, max_depth=5, min_child_weight=2, n_estimators=352, subsample=0.6377771843023713; total time=  41.4s\n",
      "[CV] END colsample_bytree=0.8732027093665427, gamma=0.035594324230114494, learning_rate=0.10569268908812839, max_depth=4, min_child_weight=2, n_estimators=214, subsample=0.7127419099093599; total time=  24.0s\n",
      "[CV] END colsample_bytree=0.987032220608099, gamma=0.27398594162404366, learning_rate=0.13704128269243557, max_depth=5, min_child_weight=1, n_estimators=198, subsample=0.7650470707645706; total time=  26.0s\n",
      "[CV] END colsample_bytree=0.6399899663272012, gamma=0.22962444598293358, learning_rate=0.11011258334170654, max_depth=5, min_child_weight=6, n_estimators=408, subsample=0.9879639408647978; total time=  43.1s\n",
      "[CV] END colsample_bytree=0.9329770563201687, gamma=0.10616955533913808, learning_rate=0.06454749016213018, max_depth=7, min_child_weight=1, n_estimators=559, subsample=0.8446612641953124; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.6028265220878869, gamma=0.011531212520707879, learning_rate=0.16743239807751675, max_depth=9, min_child_weight=3, n_estimators=975, subsample=0.8056937753654446; total time= 2.8min\n",
      "[CV] END colsample_bytree=0.7760609974958406, gamma=0.06101911742238941, learning_rate=0.15855307303338106, max_depth=9, min_child_weight=3, n_estimators=305, subsample=0.7564242430292963; total time=  56.1s\n",
      "[CV] END colsample_bytree=0.672894435115225, gamma=0.37768070515882624, learning_rate=0.13754676234737342, max_depth=8, min_child_weight=4, n_estimators=921, subsample=0.6125253169822235; total time= 2.5min\n",
      "[CV] END colsample_bytree=0.7123738038749523, gamma=0.27134804157912423, learning_rate=0.052277267492428794, max_depth=9, min_child_weight=1, n_estimators=956, subsample=0.9947547746402069; total time= 2.7min\n",
      "[CV] END colsample_bytree=0.9160702162124823, gamma=0.3029799873905057, learning_rate=0.2878902635540047, max_depth=4, min_child_weight=9, n_estimators=127, subsample=0.9452413703502374; total time=  14.5s\n",
      "[CV] END colsample_bytree=0.8493192507310232, gamma=0.1654490124263246, learning_rate=0.02906750508580709, max_depth=9, min_child_weight=8, n_estimators=904, subsample=0.8918424713352255; total time= 2.7min\n",
      "[CV] END colsample_bytree=0.6943939678995823, gamma=0.12803416138066198, learning_rate=0.022130076861529402, max_depth=9, min_child_weight=3, n_estimators=674, subsample=0.9583054382694077; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.9022204554172195, gamma=0.11439908274581123, learning_rate=0.0330939729486379, max_depth=5, min_child_weight=3, n_estimators=185, subsample=0.9521871356061031; total time=  26.0s\n",
      "[CV] END colsample_bytree=0.7297380084021096, gamma=0.061043977350336676, learning_rate=0.11688935142309247, max_depth=8, min_child_weight=1, n_estimators=484, subsample=0.6911740650167767; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.7708431154505025, gamma=0.40900738296124656, learning_rate=0.268219174976903, max_depth=9, min_child_weight=6, n_estimators=555, subsample=0.8136357677501768; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.7939319885435933, gamma=0.34621801644513517, learning_rate=0.09082370013955643, max_depth=9, min_child_weight=6, n_estimators=771, subsample=0.8075162486973464; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.7114585856946446, gamma=0.45413294298332685, learning_rate=0.08186856720009172, max_depth=4, min_child_weight=5, n_estimators=783, subsample=0.7791132658292367; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.7881202537784153, gamma=0.4917115704474215, learning_rate=0.12964733273336593, max_depth=9, min_child_weight=1, n_estimators=300, subsample=0.7283120259886944; total time=  57.9s\n",
      "[CV] END colsample_bytree=0.6746074041599417, gamma=0.020387570777381958, learning_rate=0.18726788295647254, max_depth=5, min_child_weight=4, n_estimators=502, subsample=0.884459812975207; total time=  54.3s\n",
      "[CV] END colsample_bytree=0.9350840423629312, gamma=0.33784505851964036, learning_rate=0.23056483577223164, max_depth=4, min_child_weight=7, n_estimators=514, subsample=0.9509357413523924; total time=  45.7s\n",
      "[CV] END colsample_bytree=0.7031766510860622, gamma=0.3299920230170895, learning_rate=0.25516666006036476, max_depth=3, min_child_weight=4, n_estimators=671, subsample=0.7043316699321636; total time=  54.4s\n",
      "[CV] END colsample_bytree=0.8801431319891084, gamma=0.42333057111915295, learning_rate=0.26689728756342773, max_depth=8, min_child_weight=8, n_estimators=772, subsample=0.8568126584617151; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.8654007076432223, gamma=0.0025307919231093434, learning_rate=0.0582424154252496, max_depth=4, min_child_weight=2, n_estimators=921, subsample=0.7793696571944989; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8598531596188859, gamma=0.42461170524708897, learning_rate=0.207283867690103, max_depth=3, min_child_weight=3, n_estimators=569, subsample=0.7060809470726902; total time=  48.6s\n",
      "[CV] END colsample_bytree=0.6975958573516334, gamma=0.4865052773762228, learning_rate=0.1279293174000281, max_depth=8, min_child_weight=8, n_estimators=382, subsample=0.7737577462041715; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.8288016796836732, gamma=0.3842770071531545, learning_rate=0.023081131526330123, max_depth=6, min_child_weight=5, n_estimators=617, subsample=0.7480634801021777; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.7177795568278342, gamma=0.19254886430096263, learning_rate=0.26534100145505707, max_depth=5, min_child_weight=2, n_estimators=892, subsample=0.7578765867237889; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.9376852562905246, gamma=0.46500841740541593, learning_rate=0.031124839254863167, max_depth=4, min_child_weight=6, n_estimators=762, subsample=0.6560336060946096; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.8211279907631631, gamma=0.14825507182389924, learning_rate=0.13593425693388295, max_depth=4, min_child_weight=9, n_estimators=507, subsample=0.9652962210225885; total time=  47.8s\n",
      "[CV] END colsample_bytree=0.8045369595443751, gamma=0.2507581473435998, learning_rate=0.24948855369003253, max_depth=3, min_child_weight=8, n_estimators=611, subsample=0.7587135308855554; total time=  52.1s\n",
      "[CV] END colsample_bytree=0.6203074124157587, gamma=0.44330857447532995, learning_rate=0.018285031562111413, max_depth=4, min_child_weight=2, n_estimators=899, subsample=0.8313120563984696; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.7001005442063456, gamma=0.0194173672147116, learning_rate=0.10097965440196684, max_depth=5, min_child_weight=5, n_estimators=363, subsample=0.9079974212394444; total time=  39.4s\n",
      "[CV] END colsample_bytree=0.8549719605992826, gamma=0.3630456668613308, learning_rate=0.30275562383876037, max_depth=6, min_child_weight=3, n_estimators=756, subsample=0.7595283605779178; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.7734082950322968, gamma=0.3720213214995577, learning_rate=0.08525815820399835, max_depth=4, min_child_weight=5, n_estimators=929, subsample=0.9343920482048823; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.8446882984937409, gamma=0.20980003121389496, learning_rate=0.08431929685034723, max_depth=5, min_child_weight=5, n_estimators=185, subsample=0.6057573954519023; total time=  25.5s\n",
      "[CV] END colsample_bytree=0.6464290562027665, gamma=0.023001321010876374, learning_rate=0.02221864069569104, max_depth=8, min_child_weight=4, n_estimators=552, subsample=0.78966953163493; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.9796582926365544, gamma=0.4433401936490238, learning_rate=0.08826808700251418, max_depth=4, min_child_weight=3, n_estimators=541, subsample=0.8004159535661037; total time=  52.3s\n",
      "[CV] END colsample_bytree=0.8545614389784044, gamma=0.40047464734119986, learning_rate=0.21315050271489452, max_depth=5, min_child_weight=1, n_estimators=790, subsample=0.9282557902975821; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.850375868040629, gamma=0.41021342188563403, learning_rate=0.20544543155733638, max_depth=5, min_child_weight=2, n_estimators=352, subsample=0.6377771843023713; total time=  41.6s\n",
      "[CV] END colsample_bytree=0.8732027093665427, gamma=0.035594324230114494, learning_rate=0.10569268908812839, max_depth=4, min_child_weight=2, n_estimators=214, subsample=0.7127419099093599; total time=  26.0s\n",
      "[CV] END colsample_bytree=0.987032220608099, gamma=0.27398594162404366, learning_rate=0.13704128269243557, max_depth=5, min_child_weight=1, n_estimators=198, subsample=0.7650470707645706; total time=  25.4s\n",
      "[CV] END colsample_bytree=0.987032220608099, gamma=0.27398594162404366, learning_rate=0.13704128269243557, max_depth=5, min_child_weight=1, n_estimators=198, subsample=0.7650470707645706; total time=  26.4s\n",
      "[CV] END colsample_bytree=0.749816047538945, gamma=0.4753571532049581, learning_rate=0.22959818254342154, max_depth=7, min_child_weight=5, n_estimators=714, subsample=0.7783331011414365; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.6028265220878869, gamma=0.011531212520707879, learning_rate=0.16743239807751675, max_depth=9, min_child_weight=3, n_estimators=975, subsample=0.8056937753654446; total time= 2.8min\n",
      "[CV] END colsample_bytree=0.9768807022739411, gamma=0.28164410892276964, learning_rate=0.12562495076197483, max_depth=4, min_child_weight=5, n_estimators=997, subsample=0.8736932106048627; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.9369139098379994, gamma=0.22487706668488283, learning_rate=0.12854507080054434, max_depth=6, min_child_weight=8, n_estimators=561, subsample=0.6783931449676581; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.9369139098379994, gamma=0.22487706668488283, learning_rate=0.12854507080054434, max_depth=6, min_child_weight=8, n_estimators=561, subsample=0.6783931449676581; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.7123738038749523, gamma=0.27134804157912423, learning_rate=0.052277267492428794, max_depth=9, min_child_weight=1, n_estimators=956, subsample=0.9947547746402069; total time= 2.7min\n",
      "[CV] END colsample_bytree=0.9160702162124823, gamma=0.3029799873905057, learning_rate=0.2878902635540047, max_depth=4, min_child_weight=9, n_estimators=127, subsample=0.9452413703502374; total time=  14.4s\n",
      "[CV] END colsample_bytree=0.8493192507310232, gamma=0.1654490124263246, learning_rate=0.02906750508580709, max_depth=9, min_child_weight=8, n_estimators=904, subsample=0.8918424713352255; total time= 2.7min\n",
      "[CV] END colsample_bytree=0.6943939678995823, gamma=0.12803416138066198, learning_rate=0.022130076861529402, max_depth=9, min_child_weight=3, n_estimators=674, subsample=0.9583054382694077; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.9022204554172195, gamma=0.11439908274581123, learning_rate=0.0330939729486379, max_depth=5, min_child_weight=3, n_estimators=185, subsample=0.9521871356061031; total time=  30.3s\n",
      "[CV] END colsample_bytree=0.7297380084021096, gamma=0.061043977350336676, learning_rate=0.11688935142309247, max_depth=8, min_child_weight=1, n_estimators=484, subsample=0.6911740650167767; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.7297380084021096, gamma=0.061043977350336676, learning_rate=0.11688935142309247, max_depth=8, min_child_weight=1, n_estimators=484, subsample=0.6911740650167767; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.7939319885435933, gamma=0.34621801644513517, learning_rate=0.09082370013955643, max_depth=9, min_child_weight=6, n_estimators=771, subsample=0.8075162486973464; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.6592347719813599, gamma=0.49887024252447093, learning_rate=0.09003430428258549, max_depth=4, min_child_weight=2, n_estimators=319, subsample=0.6205915004999957; total time=  32.3s\n",
      "[CV] END colsample_bytree=0.8211572356285312, gamma=0.29634836193969677, learning_rate=0.03425599789981457, max_depth=5, min_child_weight=1, n_estimators=866, subsample=0.9212559025519583; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.7881202537784153, gamma=0.4917115704474215, learning_rate=0.12964733273336593, max_depth=9, min_child_weight=1, n_estimators=300, subsample=0.7283120259886944; total time=  54.6s\n",
      "[CV] END colsample_bytree=0.9238004184558861, gamma=0.1743329936458647, learning_rate=0.03885296532742623, max_depth=3, min_child_weight=1, n_estimators=739, subsample=0.8071005402109921; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.9350840423629312, gamma=0.33784505851964036, learning_rate=0.23056483577223164, max_depth=4, min_child_weight=7, n_estimators=514, subsample=0.9509357413523924; total time=  49.7s\n",
      "[CV] END colsample_bytree=0.9985014799031697, gamma=0.4827096756443968, learning_rate=0.1774880360821293, max_depth=5, min_child_weight=7, n_estimators=108, subsample=0.7115485410368727; total time=  15.2s\n",
      "[CV] END colsample_bytree=0.8801431319891084, gamma=0.42333057111915295, learning_rate=0.26689728756342773, max_depth=8, min_child_weight=8, n_estimators=772, subsample=0.8568126584617151; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.9977829850443283, gamma=0.08796262633867269, learning_rate=0.015422609084656261, max_depth=8, min_child_weight=5, n_estimators=915, subsample=0.8985965620472096; total time= 2.5min\n",
      "[CV] END colsample_bytree=0.6975958573516334, gamma=0.4865052773762228, learning_rate=0.1279293174000281, max_depth=8, min_child_weight=8, n_estimators=382, subsample=0.7737577462041715; total time=  59.0s\n",
      "[CV] END colsample_bytree=0.8288016796836732, gamma=0.3842770071531545, learning_rate=0.023081131526330123, max_depth=6, min_child_weight=5, n_estimators=617, subsample=0.7480634801021777; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.606182646611547, gamma=0.4641592812938627, learning_rate=0.1384552444951943, max_depth=7, min_child_weight=3, n_estimators=799, subsample=0.941203782186944; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.9376852562905246, gamma=0.46500841740541593, learning_rate=0.031124839254863167, max_depth=4, min_child_weight=6, n_estimators=762, subsample=0.6560336060946096; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8073318609454947, gamma=0.4386865359639777, learning_rate=0.23223058532626134, max_depth=3, min_child_weight=9, n_estimators=910, subsample=0.8835643987640474; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8045369595443751, gamma=0.2507581473435998, learning_rate=0.24948855369003253, max_depth=3, min_child_weight=8, n_estimators=611, subsample=0.7587135308855554; total time=  49.4s\n",
      "[CV] END colsample_bytree=0.6143769095186968, gamma=0.23279900906623008, learning_rate=0.17279339041227298, max_depth=7, min_child_weight=6, n_estimators=413, subsample=0.9441618473246701; total time=  57.0s\n",
      "[CV] END colsample_bytree=0.7001005442063456, gamma=0.0194173672147116, learning_rate=0.10097965440196684, max_depth=5, min_child_weight=5, n_estimators=363, subsample=0.9079974212394444; total time=  39.6s\n",
      "[CV] END colsample_bytree=0.6863284109987373, gamma=0.31144523790950013, learning_rate=0.0356042394981304, max_depth=7, min_child_weight=4, n_estimators=449, subsample=0.8162540486440426; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.7734082950322968, gamma=0.3720213214995577, learning_rate=0.08525815820399835, max_depth=4, min_child_weight=5, n_estimators=929, subsample=0.9343920482048823; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.8858383690800249, gamma=0.33009868835886563, learning_rate=0.09398016908378284, max_depth=7, min_child_weight=7, n_estimators=803, subsample=0.8217416210045603; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.6391336642604005, gamma=0.24580793755841618, learning_rate=0.15204153123416972, max_depth=3, min_child_weight=5, n_estimators=122, subsample=0.6192235696788134; total time=  14.1s\n",
      "[CV] END colsample_bytree=0.6391336642604005, gamma=0.24580793755841618, learning_rate=0.15204153123416972, max_depth=3, min_child_weight=5, n_estimators=122, subsample=0.6192235696788134; total time=  14.4s\n",
      "[CV] END colsample_bytree=0.9796582926365544, gamma=0.4433401936490238, learning_rate=0.08826808700251418, max_depth=4, min_child_weight=3, n_estimators=541, subsample=0.8004159535661037; total time=  51.4s\n",
      "[CV] END colsample_bytree=0.815750979360025, gamma=0.34198188469907054, learning_rate=0.19475534931697416, max_depth=9, min_child_weight=2, n_estimators=596, subsample=0.9468795734220015; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.850375868040629, gamma=0.41021342188563403, learning_rate=0.20544543155733638, max_depth=5, min_child_weight=2, n_estimators=352, subsample=0.6377771843023713; total time=  42.8s\n",
      "[CV] END colsample_bytree=0.6472659310486625, gamma=0.3483685826820753, learning_rate=0.1986828540339652, max_depth=7, min_child_weight=8, n_estimators=519, subsample=0.7671784126862315; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.7488072343171133, gamma=0.3882064803709984, learning_rate=0.11224106207590534, max_depth=8, min_child_weight=5, n_estimators=879, subsample=0.7715976109500073; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.6399899663272012, gamma=0.22962444598293358, learning_rate=0.11011258334170654, max_depth=5, min_child_weight=6, n_estimators=408, subsample=0.9879639408647978; total time=  42.5s\n",
      "[CV] END colsample_bytree=0.6399899663272012, gamma=0.22962444598293358, learning_rate=0.11011258334170654, max_depth=5, min_child_weight=6, n_estimators=408, subsample=0.9879639408647978; total time=  46.8s\n",
      "[CV] END colsample_bytree=0.6028265220878869, gamma=0.011531212520707879, learning_rate=0.16743239807751675, max_depth=9, min_child_weight=3, n_estimators=975, subsample=0.8056937753654446; total time= 2.7min\n",
      "[CV] END colsample_bytree=0.9768807022739411, gamma=0.28164410892276964, learning_rate=0.12562495076197483, max_depth=4, min_child_weight=5, n_estimators=997, subsample=0.8736932106048627; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.672894435115225, gamma=0.37768070515882624, learning_rate=0.13754676234737342, max_depth=8, min_child_weight=4, n_estimators=921, subsample=0.6125253169822235; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.7123738038749523, gamma=0.27134804157912423, learning_rate=0.052277267492428794, max_depth=9, min_child_weight=1, n_estimators=956, subsample=0.9947547746402069; total time= 2.7min\n",
      "[CV] END colsample_bytree=0.9160702162124823, gamma=0.3029799873905057, learning_rate=0.2878902635540047, max_depth=4, min_child_weight=9, n_estimators=127, subsample=0.9452413703502374; total time=  15.0s\n",
      "[CV] END colsample_bytree=0.9160702162124823, gamma=0.3029799873905057, learning_rate=0.2878902635540047, max_depth=4, min_child_weight=9, n_estimators=127, subsample=0.9452413703502374; total time=  15.3s\n",
      "[CV] END colsample_bytree=0.8493192507310232, gamma=0.1654490124263246, learning_rate=0.02906750508580709, max_depth=9, min_child_weight=8, n_estimators=904, subsample=0.8918424713352255; total time= 2.6min\n",
      "[CV] END colsample_bytree=0.6943939678995823, gamma=0.12803416138066198, learning_rate=0.022130076861529402, max_depth=9, min_child_weight=3, n_estimators=674, subsample=0.9583054382694077; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.8497416192535173, gamma=0.147816842918857, learning_rate=0.041648277949081186, max_depth=6, min_child_weight=4, n_estimators=897, subsample=0.9533121035675474; total time= 3.4min\n",
      "[CV] END colsample_bytree=0.7939319885435933, gamma=0.34621801644513517, learning_rate=0.09082370013955643, max_depth=9, min_child_weight=6, n_estimators=771, subsample=0.8075162486973464; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.7114585856946446, gamma=0.45413294298332685, learning_rate=0.08186856720009172, max_depth=4, min_child_weight=5, n_estimators=783, subsample=0.7791132658292367; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8211572356285312, gamma=0.29634836193969677, learning_rate=0.03425599789981457, max_depth=5, min_child_weight=1, n_estimators=866, subsample=0.9212559025519583; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.6746074041599417, gamma=0.020387570777381958, learning_rate=0.18726788295647254, max_depth=5, min_child_weight=4, n_estimators=502, subsample=0.884459812975207; total time=  53.5s\n",
      "[CV] END colsample_bytree=0.9350840423629312, gamma=0.33784505851964036, learning_rate=0.23056483577223164, max_depth=4, min_child_weight=7, n_estimators=514, subsample=0.9509357413523924; total time=  45.5s\n",
      "[CV] END colsample_bytree=0.9985014799031697, gamma=0.4827096756443968, learning_rate=0.1774880360821293, max_depth=5, min_child_weight=7, n_estimators=108, subsample=0.7115485410368727; total time=  15.6s\n",
      "[CV] END colsample_bytree=0.9985014799031697, gamma=0.4827096756443968, learning_rate=0.1774880360821293, max_depth=5, min_child_weight=7, n_estimators=108, subsample=0.7115485410368727; total time=  15.2s\n",
      "[CV] END colsample_bytree=0.8801431319891084, gamma=0.42333057111915295, learning_rate=0.26689728756342773, max_depth=8, min_child_weight=8, n_estimators=772, subsample=0.8568126584617151; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.9977829850443283, gamma=0.08796262633867269, learning_rate=0.015422609084656261, max_depth=8, min_child_weight=5, n_estimators=915, subsample=0.8985965620472096; total time= 2.5min\n",
      "[CV] END colsample_bytree=0.6975958573516334, gamma=0.4865052773762228, learning_rate=0.1279293174000281, max_depth=8, min_child_weight=8, n_estimators=382, subsample=0.7737577462041715; total time=  57.6s\n",
      "[CV] END colsample_bytree=0.7400313630778703, gamma=0.3225516810152824, learning_rate=0.2106772178989299, max_depth=3, min_child_weight=5, n_estimators=928, subsample=0.7996773519539009; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.606182646611547, gamma=0.4641592812938627, learning_rate=0.1384552444951943, max_depth=7, min_child_weight=3, n_estimators=799, subsample=0.941203782186944; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.9376852562905246, gamma=0.46500841740541593, learning_rate=0.031124839254863167, max_depth=4, min_child_weight=6, n_estimators=762, subsample=0.6560336060946096; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.8073318609454947, gamma=0.4386865359639777, learning_rate=0.23223058532626134, max_depth=3, min_child_weight=9, n_estimators=910, subsample=0.8835643987640474; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8045369595443751, gamma=0.2507581473435998, learning_rate=0.24948855369003253, max_depth=3, min_child_weight=8, n_estimators=611, subsample=0.7587135308855554; total time=  50.6s\n",
      "[CV] END colsample_bytree=0.6203074124157587, gamma=0.44330857447532995, learning_rate=0.018285031562111413, max_depth=4, min_child_weight=2, n_estimators=899, subsample=0.8313120563984696; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.7001005442063456, gamma=0.0194173672147116, learning_rate=0.10097965440196684, max_depth=5, min_child_weight=5, n_estimators=363, subsample=0.9079974212394444; total time=  46.1s\n",
      "[CV] END colsample_bytree=0.8549719605992826, gamma=0.3630456668613308, learning_rate=0.30275562383876037, max_depth=6, min_child_weight=3, n_estimators=756, subsample=0.7595283605779178; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8783896824374792, gamma=0.20447647220713494, learning_rate=0.06198829602125373, max_depth=3, min_child_weight=2, n_estimators=195, subsample=0.8196906658824482; total time=  20.2s\n",
      "[CV] END colsample_bytree=0.8783896824374792, gamma=0.20447647220713494, learning_rate=0.06198829602125373, max_depth=3, min_child_weight=2, n_estimators=195, subsample=0.8196906658824482; total time=  19.5s\n",
      "[CV] END colsample_bytree=0.8858383690800249, gamma=0.33009868835886563, learning_rate=0.09398016908378284, max_depth=7, min_child_weight=7, n_estimators=803, subsample=0.8217416210045603; total time= 1.8min\n",
      "[CV] END colsample_bytree=0.6391336642604005, gamma=0.24580793755841618, learning_rate=0.15204153123416972, max_depth=3, min_child_weight=5, n_estimators=122, subsample=0.6192235696788134; total time=  16.7s\n",
      "[CV] END colsample_bytree=0.9796582926365544, gamma=0.4433401936490238, learning_rate=0.08826808700251418, max_depth=4, min_child_weight=3, n_estimators=541, subsample=0.8004159535661037; total time=  56.0s\n",
      "[CV] END colsample_bytree=0.815750979360025, gamma=0.34198188469907054, learning_rate=0.19475534931697416, max_depth=9, min_child_weight=2, n_estimators=596, subsample=0.9468795734220015; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.8732027093665427, gamma=0.035594324230114494, learning_rate=0.10569268908812839, max_depth=4, min_child_weight=2, n_estimators=214, subsample=0.7127419099093599; total time=  25.0s\n",
      "[CV] END colsample_bytree=0.6472659310486625, gamma=0.3483685826820753, learning_rate=0.1986828540339652, max_depth=7, min_child_weight=8, n_estimators=519, subsample=0.7671784126862315; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.7488072343171133, gamma=0.3882064803709984, learning_rate=0.11224106207590534, max_depth=8, min_child_weight=5, n_estimators=879, subsample=0.7715976109500073; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.7977681218810326, gamma=0.028779380008322142, learning_rate=0.17485866469712066, max_depth=7, min_child_weight=3, n_estimators=314, subsample=0.8183663080659955; total time=  47.2s\n",
      "Best parameters: {'colsample_bytree': np.float64(0.6464290562027665), 'gamma': np.float64(0.023001321010876374), 'learning_rate': np.float64(0.02221864069569104), 'max_depth': 8, 'min_child_weight': 4, 'n_estimators': 552, 'subsample': np.float64(0.78966953163493)}\n",
      "Best ROC AUC score: 0.7643728869432332\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'average_precision_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m test_roc_auc \u001b[38;5;241m=\u001b[39m roc_auc_score(test_labels, test_prob)\n\u001b[1;32m     61\u001b[0m validation_roc_auc \u001b[38;5;241m=\u001b[39m roc_auc_score(validation_labels, validation_prob)\n\u001b[0;32m---> 63\u001b[0m train_pr_auc \u001b[38;5;241m=\u001b[39m \u001b[43maverage_precision_score\u001b[49m(train_labels, train_prob)\n\u001b[1;32m     64\u001b[0m test_pr_auc \u001b[38;5;241m=\u001b[39m average_precision_score(test_labels, test_prob)\n\u001b[1;32m     65\u001b[0m validation_pr_auc \u001b[38;5;241m=\u001b[39m average_precision_score(validation_labels, validation_prob)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'average_precision_score' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8823325935514779, gamma=0.48432597198706745, learning_rate=0.21640900305251173, max_depth=4, min_child_weight=1, n_estimators=352, subsample=0.935392305505594; total time=  34.7s\n",
      "[CV] END colsample_bytree=0.6932912289698111, gamma=0.29065270861339115, learning_rate=0.26894155659670116, max_depth=8, min_child_weight=4, n_estimators=756, subsample=0.9788994309535435; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.9944004255291483, gamma=0.3766890926294708, learning_rate=0.12287787565927472, max_depth=9, min_child_weight=3, n_estimators=544, subsample=0.7511950360489018; total time= 1.8min\n",
      "[CV] END colsample_bytree=0.6950025885169879, gamma=0.20011144643601242, learning_rate=0.1533144672640683, max_depth=3, min_child_weight=8, n_estimators=216, subsample=0.9852890157762445; total time=  22.5s\n",
      "[CV] END colsample_bytree=0.6295186258941595, gamma=0.2769271422006604, learning_rate=0.3007907606857297, max_depth=4, min_child_weight=9, n_estimators=404, subsample=0.8782994755938469; total time=  40.7s\n",
      "[CV] END colsample_bytree=0.7818164259071093, gamma=0.3137790400420317, learning_rate=0.18529429357693009, max_depth=9, min_child_weight=9, n_estimators=111, subsample=0.9834809629631385; total time=  23.3s\n",
      "[CV] END colsample_bytree=0.7854793619759929, gamma=0.1766761140130264, learning_rate=0.18509683355526163, max_depth=6, min_child_weight=5, n_estimators=140, subsample=0.703979945141705; total time=  21.5s\n",
      "[CV] END colsample_bytree=0.7854793619759929, gamma=0.1766761140130264, learning_rate=0.18509683355526163, max_depth=6, min_child_weight=5, n_estimators=140, subsample=0.703979945141705; total time=  20.7s\n",
      "[CV] END colsample_bytree=0.6650467757379565, gamma=0.45546359224692123, learning_rate=0.25676117287695066, max_depth=4, min_child_weight=3, n_estimators=355, subsample=0.8324650484723305; total time=  37.9s\n",
      "[CV] END colsample_bytree=0.6650467757379565, gamma=0.45546359224692123, learning_rate=0.25676117287695066, max_depth=4, min_child_weight=3, n_estimators=355, subsample=0.8324650484723305; total time=  38.5s\n",
      "[CV] END colsample_bytree=0.9110409134940902, gamma=0.24018504093317733, learning_rate=0.3055858151996513, max_depth=5, min_child_weight=5, n_estimators=381, subsample=0.7523563426524086; total time=  48.1s\n",
      "[CV] END colsample_bytree=0.6054687859307989, gamma=0.03767953017623116, learning_rate=0.21751431915067362, max_depth=7, min_child_weight=5, n_estimators=969, subsample=0.9652663009030571; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.6187585871164879, gamma=0.13433624101265368, learning_rate=0.01665542260909049, max_depth=4, min_child_weight=2, n_estimators=537, subsample=0.9089273566942557; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.8080654004447974, gamma=0.42609075015927006, learning_rate=0.17557205163234565, max_depth=7, min_child_weight=9, n_estimators=500, subsample=0.6536060913802563; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.6115130705253355, gamma=0.3775686278368095, learning_rate=0.1960928654060394, max_depth=4, min_child_weight=3, n_estimators=202, subsample=0.6545485902347079; total time=  24.2s\n",
      "[CV] END colsample_bytree=0.6058178662671527, gamma=0.1752937794032985, learning_rate=0.18697530605638993, max_depth=8, min_child_weight=3, n_estimators=546, subsample=0.8905588130600912; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.6183445065618531, gamma=0.31040284221104564, learning_rate=0.11422402246762425, max_depth=8, min_child_weight=3, n_estimators=210, subsample=0.6515518887642596; total time=  38.7s\n",
      "[CV] END colsample_bytree=0.6512183355831089, gamma=0.07595134675614718, learning_rate=0.051648151794823044, max_depth=6, min_child_weight=8, n_estimators=999, subsample=0.7382669133295453; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.850375868040629, gamma=0.41021342188563403, learning_rate=0.20544543155733638, max_depth=5, min_child_weight=2, n_estimators=352, subsample=0.6377771843023713; total time=  41.4s\n",
      "[CV] END colsample_bytree=0.8732027093665427, gamma=0.035594324230114494, learning_rate=0.10569268908812839, max_depth=4, min_child_weight=2, n_estimators=214, subsample=0.7127419099093599; total time=  24.5s\n",
      "[CV] END colsample_bytree=0.987032220608099, gamma=0.27398594162404366, learning_rate=0.13704128269243557, max_depth=5, min_child_weight=1, n_estimators=198, subsample=0.7650470707645706; total time=  25.8s\n",
      "[CV] END colsample_bytree=0.987032220608099, gamma=0.27398594162404366, learning_rate=0.13704128269243557, max_depth=5, min_child_weight=1, n_estimators=198, subsample=0.7650470707645706; total time=  25.6s\n",
      "[CV] END colsample_bytree=0.9003484271165989, gamma=0.3772714370423412, learning_rate=0.040937160650779784, max_depth=3, min_child_weight=1, n_estimators=161, subsample=0.7986244762732311; total time=  19.5s\n",
      "[CV] END colsample_bytree=0.9003484271165989, gamma=0.3772714370423412, learning_rate=0.040937160650779784, max_depth=3, min_child_weight=1, n_estimators=161, subsample=0.7986244762732311; total time=  19.2s\n",
      "[CV] END colsample_bytree=0.7558472344876814, gamma=0.14881758725465544, learning_rate=0.03999546614855901, max_depth=6, min_child_weight=9, n_estimators=175, subsample=0.7419620761850882; total time=  31.0s\n",
      "[CV] END colsample_bytree=0.7558472344876814, gamma=0.14881758725465544, learning_rate=0.03999546614855901, max_depth=6, min_child_weight=9, n_estimators=175, subsample=0.7419620761850882; total time=  29.5s\n",
      "[CV] END colsample_bytree=0.7314658181479664, gamma=0.3362592280385192, learning_rate=0.235712358831304, max_depth=9, min_child_weight=9, n_estimators=138, subsample=0.6364824412194762; total time=  27.7s\n",
      "[CV] END colsample_bytree=0.7977681218810326, gamma=0.028779380008322142, learning_rate=0.17485866469712066, max_depth=7, min_child_weight=3, n_estimators=314, subsample=0.8183663080659955; total time=  48.3s\n",
      "[CV] END colsample_bytree=0.7704365900187764, gamma=0.11128820878551526, learning_rate=0.12899548058627555, max_depth=5, min_child_weight=2, n_estimators=616, subsample=0.8053304674769571; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.6932912289698111, gamma=0.29065270861339115, learning_rate=0.26894155659670116, max_depth=8, min_child_weight=4, n_estimators=756, subsample=0.9788994309535435; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.882033745718664, gamma=0.12436207721514397, learning_rate=0.10907575870572955, max_depth=8, min_child_weight=3, n_estimators=272, subsample=0.6045414579069676; total time=  48.8s\n",
      "[CV] END colsample_bytree=0.7874642567976505, gamma=0.028151637840918675, learning_rate=0.04564537488042158, max_depth=7, min_child_weight=9, n_estimators=100, subsample=0.8724157708387636; total time=  20.5s\n",
      "[CV] END colsample_bytree=0.6048617898759265, gamma=0.4849394133538195, learning_rate=0.022947973585172834, max_depth=7, min_child_weight=6, n_estimators=312, subsample=0.9971859184477201; total time=  53.5s\n",
      "[CV] END colsample_bytree=0.7818164259071093, gamma=0.3137790400420317, learning_rate=0.18529429357693009, max_depth=9, min_child_weight=9, n_estimators=111, subsample=0.9834809629631385; total time=  22.6s\n",
      "[CV] END colsample_bytree=0.6231455591030829, gamma=0.19726089930406493, learning_rate=0.042026819780765313, max_depth=7, min_child_weight=8, n_estimators=221, subsample=0.6752484638895045; total time=  43.5s\n",
      "[CV] END colsample_bytree=0.7812963387315324, gamma=0.016157975554825232, learning_rate=0.0939290521506837, max_depth=7, min_child_weight=4, n_estimators=560, subsample=0.8738924690215517; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.9676706207342238, gamma=0.04137421001765307, learning_rate=0.2729984457777378, max_depth=5, min_child_weight=5, n_estimators=218, subsample=0.7645020466341477; total time=  28.4s\n",
      "[CV] END colsample_bytree=0.9879657591258413, gamma=0.42105946156785434, learning_rate=0.26149861141334135, max_depth=4, min_child_weight=9, n_estimators=198, subsample=0.6535408475202532; total time=  23.5s\n",
      "[CV] END colsample_bytree=0.6054687859307989, gamma=0.03767953017623116, learning_rate=0.21751431915067362, max_depth=7, min_child_weight=5, n_estimators=969, subsample=0.9652663009030571; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.651663766060598, gamma=0.4770255136293612, learning_rate=0.19185239033526402, max_depth=8, min_child_weight=1, n_estimators=980, subsample=0.9027996528688915; total time= 2.5min\n",
      "[CV] END colsample_bytree=0.6115130705253355, gamma=0.3775686278368095, learning_rate=0.1960928654060394, max_depth=4, min_child_weight=3, n_estimators=202, subsample=0.6545485902347079; total time=  23.4s\n",
      "[CV] END colsample_bytree=0.6058178662671527, gamma=0.1752937794032985, learning_rate=0.18697530605638993, max_depth=8, min_child_weight=3, n_estimators=546, subsample=0.8905588130600912; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.9798082494630569, gamma=0.07353674046451897, learning_rate=0.28797628754844834, max_depth=4, min_child_weight=1, n_estimators=607, subsample=0.6264943911407463; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.6512183355831089, gamma=0.07595134675614718, learning_rate=0.051648151794823044, max_depth=6, min_child_weight=8, n_estimators=999, subsample=0.7382669133295453; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.7558472344876814, gamma=0.14881758725465544, learning_rate=0.03999546614855901, max_depth=6, min_child_weight=9, n_estimators=175, subsample=0.7419620761850882; total time=  30.5s\n",
      "[CV] END colsample_bytree=0.7558472344876814, gamma=0.14881758725465544, learning_rate=0.03999546614855901, max_depth=6, min_child_weight=9, n_estimators=175, subsample=0.7419620761850882; total time=  30.7s\n",
      "[CV] END colsample_bytree=0.9827203540505826, gamma=0.33838495221215165, learning_rate=0.15475628489279275, max_depth=4, min_child_weight=5, n_estimators=168, subsample=0.7172843086792258; total time=  20.6s\n",
      "[CV] END colsample_bytree=0.7314658181479664, gamma=0.3362592280385192, learning_rate=0.235712358831304, max_depth=9, min_child_weight=9, n_estimators=138, subsample=0.6364824412194762; total time=  27.8s\n",
      "[CV] END colsample_bytree=0.7977681218810326, gamma=0.028779380008322142, learning_rate=0.17485866469712066, max_depth=7, min_child_weight=3, n_estimators=314, subsample=0.8183663080659955; total time=  47.0s\n",
      "[CV] END colsample_bytree=0.7704365900187764, gamma=0.11128820878551526, learning_rate=0.12899548058627555, max_depth=5, min_child_weight=2, n_estimators=616, subsample=0.8053304674769571; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.9944004255291483, gamma=0.3766890926294708, learning_rate=0.12287787565927472, max_depth=9, min_child_weight=3, n_estimators=544, subsample=0.7511950360489018; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.882033745718664, gamma=0.12436207721514397, learning_rate=0.10907575870572955, max_depth=8, min_child_weight=3, n_estimators=272, subsample=0.6045414579069676; total time=  47.8s\n",
      "[CV] END colsample_bytree=0.6950025885169879, gamma=0.20011144643601242, learning_rate=0.1533144672640683, max_depth=3, min_child_weight=8, n_estimators=216, subsample=0.9852890157762445; total time=  20.7s\n",
      "[CV] END colsample_bytree=0.6048617898759265, gamma=0.4849394133538195, learning_rate=0.022947973585172834, max_depth=7, min_child_weight=6, n_estimators=312, subsample=0.9971859184477201; total time=  54.5s\n",
      "[CV] END colsample_bytree=0.6231455591030829, gamma=0.19726089930406493, learning_rate=0.042026819780765313, max_depth=7, min_child_weight=8, n_estimators=221, subsample=0.6752484638895045; total time=  38.6s\n",
      "[CV] END colsample_bytree=0.7854793619759929, gamma=0.1766761140130264, learning_rate=0.18509683355526163, max_depth=6, min_child_weight=5, n_estimators=140, subsample=0.703979945141705; total time=  24.5s\n",
      "[CV] END colsample_bytree=0.6650467757379565, gamma=0.45546359224692123, learning_rate=0.25676117287695066, max_depth=4, min_child_weight=3, n_estimators=355, subsample=0.8324650484723305; total time=  37.6s\n",
      "[CV] END colsample_bytree=0.6650467757379565, gamma=0.45546359224692123, learning_rate=0.25676117287695066, max_depth=4, min_child_weight=3, n_estimators=355, subsample=0.8324650484723305; total time=  36.3s\n",
      "[CV] END colsample_bytree=0.9110409134940902, gamma=0.24018504093317733, learning_rate=0.3055858151996513, max_depth=5, min_child_weight=5, n_estimators=381, subsample=0.7523563426524086; total time=  44.5s\n",
      "[CV] END colsample_bytree=0.6054687859307989, gamma=0.03767953017623116, learning_rate=0.21751431915067362, max_depth=7, min_child_weight=5, n_estimators=969, subsample=0.9652663009030571; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.651663766060598, gamma=0.4770255136293612, learning_rate=0.19185239033526402, max_depth=8, min_child_weight=1, n_estimators=980, subsample=0.9027996528688915; total time= 2.6min\n",
      "[CV] END colsample_bytree=0.6115130705253355, gamma=0.3775686278368095, learning_rate=0.1960928654060394, max_depth=4, min_child_weight=3, n_estimators=202, subsample=0.6545485902347079; total time=  25.8s\n",
      "[CV] END colsample_bytree=0.8189785227596447, gamma=0.2254552237080522, learning_rate=0.2831413835209819, max_depth=8, min_child_weight=7, n_estimators=205, subsample=0.9449454834986981; total time=  33.9s\n",
      "[CV] END colsample_bytree=0.8189785227596447, gamma=0.2254552237080522, learning_rate=0.2831413835209819, max_depth=8, min_child_weight=7, n_estimators=205, subsample=0.9449454834986981; total time=  34.2s\n",
      "[CV] END colsample_bytree=0.6183445065618531, gamma=0.31040284221104564, learning_rate=0.11422402246762425, max_depth=8, min_child_weight=3, n_estimators=210, subsample=0.6515518887642596; total time=  41.8s\n",
      "[CV] END colsample_bytree=0.6183445065618531, gamma=0.31040284221104564, learning_rate=0.11422402246762425, max_depth=8, min_child_weight=3, n_estimators=210, subsample=0.6515518887642596; total time=  38.1s\n",
      "[CV] END colsample_bytree=0.9587153639624048, gamma=0.23698082013143618, learning_rate=0.21026732155630815, max_depth=4, min_child_weight=3, n_estimators=502, subsample=0.9287162547377796; total time=  49.8s\n",
      "[CV] END colsample_bytree=0.9587153639624048, gamma=0.23698082013143618, learning_rate=0.21026732155630815, max_depth=4, min_child_weight=3, n_estimators=502, subsample=0.9287162547377796; total time=  51.5s\n",
      "[CV] END colsample_bytree=0.7314658181479664, gamma=0.3362592280385192, learning_rate=0.235712358831304, max_depth=9, min_child_weight=9, n_estimators=138, subsample=0.6364824412194762; total time=  31.0s\n",
      "[CV] END colsample_bytree=0.8823325935514779, gamma=0.48432597198706745, learning_rate=0.21640900305251173, max_depth=4, min_child_weight=1, n_estimators=352, subsample=0.935392305505594; total time=  36.0s\n",
      "[CV] END colsample_bytree=0.6932912289698111, gamma=0.29065270861339115, learning_rate=0.26894155659670116, max_depth=8, min_child_weight=4, n_estimators=756, subsample=0.9788994309535435; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.9944004255291483, gamma=0.3766890926294708, learning_rate=0.12287787565927472, max_depth=9, min_child_weight=3, n_estimators=544, subsample=0.7511950360489018; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.7874642567976505, gamma=0.028151637840918675, learning_rate=0.04564537488042158, max_depth=7, min_child_weight=9, n_estimators=100, subsample=0.8724157708387636; total time=  23.7s\n",
      "[CV] END colsample_bytree=0.6048617898759265, gamma=0.4849394133538195, learning_rate=0.022947973585172834, max_depth=7, min_child_weight=6, n_estimators=312, subsample=0.9971859184477201; total time=  55.6s\n",
      "[CV] END colsample_bytree=0.7818164259071093, gamma=0.3137790400420317, learning_rate=0.18529429357693009, max_depth=9, min_child_weight=9, n_estimators=111, subsample=0.9834809629631385; total time=  24.7s\n",
      "[CV] END colsample_bytree=0.6231455591030829, gamma=0.19726089930406493, learning_rate=0.042026819780765313, max_depth=7, min_child_weight=8, n_estimators=221, subsample=0.6752484638895045; total time=  38.1s\n",
      "[CV] END colsample_bytree=0.7812963387315324, gamma=0.016157975554825232, learning_rate=0.0939290521506837, max_depth=7, min_child_weight=4, n_estimators=560, subsample=0.8738924690215517; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.9676706207342238, gamma=0.04137421001765307, learning_rate=0.2729984457777378, max_depth=5, min_child_weight=5, n_estimators=218, subsample=0.7645020466341477; total time=  30.6s\n",
      "[CV] END colsample_bytree=0.9879657591258413, gamma=0.42105946156785434, learning_rate=0.26149861141334135, max_depth=4, min_child_weight=9, n_estimators=198, subsample=0.6535408475202532; total time=  23.0s\n",
      "[CV] END colsample_bytree=0.6054687859307989, gamma=0.03767953017623116, learning_rate=0.21751431915067362, max_depth=7, min_child_weight=5, n_estimators=969, subsample=0.9652663009030571; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.6187585871164879, gamma=0.13433624101265368, learning_rate=0.01665542260909049, max_depth=4, min_child_weight=2, n_estimators=537, subsample=0.9089273566942557; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.6187585871164879, gamma=0.13433624101265368, learning_rate=0.01665542260909049, max_depth=4, min_child_weight=2, n_estimators=537, subsample=0.9089273566942557; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.6115130705253355, gamma=0.3775686278368095, learning_rate=0.1960928654060394, max_depth=4, min_child_weight=3, n_estimators=202, subsample=0.6545485902347079; total time=  25.6s\n",
      "[CV] END colsample_bytree=0.6058178662671527, gamma=0.1752937794032985, learning_rate=0.18697530605638993, max_depth=8, min_child_weight=3, n_estimators=546, subsample=0.8905588130600912; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.6183445065618531, gamma=0.31040284221104564, learning_rate=0.11422402246762425, max_depth=8, min_child_weight=3, n_estimators=210, subsample=0.6515518887642596; total time=  39.0s\n",
      "[CV] END colsample_bytree=0.6183445065618531, gamma=0.31040284221104564, learning_rate=0.11422402246762425, max_depth=8, min_child_weight=3, n_estimators=210, subsample=0.6515518887642596; total time=  38.2s\n",
      "[CV] END colsample_bytree=0.9587153639624048, gamma=0.23698082013143618, learning_rate=0.21026732155630815, max_depth=4, min_child_weight=3, n_estimators=502, subsample=0.9287162547377796; total time=  50.2s\n",
      "[CV] END colsample_bytree=0.7380330511289547, gamma=0.17380960724134176, learning_rate=0.01954140444554954, max_depth=8, min_child_weight=1, n_estimators=744, subsample=0.7843115072130903; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.850375868040629, gamma=0.41021342188563403, learning_rate=0.20544543155733638, max_depth=5, min_child_weight=2, n_estimators=352, subsample=0.6377771843023713; total time=  41.8s\n",
      "[CV] END colsample_bytree=0.8732027093665427, gamma=0.035594324230114494, learning_rate=0.10569268908812839, max_depth=4, min_child_weight=2, n_estimators=214, subsample=0.7127419099093599; total time=  27.6s\n",
      "[CV] END colsample_bytree=0.6472659310486625, gamma=0.3483685826820753, learning_rate=0.1986828540339652, max_depth=7, min_child_weight=8, n_estimators=519, subsample=0.7671784126862315; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.9003484271165989, gamma=0.3772714370423412, learning_rate=0.040937160650779784, max_depth=3, min_child_weight=1, n_estimators=161, subsample=0.7986244762732311; total time=  19.2s\n",
      "[CV] END colsample_bytree=0.9003484271165989, gamma=0.3772714370423412, learning_rate=0.040937160650779784, max_depth=3, min_child_weight=1, n_estimators=161, subsample=0.7986244762732311; total time=  20.0s\n",
      "[CV] END colsample_bytree=0.7558472344876814, gamma=0.14881758725465544, learning_rate=0.03999546614855901, max_depth=6, min_child_weight=9, n_estimators=175, subsample=0.7419620761850882; total time=  30.4s\n",
      "[CV] END colsample_bytree=0.9827203540505826, gamma=0.33838495221215165, learning_rate=0.15475628489279275, max_depth=4, min_child_weight=5, n_estimators=168, subsample=0.7172843086792258; total time=  21.2s\n",
      "[CV] END colsample_bytree=0.9827203540505826, gamma=0.33838495221215165, learning_rate=0.15475628489279275, max_depth=4, min_child_weight=5, n_estimators=168, subsample=0.7172843086792258; total time=  20.5s\n",
      "[CV] END colsample_bytree=0.7314658181479664, gamma=0.3362592280385192, learning_rate=0.235712358831304, max_depth=9, min_child_weight=9, n_estimators=138, subsample=0.6364824412194762; total time=  29.7s\n",
      "[CV] END colsample_bytree=0.8823325935514779, gamma=0.48432597198706745, learning_rate=0.21640900305251173, max_depth=4, min_child_weight=1, n_estimators=352, subsample=0.935392305505594; total time=  35.4s\n",
      "[CV] END colsample_bytree=0.7704365900187764, gamma=0.11128820878551526, learning_rate=0.12899548058627555, max_depth=5, min_child_weight=2, n_estimators=616, subsample=0.8053304674769571; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.9944004255291483, gamma=0.3766890926294708, learning_rate=0.12287787565927472, max_depth=9, min_child_weight=3, n_estimators=544, subsample=0.7511950360489018; total time= 2.6min\n",
      "[CV] END colsample_bytree=0.6950025885169879, gamma=0.20011144643601242, learning_rate=0.1533144672640683, max_depth=3, min_child_weight=8, n_estimators=216, subsample=0.9852890157762445; total time=  20.7s\n",
      "[CV] END colsample_bytree=0.6048617898759265, gamma=0.4849394133538195, learning_rate=0.022947973585172834, max_depth=7, min_child_weight=6, n_estimators=312, subsample=0.9971859184477201; total time=  58.3s\n",
      "[CV] END colsample_bytree=0.7818164259071093, gamma=0.3137790400420317, learning_rate=0.18529429357693009, max_depth=9, min_child_weight=9, n_estimators=111, subsample=0.9834809629631385; total time=  23.7s\n",
      "[CV] END colsample_bytree=0.7854793619759929, gamma=0.1766761140130264, learning_rate=0.18509683355526163, max_depth=6, min_child_weight=5, n_estimators=140, subsample=0.703979945141705; total time=  21.2s\n",
      "[CV] END colsample_bytree=0.7812963387315324, gamma=0.016157975554825232, learning_rate=0.0939290521506837, max_depth=7, min_child_weight=4, n_estimators=560, subsample=0.8738924690215517; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.9676706207342238, gamma=0.04137421001765307, learning_rate=0.2729984457777378, max_depth=5, min_child_weight=5, n_estimators=218, subsample=0.7645020466341477; total time=  27.2s\n",
      "[CV] END colsample_bytree=0.9110409134940902, gamma=0.24018504093317733, learning_rate=0.3055858151996513, max_depth=5, min_child_weight=5, n_estimators=381, subsample=0.7523563426524086; total time=  45.3s\n",
      "[CV] END colsample_bytree=0.8340598129301888, gamma=0.36306042203592404, learning_rate=0.23712436061273676, max_depth=5, min_child_weight=1, n_estimators=184, subsample=0.7802176541240374; total time=  26.8s\n",
      "[CV] END colsample_bytree=0.8340598129301888, gamma=0.36306042203592404, learning_rate=0.23712436061273676, max_depth=5, min_child_weight=1, n_estimators=184, subsample=0.7802176541240374; total time=  42.3s\n",
      "[CV] END colsample_bytree=0.651663766060598, gamma=0.4770255136293612, learning_rate=0.19185239033526402, max_depth=8, min_child_weight=1, n_estimators=980, subsample=0.9027996528688915; total time= 2.5min\n",
      "[CV] END colsample_bytree=0.8080654004447974, gamma=0.42609075015927006, learning_rate=0.17557205163234565, max_depth=7, min_child_weight=9, n_estimators=500, subsample=0.6536060913802563; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8189785227596447, gamma=0.2254552237080522, learning_rate=0.2831413835209819, max_depth=8, min_child_weight=7, n_estimators=205, subsample=0.9449454834986981; total time=  35.2s\n",
      "[CV] END colsample_bytree=0.8189785227596447, gamma=0.2254552237080522, learning_rate=0.2831413835209819, max_depth=8, min_child_weight=7, n_estimators=205, subsample=0.9449454834986981; total time=  34.7s\n",
      "[CV] END colsample_bytree=0.9798082494630569, gamma=0.07353674046451897, learning_rate=0.28797628754844834, max_depth=4, min_child_weight=1, n_estimators=607, subsample=0.6264943911407463; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.9587153639624048, gamma=0.23698082013143618, learning_rate=0.21026732155630815, max_depth=4, min_child_weight=3, n_estimators=502, subsample=0.9287162547377796; total time=  50.6s\n",
      "[CV] END colsample_bytree=0.9587153639624048, gamma=0.23698082013143618, learning_rate=0.21026732155630815, max_depth=4, min_child_weight=3, n_estimators=502, subsample=0.9287162547377796; total time=  49.0s\n",
      "[CV] END colsample_bytree=0.7380330511289547, gamma=0.17380960724134176, learning_rate=0.01954140444554954, max_depth=8, min_child_weight=1, n_estimators=744, subsample=0.7843115072130903; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.7488072343171133, gamma=0.3882064803709984, learning_rate=0.11224106207590534, max_depth=8, min_child_weight=5, n_estimators=879, subsample=0.7715976109500073; total time= 2.3min\n",
      "[CV] END colsample_bytree=0.9827203540505826, gamma=0.33838495221215165, learning_rate=0.15475628489279275, max_depth=4, min_child_weight=5, n_estimators=168, subsample=0.7172843086792258; total time=  21.4s\n",
      "[CV] END colsample_bytree=0.7977681218810326, gamma=0.028779380008322142, learning_rate=0.17485866469712066, max_depth=7, min_child_weight=3, n_estimators=314, subsample=0.8183663080659955; total time=  49.2s\n",
      "[CV] END colsample_bytree=0.8823325935514779, gamma=0.48432597198706745, learning_rate=0.21640900305251173, max_depth=4, min_child_weight=1, n_estimators=352, subsample=0.935392305505594; total time=  37.5s\n",
      "[CV] END colsample_bytree=0.6932912289698111, gamma=0.29065270861339115, learning_rate=0.26894155659670116, max_depth=8, min_child_weight=4, n_estimators=756, subsample=0.9788994309535435; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.882033745718664, gamma=0.12436207721514397, learning_rate=0.10907575870572955, max_depth=8, min_child_weight=3, n_estimators=272, subsample=0.6045414579069676; total time=  47.4s\n",
      "[CV] END colsample_bytree=0.7874642567976505, gamma=0.028151637840918675, learning_rate=0.04564537488042158, max_depth=7, min_child_weight=9, n_estimators=100, subsample=0.8724157708387636; total time=  25.5s\n",
      "[CV] END colsample_bytree=0.7874642567976505, gamma=0.028151637840918675, learning_rate=0.04564537488042158, max_depth=7, min_child_weight=9, n_estimators=100, subsample=0.8724157708387636; total time=  23.3s\n",
      "[CV] END colsample_bytree=0.6048617898759265, gamma=0.4849394133538195, learning_rate=0.022947973585172834, max_depth=7, min_child_weight=6, n_estimators=312, subsample=0.9971859184477201; total time=  54.7s\n",
      "[CV] END colsample_bytree=0.6295186258941595, gamma=0.2769271422006604, learning_rate=0.3007907606857297, max_depth=4, min_child_weight=9, n_estimators=404, subsample=0.8782994755938469; total time=  41.9s\n",
      "[CV] END colsample_bytree=0.7854793619759929, gamma=0.1766761140130264, learning_rate=0.18509683355526163, max_depth=6, min_child_weight=5, n_estimators=140, subsample=0.703979945141705; total time=  36.0s\n",
      "[CV] END colsample_bytree=0.7812963387315324, gamma=0.016157975554825232, learning_rate=0.0939290521506837, max_depth=7, min_child_weight=4, n_estimators=560, subsample=0.8738924690215517; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.9110409134940902, gamma=0.24018504093317733, learning_rate=0.3055858151996513, max_depth=5, min_child_weight=5, n_estimators=381, subsample=0.7523563426524086; total time=  44.7s\n",
      "[CV] END colsample_bytree=0.9879657591258413, gamma=0.42105946156785434, learning_rate=0.26149861141334135, max_depth=4, min_child_weight=9, n_estimators=198, subsample=0.6535408475202532; total time=  22.7s\n",
      "[CV] END colsample_bytree=0.8340598129301888, gamma=0.36306042203592404, learning_rate=0.23712436061273676, max_depth=5, min_child_weight=1, n_estimators=184, subsample=0.7802176541240374; total time=  24.7s\n",
      "[CV] END colsample_bytree=0.651663766060598, gamma=0.4770255136293612, learning_rate=0.19185239033526402, max_depth=8, min_child_weight=1, n_estimators=980, subsample=0.9027996528688915; total time= 2.7min\n",
      "[CV] END colsample_bytree=0.8080654004447974, gamma=0.42609075015927006, learning_rate=0.17557205163234565, max_depth=7, min_child_weight=9, n_estimators=500, subsample=0.6536060913802563; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.6058178662671527, gamma=0.1752937794032985, learning_rate=0.18697530605638993, max_depth=8, min_child_weight=3, n_estimators=546, subsample=0.8905588130600912; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.9798082494630569, gamma=0.07353674046451897, learning_rate=0.28797628754844834, max_depth=4, min_child_weight=1, n_estimators=607, subsample=0.6264943911407463; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.6512183355831089, gamma=0.07595134675614718, learning_rate=0.051648151794823044, max_depth=6, min_child_weight=8, n_estimators=999, subsample=0.7382669133295453; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.7380330511289547, gamma=0.17380960724134176, learning_rate=0.01954140444554954, max_depth=8, min_child_weight=1, n_estimators=744, subsample=0.7843115072130903; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.8858383690800249, gamma=0.33009868835886563, learning_rate=0.09398016908378284, max_depth=7, min_child_weight=7, n_estimators=803, subsample=0.8217416210045603; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.6464290562027665, gamma=0.023001321010876374, learning_rate=0.02221864069569104, max_depth=8, min_child_weight=4, n_estimators=552, subsample=0.78966953163493; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.815750979360025, gamma=0.34198188469907054, learning_rate=0.19475534931697416, max_depth=9, min_child_weight=2, n_estimators=596, subsample=0.9468795734220015; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8545614389784044, gamma=0.40047464734119986, learning_rate=0.21315050271489452, max_depth=5, min_child_weight=1, n_estimators=790, subsample=0.9282557902975821; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.6472659310486625, gamma=0.3483685826820753, learning_rate=0.1986828540339652, max_depth=7, min_child_weight=8, n_estimators=519, subsample=0.7671784126862315; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.7488072343171133, gamma=0.3882064803709984, learning_rate=0.11224106207590534, max_depth=8, min_child_weight=5, n_estimators=879, subsample=0.7715976109500073; total time= 2.1min\n",
      "[CV] END colsample_bytree=0.9827203540505826, gamma=0.33838495221215165, learning_rate=0.15475628489279275, max_depth=4, min_child_weight=5, n_estimators=168, subsample=0.7172843086792258; total time=  18.8s\n",
      "[CV] END colsample_bytree=0.7314658181479664, gamma=0.3362592280385192, learning_rate=0.235712358831304, max_depth=9, min_child_weight=9, n_estimators=138, subsample=0.6364824412194762; total time=  26.5s\n",
      "[CV] END colsample_bytree=0.8823325935514779, gamma=0.48432597198706745, learning_rate=0.21640900305251173, max_depth=4, min_child_weight=1, n_estimators=352, subsample=0.935392305505594; total time=  31.9s\n",
      "[CV] END colsample_bytree=0.7704365900187764, gamma=0.11128820878551526, learning_rate=0.12899548058627555, max_depth=5, min_child_weight=2, n_estimators=616, subsample=0.8053304674769571; total time=  59.8s\n",
      "[CV] END colsample_bytree=0.9944004255291483, gamma=0.3766890926294708, learning_rate=0.12287787565927472, max_depth=9, min_child_weight=3, n_estimators=544, subsample=0.7511950360489018; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.882033745718664, gamma=0.12436207721514397, learning_rate=0.10907575870572955, max_depth=8, min_child_weight=3, n_estimators=272, subsample=0.6045414579069676; total time=  42.3s\n",
      "[CV] END colsample_bytree=0.7874642567976505, gamma=0.028151637840918675, learning_rate=0.04564537488042158, max_depth=7, min_child_weight=9, n_estimators=100, subsample=0.8724157708387636; total time=  20.9s\n",
      "[CV] END colsample_bytree=0.6950025885169879, gamma=0.20011144643601242, learning_rate=0.1533144672640683, max_depth=3, min_child_weight=8, n_estimators=216, subsample=0.9852890157762445; total time=  19.2s\n",
      "[CV] END colsample_bytree=0.6295186258941595, gamma=0.2769271422006604, learning_rate=0.3007907606857297, max_depth=4, min_child_weight=9, n_estimators=404, subsample=0.8782994755938469; total time=  36.0s\n",
      "[CV] END colsample_bytree=0.6295186258941595, gamma=0.2769271422006604, learning_rate=0.3007907606857297, max_depth=4, min_child_weight=9, n_estimators=404, subsample=0.8782994755938469; total time=  35.8s\n",
      "[CV] END colsample_bytree=0.6231455591030829, gamma=0.19726089930406493, learning_rate=0.042026819780765313, max_depth=7, min_child_weight=8, n_estimators=221, subsample=0.6752484638895045; total time=  57.6s\n",
      "[CV] END colsample_bytree=0.6650467757379565, gamma=0.45546359224692123, learning_rate=0.25676117287695066, max_depth=4, min_child_weight=3, n_estimators=355, subsample=0.8324650484723305; total time=  33.7s\n",
      "[CV] END colsample_bytree=0.9676706207342238, gamma=0.04137421001765307, learning_rate=0.2729984457777378, max_depth=5, min_child_weight=5, n_estimators=218, subsample=0.7645020466341477; total time=  25.2s\n",
      "[CV] END colsample_bytree=0.9110409134940902, gamma=0.24018504093317733, learning_rate=0.3055858151996513, max_depth=5, min_child_weight=5, n_estimators=381, subsample=0.7523563426524086; total time=  38.6s\n",
      "[CV] END colsample_bytree=0.9879657591258413, gamma=0.42105946156785434, learning_rate=0.26149861141334135, max_depth=4, min_child_weight=9, n_estimators=198, subsample=0.6535408475202532; total time=  20.8s\n",
      "[CV] END colsample_bytree=0.8340598129301888, gamma=0.36306042203592404, learning_rate=0.23712436061273676, max_depth=5, min_child_weight=1, n_estimators=184, subsample=0.7802176541240374; total time=  23.3s\n",
      "[CV] END colsample_bytree=0.8340598129301888, gamma=0.36306042203592404, learning_rate=0.23712436061273676, max_depth=5, min_child_weight=1, n_estimators=184, subsample=0.7802176541240374; total time=  24.3s\n",
      "[CV] END colsample_bytree=0.651663766060598, gamma=0.4770255136293612, learning_rate=0.19185239033526402, max_depth=8, min_child_weight=1, n_estimators=980, subsample=0.9027996528688915; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.8080654004447974, gamma=0.42609075015927006, learning_rate=0.17557205163234565, max_depth=7, min_child_weight=9, n_estimators=500, subsample=0.6536060913802563; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.6115130705253355, gamma=0.3775686278368095, learning_rate=0.1960928654060394, max_depth=4, min_child_weight=3, n_estimators=202, subsample=0.6545485902347079; total time=  23.0s\n",
      "[CV] END colsample_bytree=0.6058178662671527, gamma=0.1752937794032985, learning_rate=0.18697530605638993, max_depth=8, min_child_weight=3, n_estimators=546, subsample=0.8905588130600912; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.9798082494630569, gamma=0.07353674046451897, learning_rate=0.28797628754844834, max_depth=4, min_child_weight=1, n_estimators=607, subsample=0.6264943911407463; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.6512183355831089, gamma=0.07595134675614718, learning_rate=0.051648151794823044, max_depth=6, min_child_weight=8, n_estimators=999, subsample=0.7382669133295453; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.7380330511289547, gamma=0.17380960724134176, learning_rate=0.01954140444554954, max_depth=8, min_child_weight=1, n_estimators=744, subsample=0.7843115072130903; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.7488072343171133, gamma=0.3882064803709984, learning_rate=0.11224106207590534, max_depth=8, min_child_weight=5, n_estimators=879, subsample=0.7715976109500073; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.7977681218810326, gamma=0.028779380008322142, learning_rate=0.17485866469712066, max_depth=7, min_child_weight=3, n_estimators=314, subsample=0.8183663080659955; total time=  48.7s\n",
      "[CV] END colsample_bytree=0.7704365900187764, gamma=0.11128820878551526, learning_rate=0.12899548058627555, max_depth=5, min_child_weight=2, n_estimators=616, subsample=0.8053304674769571; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.6932912289698111, gamma=0.29065270861339115, learning_rate=0.26894155659670116, max_depth=8, min_child_weight=4, n_estimators=756, subsample=0.9788994309535435; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.882033745718664, gamma=0.12436207721514397, learning_rate=0.10907575870572955, max_depth=8, min_child_weight=3, n_estimators=272, subsample=0.6045414579069676; total time=  51.8s\n",
      "[CV] END colsample_bytree=0.6950025885169879, gamma=0.20011144643601242, learning_rate=0.1533144672640683, max_depth=3, min_child_weight=8, n_estimators=216, subsample=0.9852890157762445; total time=  20.9s\n",
      "[CV] END colsample_bytree=0.6295186258941595, gamma=0.2769271422006604, learning_rate=0.3007907606857297, max_depth=4, min_child_weight=9, n_estimators=404, subsample=0.8782994755938469; total time=  43.2s\n",
      "[CV] END colsample_bytree=0.7818164259071093, gamma=0.3137790400420317, learning_rate=0.18529429357693009, max_depth=9, min_child_weight=9, n_estimators=111, subsample=0.9834809629631385; total time=  22.7s\n",
      "[CV] END colsample_bytree=0.6231455591030829, gamma=0.19726089930406493, learning_rate=0.042026819780765313, max_depth=7, min_child_weight=8, n_estimators=221, subsample=0.6752484638895045; total time=  38.1s\n",
      "[CV] END colsample_bytree=0.7812963387315324, gamma=0.016157975554825232, learning_rate=0.0939290521506837, max_depth=7, min_child_weight=4, n_estimators=560, subsample=0.8738924690215517; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.9676706207342238, gamma=0.04137421001765307, learning_rate=0.2729984457777378, max_depth=5, min_child_weight=5, n_estimators=218, subsample=0.7645020466341477; total time=  27.5s\n",
      "[CV] END colsample_bytree=0.9879657591258413, gamma=0.42105946156785434, learning_rate=0.26149861141334135, max_depth=4, min_child_weight=9, n_estimators=198, subsample=0.6535408475202532; total time=  28.1s\n",
      "[CV] END colsample_bytree=0.6054687859307989, gamma=0.03767953017623116, learning_rate=0.21751431915067362, max_depth=7, min_child_weight=5, n_estimators=969, subsample=0.9652663009030571; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.6187585871164879, gamma=0.13433624101265368, learning_rate=0.01665542260909049, max_depth=4, min_child_weight=2, n_estimators=537, subsample=0.9089273566942557; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.6187585871164879, gamma=0.13433624101265368, learning_rate=0.01665542260909049, max_depth=4, min_child_weight=2, n_estimators=537, subsample=0.9089273566942557; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8080654004447974, gamma=0.42609075015927006, learning_rate=0.17557205163234565, max_depth=7, min_child_weight=9, n_estimators=500, subsample=0.6536060913802563; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8189785227596447, gamma=0.2254552237080522, learning_rate=0.2831413835209819, max_depth=8, min_child_weight=7, n_estimators=205, subsample=0.9449454834986981; total time=  35.8s\n",
      "[CV] END colsample_bytree=0.9798082494630569, gamma=0.07353674046451897, learning_rate=0.28797628754844834, max_depth=4, min_child_weight=1, n_estimators=607, subsample=0.6264943911407463; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.6512183355831089, gamma=0.07595134675614718, learning_rate=0.051648151794823044, max_depth=6, min_child_weight=8, n_estimators=999, subsample=0.7382669133295453; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.7380330511289547, gamma=0.17380960724134176, learning_rate=0.01954140444554954, max_depth=8, min_child_weight=1, n_estimators=744, subsample=0.7843115072130903; total time= 1.9min\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter search\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': uniform(0, 0.5),\n",
    "    'min_child_weight': randint(1, 10)\n",
    "}\n",
    "\n",
    "# Create the base model\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,  # number of parameter settings that are sampled\n",
    "    scoring='roc_auc',  # could also use 'accuracy' or 'average_precision'\n",
    "    cv=5,  # number of folds for cross-validation\n",
    "    verbose=2,\n",
    "    random_state=RANDOM,\n",
    "    n_jobs=-1  # use all available cores\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(train_features, train_labels)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best ROC AUC score:\", random_search.best_score_)\n",
    "\n",
    "# Use the best estimator for predictions\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = best_xgb.predict(train_features)\n",
    "test_predictions = best_xgb.predict(np.array([x[1:-1] for x in test_flat]))\n",
    "validation_predictions = best_xgb.predict(np.array([x[1:-1] for x in validation_flat]))\n",
    "\n",
    "# Get probabilities for ROC AUC and PR AUC\n",
    "train_prob = best_xgb.predict_proba(train_features)[:, 1]\n",
    "test_prob = best_xgb.predict_proba(np.array([x[1:-1] for x in test_flat]))[:, 1]\n",
    "validation_prob = best_xgb.predict_proba(np.array([x[1:-1] for x in validation_flat]))[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
    "validation_accuracy = accuracy_score(validation_labels, validation_predictions)\n",
    "\n",
    "train_roc_auc = roc_auc_score(train_labels, train_prob)\n",
    "test_roc_auc = roc_auc_score(test_labels, test_prob)\n",
    "validation_roc_auc = roc_auc_score(validation_labels, validation_prob)\n",
    "\n",
    "train_pr_auc = average_precision_score(train_labels, train_prob)\n",
    "test_pr_auc = average_precision_score(test_labels, test_prob)\n",
    "validation_pr_auc = average_precision_score(validation_labels, validation_prob)\n",
    "\n",
    "# Print results\n",
    "print(f\"Train accuracy: {train_accuracy:.3f}, ROC AUC: {train_roc_auc:.2f}, PR AUC: {train_pr_auc:.2f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.3f}, ROC AUC: {test_roc_auc:.2f}, PR AUC: {test_pr_auc:.2f}\")\n",
    "print(f\"Validation accuracy: {validation_accuracy:.3f}, ROC AUC: {validation_roc_auc:.2f}, PR AUC: {validation_pr_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/horse/ws/jori152b-medinf/KP_MedInf/venv/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [10:40:39] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "now = datetime.now()\n",
    "os.makedirs(f'data/models/{now.strftime(\"%d-%m-%Y_%H-%M-%S\")}', exist_ok=True)\n",
    "# save the xgb model\n",
    "xgb.save_model(f'data/models/{now.strftime(\"%d-%m-%Y_%H-%M-%S\")}/xgboost_model_1H.model')\n",
    "# save normalization parameters\n",
    "try:\n",
    "    np.save(f'data/models/{now.strftime(\"%d-%m-%Y_%H-%M-%S\")}/normalization_parameters.npy', normalization_parameters)\n",
    "except:\n",
    "    pass\n",
    "# save the train feature names\n",
    "np.save(f'data/models/{now.strftime(\"%d-%m-%Y_%H-%M-%S\")}/train_feature_names.npy', X.columns[2:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model and predict    \n",
    "loaded_model = XGBClassifier()\n",
    "loaded_model.load_model('data/models/simple_xgboost_model.model')\n",
    "test_predictions = loaded_model.predict(np.array([x[1:-1] for x in test_flat]))\n",
    "validation_predictions = loaded_model.predict(np.array([x[1:-1] for x in validation_flat]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aki_stage\n",
      "0.0    84.000491\n",
      "1.0    15.999509\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"data/preprocessed/preprocessed_data_extended.csv\")\n",
    "# show aki_stage class distribution in percent\n",
    "print(train_data['aki_stage'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selector              Feature         Score\n",
      "27         spo2_mean  59843.790421\n",
      "14    heartrate_mean  58049.728082\n",
      "25     resprate_mean  55454.730523\n",
      "29        tempc_mean  53779.776276\n",
      "28        sysbp_mean  48227.848194\n",
      "19       meanbp_mean  47804.803725\n",
      "37              vent  44955.401562\n",
      "11       diasbp_mean  44674.851004\n",
      "9              creat  40412.686863\n",
      "36       vasopressor  35933.347885\n",
      "35          sedative  26854.445178\n",
      "38           hadm_id  26840.017439\n",
      "10   creatinine_mean  24543.207428\n",
      "42      weight_first  17320.669042\n",
      "18      lactate_mean  16095.448269\n",
      "5           bun_mean  13121.784340\n",
      "43      height_first  12806.395103\n",
      "22    potassium_mean  10860.783600\n",
      "1      aniongap_mean   9634.563239\n",
      "12    glucose_mean_x   8745.174495\n",
      "6            calcium   8436.920982\n",
      "20    phosphate_mean   7889.629907\n",
      "23           pt_mean   6853.308971\n",
      "26       sodium_mean   6200.664806\n",
      "17          inr_mean   6170.761675\n",
      "24          ptt_mean   6145.036507\n",
      "8      chloride_mean   5766.607316\n",
      "15   hematocrit_mean   5707.149887\n",
      "16   hemoglobin_mean   4324.031933\n",
      "34          wbc_mean   3857.772293\n",
      "44           inr_max   3642.863123\n",
      "7       calcium_mean   3312.413770\n",
      "3   bicarbonate_mean   2912.080725\n",
      "4     bilirubin_mean   2745.929584\n",
      "39     admission_age   1382.947984\n",
      "0       albumin_mean    764.787943\n",
      "2         bands_mean    497.019308\n",
      "13    glucose_mean_y    415.501305\n",
      "31        uo_rt_24hr    109.313474\n",
      "30        uo_rt_12hr     63.337600\n",
      "21     platelet_mean     55.838748\n",
      "40          gender_F     33.245100\n",
      "41          gender_M     33.245100\n",
      "33    uric_acid_mean     13.974893\n",
      "32         uo_rt_6hr     13.127856\n",
      "XGB              Feature     Score\n",
      "32         uo_rt_6hr  0.180145\n",
      "14    heartrate_mean  0.119874\n",
      "25     resprate_mean  0.090552\n",
      "31        uo_rt_24hr  0.090375\n",
      "9              creat  0.057706\n",
      "6            calcium  0.051786\n",
      "36       vasopressor  0.044691\n",
      "37              vent  0.039054\n",
      "10   creatinine_mean  0.036132\n",
      "30        uo_rt_12hr  0.035008\n",
      "42      weight_first  0.017246\n",
      "18      lactate_mean  0.014483\n",
      "43      height_first  0.013406\n",
      "39     admission_age  0.012191\n",
      "40          gender_F  0.012148\n",
      "20    phosphate_mean  0.010877\n",
      "44           inr_max  0.010779\n",
      "1      aniongap_mean  0.010533\n",
      "35          sedative  0.008874\n",
      "29        tempc_mean  0.008739\n",
      "15   hematocrit_mean  0.008244\n",
      "24          ptt_mean  0.008161\n",
      "13    glucose_mean_y  0.007823\n",
      "28        sysbp_mean  0.007533\n",
      "19       meanbp_mean  0.007368\n",
      "8      chloride_mean  0.007348\n",
      "4     bilirubin_mean  0.007327\n",
      "21     platelet_mean  0.007233\n",
      "5           bun_mean  0.006360\n",
      "3   bicarbonate_mean  0.006319\n",
      "7       calcium_mean  0.006269\n",
      "38           hadm_id  0.006253\n",
      "22    potassium_mean  0.005434\n",
      "27         spo2_mean  0.004738\n",
      "26       sodium_mean  0.004738\n",
      "23           pt_mean  0.004611\n",
      "16   hemoglobin_mean  0.004280\n",
      "11       diasbp_mean  0.004226\n",
      "0       albumin_mean  0.004034\n",
      "17          inr_mean  0.003924\n",
      "34          wbc_mean  0.003789\n",
      "33    uric_acid_mean  0.003366\n",
      "2         bands_mean  0.003159\n",
      "12    glucose_mean_x  0.002864\n",
      "41          gender_M  0.000000\n",
      "            Feature     Score\n",
      "32        uo_rt_6hr  0.180145\n",
      "14   heartrate_mean  0.119874\n",
      "25    resprate_mean  0.090552\n",
      "31       uo_rt_24hr  0.090375\n",
      "9             creat  0.057706\n",
      "6           calcium  0.051786\n",
      "36      vasopressor  0.044691\n",
      "37             vent  0.039054\n",
      "10  creatinine_mean  0.036132\n",
      "30       uo_rt_12hr  0.035008\n",
      "42     weight_first  0.017246\n",
      "18     lactate_mean  0.014483\n",
      "43     height_first  0.013406\n",
      "39    admission_age  0.012191\n",
      "40         gender_F  0.012148\n",
      "20   phosphate_mean  0.010877\n",
      "44          inr_max  0.010779\n",
      "1     aniongap_mean  0.010533\n",
      "35         sedative  0.008874\n",
      "29       tempc_mean  0.008739\n",
      "Features: 5, Val ROC AUC: 0.7181, Val PR AUC: 0.4278, Val Accuracy: 0.8559, Val F1: 0.2940\n",
      "Features: 10, Val ROC AUC: 0.7341, Val PR AUC: 0.4461, Val Accuracy: 0.8568, Val F1: 0.3058\n",
      "Features: 15, Val ROC AUC: 0.7811, Val PR AUC: 0.4950, Val Accuracy: 0.8601, Val F1: 0.3359\n",
      "Features: 20, Val ROC AUC: 0.7870, Val PR AUC: 0.5017, Val Accuracy: 0.8608, Val F1: 0.3430\n",
      "Features: 25, Val ROC AUC: 0.7840, Val PR AUC: 0.4993, Val Accuracy: 0.8609, Val F1: 0.3460\n",
      "Features: 30, Val ROC AUC: 0.7837, Val PR AUC: 0.5010, Val Accuracy: 0.8609, Val F1: 0.3488\n",
      "Features: 35, Val ROC AUC: 0.7852, Val PR AUC: 0.5032, Val Accuracy: 0.8612, Val F1: 0.3513\n",
      "Features: 40, Val ROC AUC: 0.7848, Val PR AUC: 0.5037, Val Accuracy: 0.8617, Val F1: 0.3514\n",
      "Features: 45, Val ROC AUC: 0.7843, Val PR AUC: 0.5026, Val Accuracy: 0.8613, Val F1: 0.3510\n"
     ]
    }
   ],
   "source": [
    "# feature selection\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(\"data/preprocessed/preprocessed_data_extended.csv\")\n",
    "\n",
    "# Prepare features and target for training data\n",
    "X_train_full = train_data.drop(['aki_stage', 'icustay_id', 'charttime'], axis=1)\n",
    "y_train_full = train_data['aki_stage']\n",
    "\n",
    "# Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use SelectKBest to get feature importance\n",
    "selector = SelectKBest(f_classif, k='all')\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importance scores\n",
    "feature_scores_selector = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Score': selector.scores_\n",
    "})  \n",
    "# Use instead a trained models srted feature importance\n",
    "model = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "feature_importances_full = model.feature_importances_\n",
    "# get indeces of highes scores\n",
    "feature_scores_xgb = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Score': feature_importances_full\n",
    "})\n",
    "\n",
    "print(\"Selector\", feature_scores_selector.sort_values('Score', ascending=False))\n",
    "print(\"XGB\",feature_scores_xgb.sort_values('Score', ascending=False))\n",
    "\n",
    "# Sort features by importance\n",
    "feature_scores = feature_scores_xgb.sort_values('Score', ascending=False)\n",
    "\n",
    "\n",
    "# Print top 20 features\n",
    "print(feature_scores.head(20))\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score\n",
    "\n",
    "def train_and_evaluate(X_train, X_val, y_train, y_val, n_estimators=100):\n",
    "    model = XGBClassifier(n_estimators=n_estimators, use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_val_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    val_roc_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "    val_pr_auc = average_precision_score(y_val, y_val_pred_proba)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance_dict = dict(zip(X_train.columns, feature_importance))\n",
    "    sorted_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return val_roc_auc, val_pr_auc, val_accuracy, val_f1, sorted_importance\n",
    "\n",
    "results = []\n",
    "\n",
    "# Try different numbers of top features\n",
    "for n_features in [5, 10, 15, 20, 25, 30, 35, 40, 45]:\n",
    "    top_features = feature_scores['Feature'][:n_features]\n",
    "    X_train_subset = X_train[top_features]\n",
    "    X_val_subset = X_val[top_features]\n",
    "    \n",
    "    val_roc_auc, val_pr_auc, val_accuracy, val_f1, sorted_importance = train_and_evaluate(\n",
    "        X_train_subset, X_val_subset, y_train, y_val,\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'n_features': n_features,\n",
    "        'val_roc_auc': val_roc_auc,\n",
    "        'val_pr_auc': val_pr_auc,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'val_f1': val_f1,\n",
    "        'sorted_importance': sorted_importance\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(f\"Features: {result['n_features']}, \"\n",
    "          f\"Val ROC AUC: {result['val_roc_auc']:.4f}, Val PR AUC: {result['val_pr_auc']:.4f}, \"\n",
    "          f\"Val Accuracy: {result['val_accuracy']:.4f}, Val F1: {result['val_f1']:.4f}\")\n",
    "\n",
    "# save to disc as np\n",
    "np.save('data/feature_importance.npy', results)\n",
    "\n",
    "# save optimal features (where validation ROC AUC is the highest)\n",
    "optimal = max(results, key=lambda x: x['val_roc_auc'])\n",
    "optimal_features = optimal['sorted_importance']\n",
    "np.save('data/optimal_features.npy', optimal_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/feature_importances.npy', results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal = max(results, key=lambda x: x['test_roc_auc'])\n",
    "optimal_features = optimal['sorted_importance']\n",
    "np.save('data/optimal_features.npy', optimal_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Training on CPU') # On mac book GPU is not possible =() \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(np.array(train)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    times = math.floor(data.shape[0]/batch_size)\n",
    "    remainder = data.shape[0]%times\n",
    "    a = 0\n",
    "    start = 0\n",
    "    end = start+batch_size\n",
    "    if remainder ==0:\n",
    "        a +=1\n",
    "    while a<times:\n",
    "        temp = pad(data[start:end,],0)\n",
    "        x = torch.from_numpy(temp[:,:,1:-1]).float() # without icustay_id and without aki_stage columns\n",
    "        y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "        X_batches.append(x)\n",
    "        y_batches.append(y)\n",
    "        start = end\n",
    "        end = start+batch_size\n",
    "        a +=1\n",
    "    temp = pad(data[start:data.shape[0]],0)\n",
    "    x = torch.from_numpy(temp[:,:,1:-1]).float()\n",
    "    y = torch.flatten(torch.from_numpy(temp[:, :,-1].reshape(-1,1)).float()).long()\n",
    "    X_batches.append(x)\n",
    "    y_batches.append(y)\n",
    "    if len(X_batches) != len(y_batches):\n",
    "        print(\"length error\")\n",
    "    return X_batches, y_batches # arrays\n",
    "\n",
    "# batching\n",
    "X_train, y_train = batch(train, batch_size) # to count weights\n",
    "\n",
    "# counting balance of the classes\n",
    "y = []\n",
    "for i in y_train:\n",
    "    for element in i:\n",
    "        y.append(element.item())\n",
    "\n",
    "#  weights\n",
    "counter=collections.Counter(y)\n",
    "print(counter)\n",
    "zeroes = counter[0]\n",
    "ones = counter[1]\n",
    "\n",
    "X_test, y_test = batch(test, test.shape[0]) \n",
    "X_val, y_val = batch(validation, validation.shape[0])\n",
    "X_val = X_val[0]\n",
    "y_val = y_val[0]\n",
    "X_test = X_test[0]\n",
    "y_test = y_test[0]\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# setup\n",
    "\n",
    "bi_directional = True\n",
    "n_epochs = 50\n",
    "lr = 0.001\n",
    "features = len(X_train[0][0][0])\n",
    "print(features)\n",
    "# features = \n",
    "emb_size = round(features/1)\n",
    "number_layers = 3\n",
    "dropout = 0 # dropout\n",
    "\n",
    "##########################\n",
    "input_size = features\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size \n",
    "        self.output_size = output_size\n",
    "        self.number_layers = number_layers\n",
    "        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "        # in bidirectional encoder we have  forward and backward hidden states\n",
    "        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "        # Create affine layer to project to the classes \n",
    "        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "        #dropout layer for regularizetion of a sequence\n",
    "        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "        h = self.relu(self.combination_layer(h))\n",
    "        h = self.dropout_layer(h)\n",
    "        h = self.projection(h) \n",
    "        return h\n",
    "\n",
    "#create a network \n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "#print(nn_model)\n",
    "#print(list(nn_model.parameters()))\n",
    "\n",
    "\n",
    "# BCE Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss() # class imbalance\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(round(zeroes/ones,0))) # class imbalance\n",
    "#print(round(zeroes/ones,0))\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=lr) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count unique values\n",
    "unique, counts = np.unique(y_val, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "# Count NaN values\n",
    "nan_count = np.isnan(y_val).sum()\n",
    "print(f\"Number of NaN values: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all nans in X_val with 0\n",
    "X_val[torch.isnan(X_val)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_original = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_original = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_extended = X_val.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training loop (full data 3.5 hours)\n",
    "\n",
    "epochs = n_epochs\n",
    "starttime = datetime.now() # datetime object containing current date and time\n",
    "train_losses, validation_losses = [], []\n",
    "best = 0\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    print (\"\\n Epoch [%d] out of %d\" % (epoch + 1, epochs))\n",
    "    running_loss = 0.0\n",
    "    validation_loss = 0.0\n",
    "    roc_auc = 0.0\n",
    "    pr_auc = 0.0\n",
    "    m = 0\n",
    "    \n",
    "    #train\n",
    "    #print(list(nn_model.parameters())[0])\n",
    "    # pbar = tqdm(X_train, desc=f\"Epoch {epoch+1}\")\n",
    "    # for i in pbar:\n",
    "    #     # zero the parameter gradients\n",
    "    #     optimizer.zero_grad() # zero the gradient buffers not to consider gradients of previous iterations\n",
    "    #     X_batch = X_train[m]\n",
    "    #     y_batch = y_train[m]\n",
    "    #     # print(X_batch.shape)\n",
    "    #     # forward + backward + optimize\n",
    "    #     outputs = nn_model(X_batch)\n",
    "    #     outputs = torch.flatten(outputs)\n",
    "    #     y_batch = y_batch.type_as(outputs)\n",
    "    #     loss = criterion(outputs, y_batch)\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step() # Does the update\n",
    "    #     running_loss += loss.item()\n",
    "    #     m +=1\n",
    "    #     pbar.set_postfix({\"Training Loss\": running_loss/len(X_train)})\n",
    "        \n",
    "   \n",
    "    #validation \n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        v_out = nn_model(X_val) \n",
    "        v_out = torch.flatten(v_out) \n",
    "        y_val = y_val.type_as(v_out)\n",
    "        v_loss = criterion(v_out, y_val)\n",
    "        validation_loss = v_loss.item()\n",
    "        # auc and pr auc\n",
    "        val_prob = torch.nn.Sigmoid() (v_out)\n",
    "        print(type(v_out))\n",
    "        print(v_out)\n",
    "        print(val_prob)\n",
    "        print(y_val)\n",
    "        roc_auc = roc_auc_score(y_val,val_prob) \n",
    "        \n",
    "    validation_losses.append(validation_loss) \n",
    "    train_losses.append(running_loss/len(X_train)) \n",
    "    print(f\"Training loss: {running_loss/len(X_train):.3f}.. \" f\"Validation loss: {validation_loss:.3f}.. \")\n",
    "    print(f\"AUC: {roc_auc:.2f}\")  \n",
    "    nn_model.train()\n",
    "    \n",
    "    \n",
    "    if roc_auc > best:\n",
    "        best = roc_auc\n",
    "        PATH = './LSTMbest.pth' \n",
    "        torch.save(nn_model.state_dict(), PATH) # save the model\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "       \n",
    "print('Finished Training')\n",
    "print(\"starttime =\", starttime)\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "PATH = './LSTM.pth' \n",
    "torch.save(nn_model.state_dict(), PATH) # save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "PATH = './LSTM.pth'\n",
    "nn_model.load_state_dict(torch.load(PATH))\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    t_out = nn_model(X_test)\n",
    "    t_out = torch.flatten(t_out)\n",
    "    y_test = y_test.type_as(t_out)\n",
    "    t_loss = criterion(t_out, y_test)\n",
    "    test_loss = t_loss.item()\n",
    "    # auc and pr auc\n",
    "    test_prob = torch.nn.Sigmoid() (t_out)\n",
    "    roc_auc = roc_auc_score(y_test,test_prob) \n",
    "    pr_auc = average_precision_score(y_test,test_prob)\n",
    "    # convert output probabilities to class labels\n",
    "    test_pred = (test_prob > 0.5).float()\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = accuracy_score(y_test.cpu().numpy(), test_pred.cpu().numpy())\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}.. \" f\"ROC AUC: {roc_auc:.2f}.. \" f\"PR AUC: {pr_auc:.2f}.. \")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a freshly initialized model on test\n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "# nn_model.load_state_dict(torch.load(PATH))\n",
    "nn_model.eval()\n",
    "with torch.no_grad():\n",
    "    t_out = nn_model(X_test)\n",
    "    t_out = torch.flatten(t_out)\n",
    "    y_test = y_test.type_as(t_out)\n",
    "    t_loss = criterion(t_out, y_test)\n",
    "    test_loss = t_loss.item()\n",
    "    # auc and pr auc\n",
    "    test_prob = torch.nn.Sigmoid() (t_out)\n",
    "    roc_auc = roc_auc_score(y_test,test_prob) \n",
    "    pr_auc = average_precision_score(y_test,test_prob)\n",
    "    print(f\"Test loss: {test_loss:.3f}.. \" f\"ROC AUC: {roc_auc:.2f}.. \" f\"PR AUC: {pr_auc:.2f}.. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './i-Bidir_3_lr_0.001_nodropbest.pth'\n",
    "\n",
    "# save the model\n",
    "#torch.save(nn_model.state_dict(), PATH)\n",
    "\n",
    "# code to load saved model\n",
    "nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "nn_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test) # single batch with zero padding to the max shape 635208"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = nn_model(X_test)\n",
    "pred = torch.nn.Sigmoid() (logits)\n",
    "pred = pred.detach().numpy()\n",
    "pred = pred.reshape(-1,1)\n",
    "print(\"Performance on full X_test where it has no batching: is padded to max dimentions. \\n\")\n",
    "print (\"Area Under ROC Curve: %0.2f\" % roc_auc_score(y_test, pred, average = 'micro')  )\n",
    "brier = round(metrics.brier_score_loss(y_test, pred, sample_weight=None, pos_label=None),3)\n",
    "print(\"Brier score : {:.3f}\".format(brier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('padded_lstm.npy', 'wb') as f:\n",
    "    np.save(f, y_test)\n",
    "    np.save(f, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = X_test.shape[1] #133\n",
    "icustays = X_test.shape[0]\n",
    "times = []\n",
    "auc_s = []\n",
    "t = 0\n",
    "\n",
    "while t < timestamps:\n",
    "    times.append(t+1)\n",
    "    row = t\n",
    "    i = 0\n",
    "    prob_t = []\n",
    "    y_t = []\n",
    "    while i < icustays:\n",
    "        prob_t.append(pred[row])\n",
    "        y_t.append(y_test[row])\n",
    "        row += timestamps\n",
    "        i +=1\n",
    "    prob_t = np.array(prob_t).reshape(-1,1)\n",
    "    y_t = np.array(y_t).reshape(-1,1)\n",
    "    auc_s.append(roc_auc_score(y_t, prob_t, average = 'micro'))\n",
    "    t +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.DataFrame(auc_s, columns = ['AUC'])\n",
    "df['Timestamps'] = times\n",
    "#df[120:133]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \n",
    "sns.lineplot(x=\"Timestamps\", y=\"AUC\", color = 'g',\n",
    "             data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing to LogR, XGB, RF models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = batch(test, test.shape[0]) \n",
    "X_test = X_test[0]\n",
    "y_test = y_test[0]\n",
    "\n",
    "\n",
    "def to_one_label (model, label_list,X_test,index_list):\n",
    "    # evaluate on a test set\n",
    "    labels = np.array(label_list)\n",
    "    labels = labels.reshape(-1,1)\n",
    "    labels = labels.astype(int)\n",
    "    logits = model(X_test)\n",
    "    pred = torch.nn.Sigmoid() (logits)\n",
    "    max_rows = pred.shape[1]\n",
    "    predictions = pred.detach().numpy()\n",
    "    predictions = predictions.reshape(-1,1) \n",
    "    # select 1 per icu stay id by index\n",
    "    prob_1_label = []\n",
    "    row = 0\n",
    "    prev = 0\n",
    "    for i in index_list:\n",
    "        prob_1_label.append(predictions[row+i-prev])\n",
    "        row += pred.shape[1]\n",
    "        prev = i\n",
    "    prob_1_label = np.array(prob_1_label).reshape(-1,1)\n",
    "    \n",
    "    return labels, prob_1_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance (y_test, pred_probabilities):\n",
    "    # performance\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    # compute roc auc\n",
    "    roc_auc = roc_auc_score(y_test, pred_probabilities, average = 'micro')\n",
    "    # compute Precision_Recall curves\n",
    "    precision, recall, _ = precision_recall_curve(y_test, pred_probabilities)\n",
    "    # compute PR_AUC\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "       \n",
    "    # I add confusion matrix\n",
    "    optimal_cut_off = round(thresholds[np.argmax(tpr - fpr)],4)\n",
    "    a = np.where(pred_probabilities > optimal_cut_off, 1, 0)\n",
    "    brier = round(metrics.brier_score_loss(y_test, pred_probabilities, sample_weight=None, pos_label=None),3)\n",
    "    predictions = np.where(pred_probabilities > optimal_cut_off, 1, 0)  \n",
    "    \n",
    "    print (\"Area Under ROC Curve: %0.2f\" % roc_auc  )\n",
    "    #print (\"Area Under PR Curve(AP): %0.2f\" % pr_auc  ) \n",
    "    print(\"Brier score : {:.3f}\".format(brier))\n",
    "    #print('Accuracy for Classifier : {:.2f}'.format(accuracy_score(y_test, predictions)))\n",
    "    #print('Cut off: ' + str(optimal_cut_off))\n",
    "    matrix = metrics.confusion_matrix(y_test, a, labels=None, normalize=None)\n",
    "    #print(str(matrix))\n",
    "    \n",
    "    #f.write(\"\\n Area Under ROC Curve: \" +str(roc_auc))\n",
    "    #f.write(\"\\n Area Under PR Curve(AP): \" + str(pr_auc))\n",
    "    #f.write(\"\\n Brier score: \" +str(brier))\n",
    "    #f.write('\\n Accuracy for Classifier '+str(round((accuracy_score(labels, predictions)),3)))\n",
    "    #f.write(\"\\n Cut off: \" +str(optimal_cut_off))\n",
    "    #f.write(str(matrix))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels, prob_1_label = to_one_label (nn_model, label_list,X_test,index_list)\n",
    "performance(labels,prob_1_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save labels, prob_1_label\n",
    "\n",
    "with open('test.npy', 'wb') as f:\n",
    "    #np.save(f, labels)\n",
    "    np.save(f, prob_1_label)\n",
    "with open('test.npy', 'rb') as f:\n",
    "    #lstm_labels = np.load(f)\n",
    "    lstm_prob = np.load(f)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply integrated gradients, we first create an IntegratedGradients object, providing the model object.\n",
    "ig = IntegratedGradients(nn_model)\n",
    "# To compute the integrated gradients, we use the attribute method of the IntegratedGradients object. The method takes\n",
    "# tensor(s) of input examples (matching the forward function of the model), and returns the input attributions for the\n",
    "# given examples. A target index, defining the index of the output for which gradients are computed is 1, \n",
    "# corresponding to AKI (1/0).\n",
    "\n",
    "#The input tensor provided should require grad, so we call requires_grad_ on the tensor. The attribute method also \n",
    "# takes a baseline, which is the starting point from which gradients are integrated. The default value is just the \n",
    "# 0 tensor, which is a reasonable baseline / default for this task.\n",
    "\n",
    "#The returned values of the attribute method are the attributions, which match the size of the given inputs, and delta,\n",
    "# which approximates the error between the approximated integral and true integral.\n",
    "print(datetime.now())\n",
    "X_test.requires_grad_()\n",
    "attr, delta = ig.attribute(X_test,target=1, return_convergence_delta=True)\n",
    "attr = attr.detach().numpy()\n",
    "attr= np.reshape(attr,(-1,35))\n",
    "importances = np.mean(attr, axis=0)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[:,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr[:,4].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature_importances(feature_names, importances, title=\"LSTM Average Feature Importances\", axis_title=\"Features\"):\n",
    "    print(title)\n",
    "    i = 0\n",
    "    while i < features:\n",
    "        print(feature_names[i], \": \", '%.3f'%(importances[i]))\n",
    "        i +=1\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    \n",
    "visualize_feature_importances(feature_names, importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df =  pd.DataFrame(importances, columns = ['Feature Importance'])\n",
    "lstm_df['Features'] = feature_names\n",
    "lstm_df = lstm_df.sort_values(by = ['Feature Importance'], ascending = False, ignore_index = True)\n",
    "#lstm_df[\"Feature Importance\"] =  lstm_df[\"Feature Importance\"]\n",
    "#lstm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df[\"Feature Importance\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df)\n",
    "ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df, color = 'grey')\n",
    "ax.set_xlabel('Feature Importance', fontsize = 15)\n",
    "ax.set_ylabel(\"Features\",fontsize=15)\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 6)\n",
    "plt.title('LSTM feature Importances')\n",
    "plt.savefig('LSTM_feature_importance_grey.png', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_df['abs'] = abs(lstm_df['Feature Importance'])\n",
    "lstm_df = lstm_df.sort_values(by = ['abs'], ascending = False, ignore_index = True)\n",
    "lstm_df_10 = lstm_df.head(10)\n",
    "#lstm_df_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df_10, palette=\"mako\")\n",
    "\n",
    "ax = sns.barplot(x='Feature Importance', y='Features', data=lstm_df_10, color = 'darkgreen')\n",
    "ax.set_xlabel('Feature Importance', fontsize = 15)\n",
    "ax.set_ylabel(\"Features\",fontsize=15)\n",
    "ax.set_yticklabels(ax.get_ymajorticklabels(), fontsize = 10)\n",
    "plt.title('LSTM top 10 features by feature importance')\n",
    "plt.savefig('LSTM_top10_feature_importance_darkgreen.png', dpi = 300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graphs (y_test,pred_probabilities, classifier_name, plot_name, algorithm):\n",
    "    \n",
    "    def bin_total(y_true, y_prob, n_bins):\n",
    "        bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "        # In sklearn.calibration.calibration_curve, the last value in the array is always 0.\n",
    "        binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "        return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "    def missing_bin(bin_array):\n",
    "        midpoint = \" \"    \n",
    "        if bin_array[0]==0:\n",
    "            midpoint = \"5%, \"\n",
    "        if bin_array[1]==0:\n",
    "            midpoint = midpoint + \"15%, \"\n",
    "        if bin_array[2]==0:\n",
    "            midpoint = midpoint + \"25%, \"\n",
    "        if bin_array[3]==0:\n",
    "            midpoint = midpoint + \"35%, \" \n",
    "        if bin_array[4]==0:\n",
    "            midpoint = midpoint + \"45%, \"\n",
    "        if bin_array[5]==0:\n",
    "            midpoint = midpoint + \"55%, \"\n",
    "        if bin_array[6]==0:\n",
    "            midpoint = midpoint + \"65%, \"\n",
    "        if bin_array[7]==0:\n",
    "            midpoint = midpoint + \"75%, \"\n",
    "        if bin_array[8]==0:\n",
    "            midpoint = midpoint + \"85%, \"\n",
    "        if bin_array[9]==0:\n",
    "            midpoint = midpoint + \"95%, \"\n",
    "        return \"The missing bins have midpoint values of \"+ str(midpoint)\n",
    "    \n",
    "    # performance\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    # compute roc auc\n",
    "    roc_auc = roc_auc_score(y_test, pred_probabilities, average = 'micro')\n",
    "    # compute Precision_Recall curves\n",
    "    precision, recall, _ = precision_recall_curve(y_test, pred_probabilities)\n",
    "    # compute PR_AUC\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "\n",
    "    # compute calibration curve\n",
    "    LR_y, LR_x = calibration_curve(y_test, pred_probabilities, n_bins=10)\n",
    "    #find out which one are the missing bins\n",
    "    bin_array = bin_total(y_test, pred_probabilities , n_bins=10)\n",
    "    print(missing_bin(bin_array))\n",
    "\n",
    "    print(\"plot curves and save in one png file\")\n",
    "    #save three plots in one png file\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, figsize=(7, 24))\n",
    "    fig.subplots_adjust(wspace=0.3, hspace= 0.3)\n",
    "    fig.suptitle('Evaluation of '+ plot_name)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    \n",
    "    # plot roc curve\n",
    "    ax1.plot(fpr, tpr,'C2', label=algorithm+\" \"+\"Classifier \" + str(classifier_name) + \", auc=\" +str(round(roc_auc,2)))\n",
    "    ax1.title.set_text('ROC AUC')\n",
    "    ax1.set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot PR curve\n",
    "    ax2.plot(recall, precision,'C2', label=algorithm+\" \"+\"Classifier \" + str(classifier_name) + \", auc=\"+str(round(pr_auc,2)))\n",
    "    ax2.title.set_text('PR AUC')\n",
    "    ax2.set(xlabel='Recall', ylabel='Precision')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot calibration curve\n",
    "    ax3.plot(LR_x, LR_y, 'C2',marker='o', linewidth=1, label='LR')\n",
    "    line = mlines.Line2D([0, 1], [0, 1], color='black')\n",
    "    transform = ax3.transAxes\n",
    "    line.set_transform(transform)\n",
    "    ax3.add_line(line)\n",
    "    ax3.title.set_text('Calibration plot for '+str(plot_name))\n",
    "    ax3.set(xlabel= 'Predicted probability', ylabel= 'True probability in each bin')\n",
    "    ax3.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.savefig(plot_name+\".png\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(pred_probabilities, y_test, dist_name):\n",
    "    #probabilities distributions graphs\n",
    "    true_1 = pd.DataFrame(pred_probabilities, columns=['Predicted probabilities'])\n",
    "    true_1['labels'] = y_test.tolist()\n",
    "    true_0 = true_1.copy(deep = True) \n",
    "    indexNames = true_1[true_1['labels'] == 0].index\n",
    "    true_1.drop(indexNames , inplace=True)\n",
    "    indexNames = true_0[ true_0['labels'] == 1 ].index\n",
    "    true_0.drop(indexNames , inplace=True)\n",
    "    true_1.drop(columns=['labels'], inplace = True)\n",
    "    true_0.drop(columns=['labels'], inplace = True)\n",
    "    \n",
    "    sns.distplot(true_1['Predicted probabilities'], hist = False, kde = True,\n",
    "                 kde_kws = {'shade': True, 'linewidth': 3,\"color\": \"g\"}, label = 'Class 1')\n",
    "    plt.ylabel('Density')\n",
    "    sns.distplot(true_0['Predicted probabilities'], hist = False, kde = True,\n",
    "                     kde_kws = {'shade': True, 'linewidth': 3}, label = 'Class 0')\n",
    "    plt.title('Density Plot'+ dist_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distribution(prob_1_label, labels.flatten(), \" Bidirectional LSTM no imputation \")\n",
    "plt.savefig('dist_LSTM_bi_NOimp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name = \"None vs. Any AKI\"    ###change every time #Moderate vs. Severe #None vs. Any #Others vs. Severe\n",
    "plot_name = \"LSTM NO imputation\"\n",
    "build_graphs(labels.flatten(), prob_1_label.flatten(), classifier_name, plot_name, \"LSTM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(labels, prob_1_label)\n",
    "fpr, tpr, thresholds = roc_curve(labels, prob_1_label)\n",
    "optimal_cut_off = round(thresholds[np.argmax(tpr - fpr)],2)\n",
    "prediction = np.where(prob_1_label > optimal_cut_off, 1, 0)\n",
    "f1 = f1_score(labels,prediction)\n",
    "prauc =auc(recall, precision)\n",
    "print('F1 = %.3f, PR auc =%.3f' % (f1,prauc))\n",
    "\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(labels[labels==1]) / len(labels)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall,precision, marker='.', label='LSTM')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search grid \n",
    "layers = [1,2,3]\n",
    "l_rate = [0.001, 0.0001]\n",
    "drop = [0,0.2]\n",
    "bidirectionality = [True,False]\n",
    "#loops count\n",
    "hypercount = 0\n",
    "# static parameters\n",
    "n_epochs = 80\n",
    "emb_size = round(features/1)\n",
    "input_size = features\n",
    "output_size = 1\n",
    "###############################\n",
    "\n",
    "f = open('lstm_no_imp_uni.txt', 'w+') #change with or without imp\n",
    "\n",
    "for q1 in bidirectionality:\n",
    "    for q2 in layers:\n",
    "        for q3 in drop:\n",
    "            for q4 in l_rate:\n",
    "                hypercount +=1\n",
    "                name = \"i-Bidir_\" if q1 else \"i-Onedir_\"\n",
    "                name = name+str(q2) + \"_lr_\"+str(q4)\n",
    "                name = name+\"_drop\"+str(q3) if q3 == 0.2 else name+\"_nodrop\"\n",
    "                #set parameters\n",
    "                bi_directional = q1\n",
    "                lr = q4\n",
    "                number_layers = q2\n",
    "                dropout = q3 # dropout\n",
    "                print('hypercount: %d' % hypercount)\n",
    "                print('\\n')\n",
    "                print(name)\n",
    "                f.write('\\n\\n' + str(name)+ '\\n\\n')\n",
    "                    \n",
    "                # create the NN\n",
    "                class Net(nn.Module):\n",
    "                    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "                        super(Net, self).__init__()\n",
    "                        self.input_size = input_size\n",
    "                        self.emb_size = emb_size \n",
    "                        self.output_size = output_size\n",
    "                        self.number_layers = number_layers\n",
    "                        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "                        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "                        # in bidirectional encoder we have  forward and backward hidden states\n",
    "                        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "                        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "                        # Create affine layer to project to the classes \n",
    "                        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "                        #dropout layer for regularizetion of a sequence\n",
    "                        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "                        self.relu = nn.ReLU()\n",
    "\n",
    "                    def forward(self, x):\n",
    "                        h = self.relu(self.fc1(x))\n",
    "                        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "                        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "                        h = self.relu(self.combination_layer(h))\n",
    "                        h = self.dropout_layer(h)\n",
    "                        h = self.projection(h) \n",
    "                        return h\n",
    "\n",
    "                #create a network \n",
    "                nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "                print(nn_model)\n",
    "                #print(list(nn_model.parameters()))\n",
    "                \n",
    "                # BCE Loss and optimizer\n",
    "                criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(round(zeroes/ones,0))) # class imbalance\n",
    "                #print(round(zeroes/ones,0))\n",
    "                optimizer = optim.Adam(nn_model.parameters(), lr=lr) \n",
    "    \n",
    "    \n",
    "                # TRAINING LOOP \n",
    "                epochs = n_epochs\n",
    "                starttime = datetime.now() # datetime object containing current date and time\n",
    "                train_losses, validation_losses = [], []\n",
    "                best = 0\n",
    "                patience = 0\n",
    "                old_auc = 0\n",
    "                old_pr = 0\n",
    "\n",
    "                for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "                    print (\"\\n Epoch [%d] out of %d\" % (epoch + 1, epochs))\n",
    "                    running_loss = 0.0\n",
    "                    validation_loss = 0.0\n",
    "                    roc_auc = 0.0\n",
    "                    pr_auc = 0.0\n",
    "                    m = 0\n",
    "                    \n",
    "                    #train\n",
    "                    #print(list(nn_model.parameters())[0])\n",
    "                    for i in X_train:\n",
    "                        # zero the parameter gradients\n",
    "                        optimizer.zero_grad() # zero the gradient buffers not to consider gradients of previous iterations\n",
    "                        X_batch = X_train[m]\n",
    "                        y_batch = y_train[m]\n",
    "                        # forward + backward + optimize\n",
    "                        outputs = nn_model(X_batch)\n",
    "                        outputs = torch.flatten(outputs)\n",
    "                        y_batch = y_batch.type_as(outputs)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                        loss.backward()\n",
    "                        optimizer.step() # Does the update\n",
    "                        running_loss += loss.item()\n",
    "                        m +=1\n",
    "                    #validation \n",
    "                    nn_model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        v_out = nn_model(X_val) \n",
    "                        v_out = torch.flatten(v_out) \n",
    "                        y_val = y_val.type_as(v_out)\n",
    "                        v_loss = criterion(v_out, y_val)\n",
    "                        validation_loss = v_loss.item()\n",
    "                        # auc and pr auc\n",
    "                        val_prob = torch.nn.Sigmoid() (v_out)\n",
    "                        precision, recall, thresholds = precision_recall_curve(y_val, val_prob)\n",
    "                        pr_auc = auc(recall, precision)\n",
    "                        roc_auc = roc_auc_score(y_val,val_prob) \n",
    "\n",
    "                    validation_losses.append(validation_loss) \n",
    "                    train_losses.append(running_loss/len(X_train)) \n",
    "                    print(f\"Training loss: {running_loss/len(X_train):.3f}.. \" f\"Validation loss: {validation_loss:.3f}.. \")\n",
    "                    print(f\"AUC: {roc_auc:.2f} \" f\"PR AUC: {pr_auc:.2f} \")  \n",
    "                    nn_model.train()\n",
    "\n",
    "                    \n",
    "                    if roc_auc > best:\n",
    "                        best = roc_auc\n",
    "                        PATH1 = './'+str(name)+'best.pth' \n",
    "                        torch.save(nn_model.state_dict(), PATH1) # save the model\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    if roc_auc == old_auc and pr_auc==old_pr:\n",
    "                        patience +=1\n",
    "                    old_auc = roc_auc\n",
    "                    old_pr = pr_auc\n",
    "                    if patience ==10:\n",
    "                        print(\"out of patience\")\n",
    "                        break\n",
    "\n",
    "                print('\\n Finished Training')\n",
    "                print(\"starttime =\", starttime)\n",
    "                now = datetime.now()\n",
    "                print(\"endtime =\", now)\n",
    "                # end of training loop\n",
    "                \n",
    "                PATH2 = './'+str(name)+'last.pth' \n",
    "                torch.save(nn_model.state_dict(), PATH2) # save the model\n",
    "                print('\\n Last model \\n')\n",
    "                labels, probs = to_one_label(nn_model,label_list,X_test,index_list)\n",
    "                performance (nn_model, labels, probs)\n",
    "                \n",
    "                #load the best model\n",
    "                best_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "                best_model.load_state_dict(torch.load(PATH1))\n",
    "                print('\\n Best model \\n')\n",
    "                labels, probs = to_one_label(best_model,label_list,X_test,index_list)\n",
    "                performance (best_model, labels, probs)\n",
    "f.close() \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
