{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/app\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-30 12:32:15.653483\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "#data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "# NN\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score, roc_auc_score, auc, accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from captum.attr import IntegratedGradients\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# the full files pathes are here\n",
    "DATA_PATH_stages=\"data/kdigo_stages_measured.csv\" \n",
    "DATA_PATH_labs = \"data/labs-kdigo_stages_measured.csv\" \n",
    "DATA_PATH_vitals = \"data/vitals-kdigo_stages_measured.csv\" \n",
    "DATA_PATH_vents = \"data/vents-vasopressor-sedatives-kdigo_stages_measured.csv\"\n",
    "DATA_PATH_detail=\"data/icustay_detail-kdigo_stages_measured.csv\" \n",
    "SEPARATOR=\";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Training on CPU') # On mac book GPU is not possible =() \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPUTE_EACH_ID = False # imputation within each icustay_id with most common value\n",
    "IMPUTE_COLUMN = False # imputation based on whole column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter as constant \n",
    "\n",
    "TESTING = False \n",
    "TEST_SIZE = 0.05\n",
    "\n",
    "SPLIT_SIZE = 0.2 \n",
    "MAX_DAYS = 35\n",
    "\n",
    "#which classifier to use, only run one classifier at one time \n",
    "CLASS1 = True   #AnyAKI\n",
    "#CLASS2 = False    #ModerateSevereAKI\n",
    "#CLASS3 = False    #SevereAKI\n",
    "ALL_STAGES = False # not binary label, each class separately 0,1,2,3\n",
    "    \n",
    "MAX_FEATURE_SET = True\n",
    "#DIAGNOSIS = False\n",
    "\n",
    "FIRST_TURN_POS = True # creating one label per one ICU stay id\n",
    "\n",
    "# resampling  and imputing\n",
    "TIME_SAMPLING = True \n",
    "SAMPLING_INTERVAL = '6H'\n",
    "RESAMPLE_LIMIT = 16 # 4 days*6h interval\n",
    "MOST_COMMON = False #resampling with most common\n",
    "# if MOST_COMMON is not applied,sampling with different strategies per kind of variable, \n",
    "# numeric variables use mean value, categorical variables use max value\n",
    "\n",
    "IMPUTE_METHOD = 'most_frequent' \n",
    "FILL_VALUE = 0 #fill missing value and ragged part of 3d array\n",
    "\n",
    "#Age constraints: adults\n",
    "ADULTS_MIN_AGE = 18\n",
    "ADULTS_MAX_AGE = 120\n",
    "\n",
    "NORMALIZATION = 'min-max' \n",
    "\n",
    "CAPPING_THRESHOLD_UPPER = 0.99\n",
    "CAPPING_THRESHOLD_LOWER = 0.01\n",
    "\n",
    "# How much time the prediction should occur (hours)\n",
    "HOURS_AHEAD = 48\n",
    "\n",
    "NORM_TYPE = 'min_max'\n",
    "\n",
    "RANDOM = 42\n",
    "\n",
    "#set changable info corresponding to each classifier as variables\n",
    "\n",
    "min_set =  [\"icustay_id\", \"charttime\", \"creat\", \"uo_rt_6hr\", \"uo_rt_12hr\", \"uo_rt_24hr\", \"aki_stage\"]\n",
    "\n",
    "max_set = ['icustay_id', 'charttime', 'aki_stage', 'hadm_id','aniongap_avg', 'bicarbonate_avg', \n",
    "           'bun_avg','chloride_avg', 'creat', 'diasbp_mean', 'glucose_avg', 'heartrate_mean',\n",
    "           'hematocrit_avg', 'hemoglobin_avg', 'potassium_avg', 'resprate_mean','sodium_avg', 'spo2_mean', 'sysbp_mean', \n",
    "           'uo_rt_12hr', 'uo_rt_24hr','uo_rt_6hr', 'wbc_avg', 'sedative', 'vasopressor', 'vent', 'age', 'F','M', \n",
    "           'asian', 'black', 'hispanic', 'native', 'other', 'unknown','white', 'ELECTIVE', 'EMERGENCY', 'URGENT']\n",
    "\n",
    "# LSTM\n",
    "batch_size = 5\n",
    "\n",
    "# naming model and plot\n",
    "classifier_name = \"None vs. Any AKI\"    ###change every time #Moderate vs. Severe #None vs. Any #Others vs. Severe\n",
    "plot_name = \"adult_AnyAKI_LR\"    ###change every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions used later\n",
    "\n",
    "def cap_data(df):\n",
    "    print(\"Capping between the {} and {} quantile\".format(CAPPING_THRESHOLD_LOWER, CAPPING_THRESHOLD_UPPER))\n",
    "    cap_mask = df.columns.difference(['icustay_id', 'charttime', 'aki_stage'])\n",
    "    df[cap_mask] = df[cap_mask].clip(df[cap_mask].quantile(CAPPING_THRESHOLD_LOWER),\n",
    "                                     df[cap_mask].quantile(CAPPING_THRESHOLD_UPPER),\n",
    "                                     axis=1)\n",
    "\n",
    "    return df\n",
    " \n",
    "    \n",
    "def normalise_data(df, norm_mask):\n",
    "    print(\"Normalizing in [0,1] with {} normalization\".format(NORMALIZATION))\n",
    "    \n",
    "    df[norm_mask] = (df[norm_mask] - df[norm_mask].min()) / (df[norm_mask].max() - df[norm_mask].min())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# impute missing value in resampleing data with most common based on each id\n",
    "def fast_mode(df, key_cols, value_col):\n",
    "    \"\"\" Calculate a column mode, by group, ignoring null values. \n",
    "    \n",
    "    key_cols : list of str - Columns to groupby for calculation of mode.\n",
    "    value_col : str - Column for which to calculate the mode. \n",
    "\n",
    "    Return\n",
    "    pandas.DataFrame\n",
    "        One row for the mode of value_col per key_cols group. If ties, returns the one which is sorted first. \"\"\"\n",
    "    return (df.groupby(key_cols + [value_col]).size() \n",
    "              .to_frame('counts').reset_index() \n",
    "              .sort_values('counts', ascending=False) \n",
    "              .drop_duplicates(subset=key_cols)).drop('counts',axis=1)\n",
    "\n",
    "\n",
    "#get max shape of 3d array\n",
    "def get_dimensions(array, level=0):   \n",
    "    yield level, len(array)\n",
    "    try:\n",
    "        for row in array:\n",
    "            yield from get_dimensions(row, level + 1)\n",
    "    except TypeError: #not an iterable\n",
    "        pass\n",
    "\n",
    "def get_max_shape(array):\n",
    "    dimensions = defaultdict(int)\n",
    "    for level, length in get_dimensions(array):\n",
    "        dimensions[level] = max(dimensions[level], length)\n",
    "    return [value for _, value in sorted(dimensions.items())]\n",
    "\n",
    "#pad the ragged 3d array to rectangular shape based on max size\n",
    "def iterate_nested_array(array, index=()):\n",
    "    try:\n",
    "        for idx, row in enumerate(array):\n",
    "            yield from iterate_nested_array(row, (*index, idx)) \n",
    "    except TypeError: # final level            \n",
    "        yield (*index, slice(len(array))), array # think of the types\n",
    "\n",
    "def pad(array, fill_value):\n",
    "    dimensions = get_max_shape(array)\n",
    "    result = np.full(dimensions, fill_value, dtype = np.float64)  \n",
    "    for index, value in iterate_nested_array(array):\n",
    "        result[index] = value \n",
    "    return result\n",
    "\n",
    "def bin_total(y_true, y_prob, n_bins):\n",
    "    bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "    # In sklearn.calibration.calibration_curve,\n",
    "    # the last value in the array is always 0.\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "def missing_bin(bin_array):\n",
    "    midpoint = \" \"    \n",
    "    if bin_array[0]==0:\n",
    "        midpoint = \"5%, \"\n",
    "    if bin_array[1]==0:\n",
    "        midpoint = midpoint + \"15%, \"\n",
    "    if bin_array[2]==0:\n",
    "        midpoint = midpoint + \"25%, \"\n",
    "    if bin_array[3]==0:\n",
    "        midpoint = midpoint + \"35%, \" \n",
    "    if bin_array[4]==0:\n",
    "        midpoint = midpoint + \"45%, \"\n",
    "    if bin_array[5]==0:\n",
    "        midpoint = midpoint + \"55%, \"\n",
    "    if bin_array[6]==0:\n",
    "        midpoint = midpoint + \"65%, \"\n",
    "    if bin_array[7]==0:\n",
    "        midpoint = midpoint + \"75%, \"\n",
    "    if bin_array[8]==0:\n",
    "        midpoint = midpoint + \"85%, \"\n",
    "    if bin_array[9]==0:\n",
    "        midpoint = midpoint + \"95%, \"\n",
    "    return \"The missing bins have midpoint values of \"+ str(midpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data_theirs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"read csv files\")\n",
    "#reading csv files\n",
    "X = pd.read_csv(DATA_PATH_stages, sep= SEPARATOR)\n",
    "# print column names\n",
    "print(X.columns)\n",
    "# print number of rows and columns\n",
    "print(X.shape)\n",
    "X.drop([\"aki_stage_creat\", \"aki_stage_uo\"], axis = 1, inplace = True)\n",
    "#remove totally empty rows \n",
    "X = X.dropna(how = 'all', subset = ['creat','uo_rt_6hr','uo_rt_12hr','uo_rt_24hr','aki_stage'])\n",
    "print(\"convert charttime to timestamp\")\n",
    "X['charttime'] = pd.to_datetime(X['charttime'])\n",
    "\n",
    "#merge rows if they have exact timestamp within same icustay_id AL : it substitutes missing values with zero\n",
    "#X = X.groupby(['icustay_id', 'charttime']).sum().reset_index(['icustay_id', 'charttime'])\n",
    "\n",
    "dataset_detail = pd.read_csv(DATA_PATH_detail, sep= SEPARATOR)  #age constraint\n",
    "dataset_detail.drop(['dod', 'admittime','dischtime', 'los_hospital','ethnicity','hospital_expire_flag', 'hospstay_seq',\n",
    "       'first_hosp_stay', 'intime', 'outtime', 'los_icu', 'icustay_seq','first_icu_stay'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labs = pd.read_csv(DATA_PATH_labs, sep= SEPARATOR) # 'bands lactate platelet ptt inr pt\n",
    "dataset_labs.drop(['albumin_min', 'albumin_max','bilirubin_min', 'bilirubin_max','bands_min', 'bands_max',\n",
    "                   'lactate_min', 'lactate_max','platelet_min', 'platelet_max','ptt_min', 'ptt_max', \n",
    "                   'inr_min', 'inr_max', 'pt_min', 'pt_max'], axis = 1, inplace = True)\n",
    "dataset_labs = dataset_labs.dropna(subset=['charttime'])\n",
    "dataset_labs = dataset_labs.dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "dataset_labs['charttime'] = pd.to_datetime(dataset_labs['charttime'])\n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "if  MAX_FEATURE_SET:\n",
    "    dataset_vitals = pd.read_csv(DATA_PATH_vitals, sep= SEPARATOR)  \n",
    "    dataset_vents = pd.read_csv(DATA_PATH_vents , sep= SEPARATOR)\n",
    "    #dataset_icd = pd.read_csv(DATA_PATH_icd, sep= SEPARATOR)\n",
    "    dataset_vitals.drop([\"heartrate_min\", \"heartrate_max\",\"sysbp_min\", \"sysbp_max\",\"diasbp_min\", \"diasbp_max\",\n",
    "                        'meanbp_min','meanbp_max', 'meanbp_mean','tempc_min', 'tempc_max', 'tempc_mean',\n",
    "                        \"resprate_min\", \"resprate_max\", \"spo2_min\", \"spo2_max\", \"glucose_min\", \"glucose_max\"], axis = 1, inplace = True)    \n",
    "    print(\"convert charttime to timestamp\")\n",
    "    dataset_vitals['charttime'] = pd.to_datetime(dataset_vitals['charttime'])\n",
    "    dataset_vents['charttime'] = pd.to_datetime(dataset_vents['charttime'])\n",
    "    dataset_vitals = dataset_vitals.sort_values(by=['icustay_id', 'charttime'])\n",
    "    dataset_vents = dataset_vents.sort_values(by=['icustay_id', 'charttime'])\n",
    "    # AL drop those where all columns are nan (empty rows)\n",
    "    dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')   \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now())\n",
    "# Labs file: instead of min and max their avg\n",
    "counter = 0\n",
    "col1 = 4\n",
    "col2 = 5\n",
    "null_l = [] # no null values in those that are different\n",
    "changed = 0 # 4316 records changed to avg\n",
    "\n",
    "while counter < 11:\n",
    "    row = 0\n",
    "# find where min and max are different and save their row indices \n",
    "    while row < len(dataset_labs):\n",
    "        a = dataset_labs.iloc[row,col1]\n",
    "        b = dataset_labs.iloc[row,col2]\n",
    "        if a==b or (np.isnan(a) and np.isnan(b)):\n",
    "            pass\n",
    "        elif a!=b:\n",
    "            changed +=1\n",
    "            avg = (a+b)/2\n",
    "            dataset_labs.iloc[row,col1] = avg\n",
    "            if (np.isnan(a) and ~np.isnan(b)) or (np.isnan(b) and ~np.isnan(a)):\n",
    "                null_l.append(row)\n",
    "        else:\n",
    "            print(a)\n",
    "            print(b)\n",
    "        row +=1       \n",
    "    # delete the redundant column max, update counters\n",
    "    dataset_labs.drop(dataset_labs.columns[col2], axis=1, inplace = True)\n",
    "    counter = counter+1\n",
    "    col1 = col1+1\n",
    "    col2 = col2+1\n",
    "\n",
    "dataset_labs.columns = ['subject_id','hadm_id', 'icustay_id', 'charttime', 'aniongap_avg', 'bicarbonate_avg', \n",
    "                        'creatinine_avg', 'chloride_avg', 'glucose_avg', 'hematocrit_avg','hemoglobin_avg',\n",
    "                        'potassium_avg', 'sodium_avg', 'bun_avg', 'wbc_avg']\n",
    "if len(null_l)>0:\n",
    "    print(\"null values encountered\")\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merge creatinine and glucose.\")\n",
    "# merge creatinine from labs and set with labels\n",
    "creat_l = dataset_labs[['icustay_id','charttime','creatinine_avg']].copy()\n",
    "creat_l = creat_l.dropna(subset=['creatinine_avg'])\n",
    "creat = X[['icustay_id','charttime', 'creat']].copy()\n",
    "creat = creat.dropna(subset=['creat'])\n",
    "creat_l = creat_l.rename(columns={\"creatinine_avg\": \"creat\"})\n",
    "creat = creat._append(creat_l, ignore_index=True)\n",
    "creat.drop_duplicates(inplace = True)\n",
    "#delete old columns\n",
    "dataset_labs.drop([\"creatinine_avg\"], axis = 1, inplace = True)\n",
    "dataset_labs = dataset_labs.dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "X.drop([\"creat\"], axis = 1, inplace = True)\n",
    "#merge new column\n",
    "X = pd.merge(X, creat, on = [\"icustay_id\", \"charttime\"], sort = True, how= \"outer\", copy = False)\n",
    "\n",
    "if MAX_FEATURE_SET:\n",
    "    # merge glucose from vitals and labs\n",
    "    glucose_v = dataset_vitals[['subject_id','hadm_id','icustay_id','charttime', 'glucose_mean']].copy()\n",
    "    glucose_v = glucose_v.dropna(subset=['glucose_mean'])\n",
    "    glucose = dataset_labs[['subject_id','hadm_id','icustay_id','charttime', 'glucose_avg']].copy()\n",
    "    glucose = glucose.dropna(subset=['glucose_avg'])\n",
    "    glucose_v = glucose_v.rename(columns={\"glucose_mean\": \"glucose_avg\"})\n",
    "    glucose = glucose._append(glucose_v, ignore_index=True)\n",
    "    glucose.drop_duplicates(inplace = True)\n",
    "    #delete old columns\n",
    "    dataset_labs.drop([\"glucose_avg\"], axis = 1, inplace = True)\n",
    "    dataset_vitals.drop([\"glucose_mean\"], axis = 1, inplace = True)\n",
    "    dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')\n",
    "    #merge new column\n",
    "    dataset_labs = pd.merge(dataset_labs, glucose, on = ['subject_id','hadm_id','icustay_id','charttime',], sort = True, how= \"outer\", copy = False)\n",
    "    \n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging labs, vitals and vents files\")\n",
    "if MAX_FEATURE_SET:\n",
    "    X = pd.merge(X, dataset_labs, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False)\n",
    "    X = pd.merge(X, dataset_vitals, on = [\"icustay_id\", \"charttime\",\"subject_id\", \"hadm_id\"], how= \"outer\", copy = False)\n",
    "    X = pd.merge(X, dataset_vents, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False) \n",
    "    X.drop([\"subject_id\"], axis = 1, inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_detail.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start preprocessing time dependent data\") \n",
    "print(\"Removing patients under the min age\")\n",
    "old_length = len(X)\n",
    "dataset_detail = dataset_detail.loc[dataset_detail['admission_age'] >= ADULTS_MIN_AGE]\n",
    "adults_icustay_id_list = dataset_detail['icustay_id'].unique()\n",
    "X = X[X.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id'], ignore_index = True)\n",
    "X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "adults_icustay_id_list = np.sort(adults_icustay_id_list)\n",
    "new_length = len(X)\n",
    "\n",
    "# print out number of patients removed\n",
    "print(\"Number of patients removed: \", old_length - new_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(old_length)\n",
    "print(new_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"drop icustay_id with time span less than 48hrs\")\n",
    "def more_than_HOURS_ahead(adults_icustay_id_list, X):\n",
    "    drop_list = []\n",
    "    los_list = [] # calculating LOS ICU based on charttime\n",
    "    long_stays_id = [] # LOS longer than MAX DAYS days\n",
    "    last_charttime_list = []\n",
    "    seq_length = X.groupby(['icustay_id'],as_index=False).size()\n",
    "    id_count = 0\n",
    "    first_row_index = 0\n",
    "\n",
    "    while id_count < len(adults_icustay_id_list):\n",
    "        icustay_id = adults_icustay_id_list[id_count]\n",
    "        last_row_index = first_row_index + seq_length.iloc[id_count,1]-1\n",
    "        first_time = X.iat[first_row_index, X.columns.get_loc('charttime')]\n",
    "        last_time = X.iat[last_row_index, X.columns.get_loc('charttime')]\n",
    "        los = round(float((last_time - first_time).total_seconds()/60/60/24),4) # in days\n",
    "        if los < HOURS_AHEAD/24:\n",
    "            drop_list.append(icustay_id)\n",
    "        else:\n",
    "            los_list.append(los)\n",
    "            if los > MAX_DAYS:\n",
    "                long_stays_id.append(icustay_id)\n",
    "                last_charttime_list.append(last_time)\n",
    "        # udpate for the next icustay_id\n",
    "        first_row_index = last_row_index+1\n",
    "        id_count +=1\n",
    "    if len(long_stays_id) != len(last_charttime_list):\n",
    "        print('ERROR')\n",
    "    print(\"%d long stays\" % len(long_stays_id))\n",
    "    # drop all the rows with the saved icustay_id\n",
    "    print(\"there are %d id-s shorter than 48 hours\" % len(drop_list))\n",
    "    X = X[~X.icustay_id.isin(drop_list)]\n",
    "    id_list = X['icustay_id'].unique()\n",
    "    X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "    \n",
    "    return id_list, X, long_stays_id,last_charttime_list\n",
    "\n",
    "id_list, X, long_stays_id,last_charttime_list  = more_than_HOURS_ahead(adults_icustay_id_list, X)\n",
    "\n",
    "long = pd.DataFrame()\n",
    "long['icustay_id']  = long_stays_id\n",
    "long['last_time']  = last_charttime_list\n",
    "\n",
    "new_length = len(X)\n",
    "print(new_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting rows that are not within MAX_DAYS (35) period\n",
    "i = 0 # long df index\n",
    "drop_long_time = []\n",
    "    \n",
    "while i < len(long_stays_id):\n",
    "    j = 0\n",
    "    all_rows = X.index[X['icustay_id'] == long.loc[i,'icustay_id']].tolist()\n",
    "    while j < len(all_rows):\n",
    "        time = X.iat[all_rows[j], X.columns.get_loc('charttime')]\n",
    "        # if keep last MAX_DAYS \n",
    "        if (long.loc[i,'last_time'] - time).total_seconds() > MAX_DAYS*24*60*60:\n",
    "            drop_long_time.append(all_rows[j])\n",
    "            j +=1\n",
    "        else:\n",
    "            break\n",
    "    i +=1       \n",
    "X.drop(X.index[drop_long_time], inplace=True) \n",
    "\n",
    "# checking for 48h min length again\n",
    "id_list, X, long_stays_id,last_charttime_list  = more_than_HOURS_ahead(id_list, X)\n",
    "dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(id_list)].sort_values(by=['icustay_id'], ignore_index = True)\n",
    "\n",
    "new_length = len(X)\n",
    "print(new_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purpose, use small amount of data first\n",
    "if TESTING:\n",
    "    rest, id_list = train_test_split(id_list, test_size= TEST_SIZE, random_state=42)\n",
    "    X = X[X.icustay_id.isin(id_list)].sort_values(by=['icustay_id'])\n",
    "    dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(id_list)].sort_values(by=['icustay_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling , imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (TIME_SAMPLING and MOST_COMMON):\n",
    "    print(\"resampling: MOST_COMMON\")\n",
    "    # Resample the data using assigned interval,mode() for most common\n",
    "    X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL).mode().reset_index()\n",
    "elif TIME_SAMPLING:\n",
    "    print(\"resampling: MEAN & ZERO\")\n",
    "    # Sampling with different strategies per kind of variable\n",
    "    label = ['aki_stage']\n",
    "    skip = ['icustay_id', 'charttime', 'aki_stage']\n",
    "    if MAX_FEATURE_SET:\n",
    "        discrete_feat = ['sedative', 'vasopressor', 'vent', 'hadm_id']\n",
    "        skip.extend(discrete_feat)    \n",
    "    # all features that are not in skip are numeric\n",
    "    numeric_feat = list(X.columns.difference(skip))\n",
    "    \n",
    "    # Applying aggregation to features depending on their type\n",
    "    X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n",
    "    if MAX_FEATURE_SET:\n",
    "        X_discrete = X[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "    X_numeric = X[numeric_feat].mean() \n",
    "    X_label = X['aki_stage'].max()\n",
    "    print(\"Merging sampled features\")\n",
    "    try:\n",
    "        X = pd.concat([X_numeric, X_discrete,X_label], axis=1).reset_index()\n",
    "    except:\n",
    "        X = pd.concat([X_numeric,X_label], axis=1).reset_index()\n",
    "print(X.shape)\n",
    "#Label forward fill\n",
    "X['aki_stage'] = X['aki_stage'].ffill(limit=RESAMPLE_LIMIT)\n",
    "\n",
    "new_length = len(X) \n",
    "print(new_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Imputation.\")\n",
    "# do imputation of label with zero if there are still missing values\n",
    "X['aki_stage'] = X['aki_stage'].fillna(0)\n",
    "\n",
    "# using most common within each icustay_id\n",
    "if IMPUTE_EACH_ID:\n",
    "    column_name = list(X.columns)\n",
    "    column_name.remove(column_name[0]) \n",
    "    for feature in column_name:\n",
    "        X.loc[X[feature].isnull(), feature] = X.icustay_id.map(fast_mode(X, ['icustay_id'], feature).set_index('icustay_id')[feature])       \n",
    "\n",
    "# imputation based on whole column\n",
    "if IMPUTE_COLUMN:\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy= IMPUTE_METHOD)\n",
    "    cols = list(X.columns)\n",
    "    cols = cols[2:23]\n",
    "    X[cols]=imp.fit_transform(X[cols])  \n",
    "\n",
    "# If no imputation method selected or only impute each id, for the remaining nan impute direclty with FILL_VALUE\n",
    "X = X.fillna(FILL_VALUE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the number of columns of X\n",
    "print(len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more comfortable to review in this order\n",
    "try:\n",
    "    cols = ['icustay_id', 'charttime','aki_stage','hadm_id','aniongap_avg','bicarbonate_avg', 'bun_avg','chloride_avg',\n",
    "            'creat','diasbp_mean', 'glucose_avg', 'heartrate_mean', 'hematocrit_avg','hemoglobin_avg', \n",
    "            'potassium_avg', 'resprate_mean', 'sodium_avg','spo2_mean', 'sysbp_mean', 'uo_rt_12hr', \n",
    "            'uo_rt_24hr', 'uo_rt_6hr','wbc_avg', 'sedative', 'vasopressor', 'vent' ]\n",
    "    X = X[cols]\n",
    "    print(\"success\")\n",
    "except:\n",
    "    try:\n",
    "        cols = ['icustay_id', 'charttime','aki_stage','creat','uo_rt_12hr', 'uo_rt_24hr', 'uo_rt_6hr']\n",
    "        X = X[cols]\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessed data to csv\n",
    "X.to_csv('data/preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the preprocessed data\n",
    "X = pd.read_csv('data/data/X1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col = \"uo_rt_12hr\"\n",
    "\n",
    "plt.figure()\n",
    "X.groupby('aki_stage')[col].mean().plot(kind='bar')\n",
    "plt.ylabel(col)\n",
    "plt.title(f'Mean {col} by aki_stage')\n",
    "plt.savefig(f'data/analysis/{col}_by_aki_stage.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print headers of X\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"binarise labels\")\n",
    "if ALL_STAGES:\n",
    "    pass\n",
    "elif CLASS1:\n",
    "    X.loc[X['aki_stage'] > 1, 'aki_stage'] = 1\n",
    "elif CLASS2:\n",
    "    X.loc[X['aki_stage'] < 2, 'aki_stage'] = 0\n",
    "    X.loc[X['aki_stage'] > 1, 'aki_stage'] = 1\n",
    "elif CLASS3:\n",
    "    X.loc[X['aki_stage'] < 3, 'aki_stage'] = 0\n",
    "    X.loc[X['aki_stage'] > 2, 'aki_stage'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHIFTING labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Shifting the labels 48 h\") # by 8 position : 6h sampling*8=48h and ffil 8 newly empty ones\n",
    "X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].shift(-(HOURS_AHEAD // int(SAMPLING_INTERVAL[:-1])))\n",
    "X = X.dropna(subset=['aki_stage'])\n",
    "X['icustay_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add categorical features (details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_detail.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start preprocessing not time dependent data\")\n",
    "if MAX_FEATURE_SET:\n",
    "    #extract datasets based on id_list\n",
    "    dataset_detail = dataset_detail.loc[dataset_detail['icustay_id'].isin(id_list)]\n",
    "    #sort by ascending order\n",
    "    dataset_detail = dataset_detail.sort_values(by=['icustay_id'])\n",
    "    subject_id = dataset_detail[\"subject_id\"].unique()\n",
    "    #transfrom categorical data to binary form\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop('gender')))\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop(\"ethnicity_grouped\")))\n",
    "    # dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop('admission_type')))\n",
    "    dataset_detail = dataset_detail.drop(['subject_id', 'hadm_id'], axis=1)\n",
    "    X =  pd.merge(X, dataset_detail, on = [\"icustay_id\"], how= \"left\", copy = False) \n",
    "    numeric_feat.append('admission_age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names =['Anion gap', 'Bicarbonate', 'Blood Urea Nitrogen', 'Chloride', 'Creatinine', 'Diastolic BP', 'Glucose', 'Heart rate', \n",
    "            'Hematocrit', 'Hemoglobin', 'Potassium', 'Respiratory rate', 'Sodium', 'Oxygen saturation', 'Systolic BP', 'Urine output 12h', 'Urine output 24h', 'Urine output 6h',\n",
    "            'White cell count', 'Sedative', 'Vasopressor', 'Ventilation', 'Age', 'Female gender', 'Male gender', 'Asian ethnicity', 'Black ethnicity', 'Hispanic ethnicity', 'Native american', \n",
    "            'Other ethnicity', 'Ethnicity unknown', 'White ethnicity', 'Elective admission', 'Emergency admission', 'Urgent admission']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"read csv files\")\n",
    "#reading csv files\n",
    "X = pd.read_csv(DATA_PATH_stages, sep= SEPARATOR)\n",
    "# print column names\n",
    "print(X.columns)\n",
    "# print number of rows and columns\n",
    "print(X.shape)\n",
    "X.drop([\"aki_stage_creat\", \"aki_stage_uo\"], axis = 1, inplace = True)\n",
    "#remove totally empty rows \n",
    "X = X.dropna(how = 'all', subset = ['creat','uo_rt_6hr','uo_rt_12hr','uo_rt_24hr','aki_stage'])\n",
    "print(\"convert charttime to timestamp\")\n",
    "X['charttime'] = pd.to_datetime(X['charttime'])\n",
    "\n",
    "#merge rows if they have exact timestamp within same icustay_id AL : it substitutes missing values with zero\n",
    "#X = X.groupby(['icustay_id', 'charttime']).sum().reset_index(['icustay_id', 'charttime'])\n",
    "\n",
    "dataset_detail = pd.read_csv(DATA_PATH_detail, sep= SEPARATOR)  #age constraint\n",
    "dataset_detail.drop(['dod', 'admittime','dischtime', 'los_hospital','ethnicity','hospital_expire_flag', 'hospstay_seq',\n",
    "       'first_hosp_stay', 'intime', 'outtime', 'los_icu', 'icustay_seq','first_icu_stay'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labs = pd.read_csv(DATA_PATH_labs, sep= SEPARATOR) # 'bands lactate platelet ptt inr pt\n",
    "dataset_labs.drop(['albumin_min', 'albumin_max','bilirubin_min', 'bilirubin_max','bands_min', 'bands_max',\n",
    "                   'lactate_min', 'lactate_max','platelet_min', 'platelet_max','ptt_min', 'ptt_max', \n",
    "                   'inr_min', 'inr_max', 'pt_min', 'pt_max'], axis = 1, inplace = True)\n",
    "dataset_labs = dataset_labs.dropna(subset=['charttime'])\n",
    "dataset_labs = dataset_labs.dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "dataset_labs['charttime'] = pd.to_datetime(dataset_labs['charttime'])\n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "if  MAX_FEATURE_SET:\n",
    "    dataset_vitals = pd.read_csv(DATA_PATH_vitals, sep= SEPARATOR)  \n",
    "    dataset_vents = pd.read_csv(DATA_PATH_vents , sep= SEPARATOR)\n",
    "    #dataset_icd = pd.read_csv(DATA_PATH_icd, sep= SEPARATOR)\n",
    "    dataset_vitals.drop([\"heartrate_min\", \"heartrate_max\",\"sysbp_min\", \"sysbp_max\",\"diasbp_min\", \"diasbp_max\",\n",
    "                        'meanbp_min','meanbp_max', 'meanbp_mean','tempc_min', 'tempc_max', 'tempc_mean',\n",
    "                        \"resprate_min\", \"resprate_max\", \"spo2_min\", \"spo2_max\", \"glucose_min\", \"glucose_max\"], axis = 1, inplace = True)    \n",
    "    print(\"convert charttime to timestamp\")\n",
    "    dataset_vitals['charttime'] = pd.to_datetime(dataset_vitals['charttime'])\n",
    "    dataset_vents['charttime'] = pd.to_datetime(dataset_vents['charttime'])\n",
    "    dataset_vitals = dataset_vitals.sort_values(by=['icustay_id', 'charttime'])\n",
    "    dataset_vents = dataset_vents.sort_values(by=['icustay_id', 'charttime'])\n",
    "    # AL drop those where all columns are nan (empty rows)\n",
    "    dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')   \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now())\n",
    "# Labs file: instead of min and max their avg\n",
    "counter = 0\n",
    "col1 = 4\n",
    "col2 = 5\n",
    "null_l = [] # no null values in those that are different\n",
    "changed = 0 # 4316 records changed to avg\n",
    "\n",
    "while counter < 11:\n",
    "    row = 0\n",
    "# find where min and max are different and save their row indices \n",
    "    while row < len(dataset_labs):\n",
    "        a = dataset_labs.iloc[row,col1]\n",
    "        b = dataset_labs.iloc[row,col2]\n",
    "        if a==b or (np.isnan(a) and np.isnan(b)):\n",
    "            pass\n",
    "        elif a!=b:\n",
    "            changed +=1\n",
    "            avg = (a+b)/2\n",
    "            dataset_labs.iloc[row,col1] = avg\n",
    "            if (np.isnan(a) and ~np.isnan(b)) or (np.isnan(b) and ~np.isnan(a)):\n",
    "                null_l.append(row)\n",
    "        else:\n",
    "            print(a)\n",
    "            print(b)\n",
    "        row +=1       \n",
    "    # delete the redundant column max, update counters\n",
    "    dataset_labs.drop(dataset_labs.columns[col2], axis=1, inplace = True)\n",
    "    counter = counter+1\n",
    "    col1 = col1+1\n",
    "    col2 = col2+1\n",
    "\n",
    "dataset_labs.columns = ['subject_id','hadm_id', 'icustay_id', 'charttime', 'aniongap_avg', 'bicarbonate_avg', \n",
    "                        'creatinine_avg', 'chloride_avg', 'glucose_avg', 'hematocrit_avg','hemoglobin_avg',\n",
    "                        'potassium_avg', 'sodium_avg', 'bun_avg', 'wbc_avg']\n",
    "if len(null_l)>0:\n",
    "    print(\"null values encountered\")\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merge creatinine and glucose.\")\n",
    "# merge creatinine from labs and set with labels\n",
    "creat_l = dataset_labs[['icustay_id','charttime','creatinine_avg']].copy()\n",
    "creat_l = creat_l.dropna(subset=['creatinine_avg'])\n",
    "creat = X[['icustay_id','charttime', 'creat']].copy()\n",
    "creat = creat.dropna(subset=['creat'])\n",
    "creat_l = creat_l.rename(columns={\"creatinine_avg\": \"creat\"})\n",
    "creat = creat._append(creat_l, ignore_index=True)\n",
    "creat.drop_duplicates(inplace = True)\n",
    "#delete old columns\n",
    "dataset_labs.drop([\"creatinine_avg\"], axis = 1, inplace = True)\n",
    "dataset_labs = dataset_labs.dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "X.drop([\"creat\"], axis = 1, inplace = True)\n",
    "#merge new column\n",
    "X = pd.merge(X, creat, on = [\"icustay_id\", \"charttime\"], sort = True, how= \"outer\", copy = False)\n",
    "\n",
    "if MAX_FEATURE_SET:\n",
    "    # merge glucose from vitals and labs\n",
    "    glucose_v = dataset_vitals[['subject_id','hadm_id','icustay_id','charttime', 'glucose_mean']].copy()\n",
    "    glucose_v = glucose_v.dropna(subset=['glucose_mean'])\n",
    "    glucose = dataset_labs[['subject_id','hadm_id','icustay_id','charttime', 'glucose_avg']].copy()\n",
    "    glucose = glucose.dropna(subset=['glucose_avg'])\n",
    "    glucose_v = glucose_v.rename(columns={\"glucose_mean\": \"glucose_avg\"})\n",
    "    glucose = glucose._append(glucose_v, ignore_index=True)\n",
    "    glucose.drop_duplicates(inplace = True)\n",
    "    #delete old columns\n",
    "    dataset_labs.drop([\"glucose_avg\"], axis = 1, inplace = True)\n",
    "    dataset_vitals.drop([\"glucose_mean\"], axis = 1, inplace = True)\n",
    "    dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')\n",
    "    #merge new column\n",
    "    dataset_labs = pd.merge(dataset_labs, glucose, on = ['subject_id','hadm_id','icustay_id','charttime',], sort = True, how= \"outer\", copy = False)\n",
    "    \n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging labs, vitals and vents files\")\n",
    "if MAX_FEATURE_SET:\n",
    "    X = pd.merge(X, dataset_labs, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False)\n",
    "    X = pd.merge(X, dataset_vitals, on = [\"icustay_id\", \"charttime\",\"subject_id\", \"hadm_id\"], how= \"outer\", copy = False)\n",
    "    X = pd.merge(X, dataset_vents, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False) \n",
    "    X.drop([\"subject_id\"], axis = 1, inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_detail.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start preprocessing time dependent data\") \n",
    "print(\"Removing patients under the min age\")\n",
    "old_length = len(X)\n",
    "dataset_detail = dataset_detail.loc[dataset_detail['admission_age'] >= ADULTS_MIN_AGE]\n",
    "adults_icustay_id_list = dataset_detail['icustay_id'].unique()\n",
    "X = X[X.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id'], ignore_index = True)\n",
    "X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "adults_icustay_id_list = np.sort(adults_icustay_id_list)\n",
    "new_length = len(X)\n",
    "\n",
    "# print out number of patients removed\n",
    "print(\"Number of patients removed: \", old_length - new_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(old_length)\n",
    "print(new_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"drop icustay_id with time span less than 48hrs\")\n",
    "def more_than_HOURS_ahead(adults_icustay_id_list, X):\n",
    "    drop_list = []\n",
    "    los_list = [] # calculating LOS ICU based on charttime\n",
    "    long_stays_id = [] # LOS longer than MAX DAYS days\n",
    "    last_charttime_list = []\n",
    "    seq_length = X.groupby(['icustay_id'],as_index=False).size()\n",
    "    id_count = 0\n",
    "    first_row_index = 0\n",
    "\n",
    "    while id_count < len(adults_icustay_id_list):\n",
    "        icustay_id = adults_icustay_id_list[id_count]\n",
    "        last_row_index = first_row_index + seq_length.iloc[id_count,1]-1\n",
    "        first_time = X.iat[first_row_index, X.columns.get_loc('charttime')]\n",
    "        last_time = X.iat[last_row_index, X.columns.get_loc('charttime')]\n",
    "        los = round(float((last_time - first_time).total_seconds()/60/60/24),4) # in days\n",
    "        if los < HOURS_AHEAD/24:\n",
    "            drop_list.append(icustay_id)\n",
    "        else:\n",
    "            los_list.append(los)\n",
    "            if los > MAX_DAYS:\n",
    "                long_stays_id.append(icustay_id)\n",
    "                last_charttime_list.append(last_time)\n",
    "        # udpate for the next icustay_id\n",
    "        first_row_index = last_row_index+1\n",
    "        id_count +=1\n",
    "    if len(long_stays_id) != len(last_charttime_list):\n",
    "        print('ERROR')\n",
    "    print(\"%d long stays\" % len(long_stays_id))\n",
    "    # drop all the rows with the saved icustay_id\n",
    "    print(\"there are %d id-s shorter than 48 hours\" % len(drop_list))\n",
    "    X = X[~X.icustay_id.isin(drop_list)]\n",
    "    id_list = X['icustay_id'].unique()\n",
    "    X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "    \n",
    "    return id_list, X, long_stays_id,last_charttime_list\n",
    "\n",
    "id_list, X, long_stays_id,last_charttime_list  = more_than_HOURS_ahead(adults_icustay_id_list, X)\n",
    "\n",
    "long = pd.DataFrame()\n",
    "long['icustay_id']  = long_stays_id\n",
    "long['last_time']  = last_charttime_list\n",
    "\n",
    "new_length = len(X)\n",
    "print(new_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting rows that are not within MAX_DAYS (35) period\n",
    "i = 0 # long df index\n",
    "drop_long_time = []\n",
    "    \n",
    "while i < len(long_stays_id):\n",
    "    j = 0\n",
    "    all_rows = X.index[X['icustay_id'] == long.loc[i,'icustay_id']].tolist()\n",
    "    while j < len(all_rows):\n",
    "        time = X.iat[all_rows[j], X.columns.get_loc('charttime')]\n",
    "        # if keep last MAX_DAYS \n",
    "        if (long.loc[i,'last_time'] - time).total_seconds() > MAX_DAYS*24*60*60:\n",
    "            drop_long_time.append(all_rows[j])\n",
    "            j +=1\n",
    "        else:\n",
    "            break\n",
    "    i +=1       \n",
    "X.drop(X.index[drop_long_time], inplace=True) \n",
    "\n",
    "# checking for 48h min length again\n",
    "id_list, X, long_stays_id,last_charttime_list  = more_than_HOURS_ahead(id_list, X)\n",
    "dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(id_list)].sort_values(by=['icustay_id'], ignore_index = True)\n",
    "\n",
    "new_length = len(X)\n",
    "print(new_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purpose, use small amount of data first\n",
    "if TESTING:\n",
    "    rest, id_list = train_test_split(id_list, test_size= TEST_SIZE, random_state=42)\n",
    "    X = X[X.icustay_id.isin(id_list)].sort_values(by=['icustay_id'])\n",
    "    dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(id_list)].sort_values(by=['icustay_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling , imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (TIME_SAMPLING and MOST_COMMON):\n",
    "    print(\"resampling: MOST_COMMON\")\n",
    "    # Resample the data using assigned interval,mode() for most common\n",
    "    X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL).mode().reset_index()\n",
    "elif TIME_SAMPLING:\n",
    "    print(\"resampling: MEAN & ZERO\")\n",
    "    # Sampling with different strategies per kind of variable\n",
    "    label = ['aki_stage']\n",
    "    skip = ['icustay_id', 'charttime', 'aki_stage']\n",
    "    if MAX_FEATURE_SET:\n",
    "        discrete_feat = ['sedative', 'vasopressor', 'vent', 'hadm_id']\n",
    "        skip.extend(discrete_feat)    \n",
    "    # all features that are not in skip are numeric\n",
    "    numeric_feat = list(X.columns.difference(skip))\n",
    "    \n",
    "    # Applying aggregation to features depending on their type\n",
    "    X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n",
    "    if MAX_FEATURE_SET:\n",
    "        X_discrete = X[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "    X_numeric = X[numeric_feat].mean() \n",
    "    X_label = X['aki_stage'].max()\n",
    "    print(\"Merging sampled features\")\n",
    "    try:\n",
    "        X = pd.concat([X_numeric, X_discrete,X_label], axis=1).reset_index()\n",
    "    except:\n",
    "        X = pd.concat([X_numeric,X_label], axis=1).reset_index()\n",
    "print(X.shape)\n",
    "#Label forward fill\n",
    "X['aki_stage'] = X['aki_stage'].ffill(limit=RESAMPLE_LIMIT)\n",
    "\n",
    "new_length = len(X) \n",
    "print(new_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Imputation.\")\n",
    "# do imputation of label with zero if there are still missing values\n",
    "X['aki_stage'] = X['aki_stage'].fillna(0)\n",
    "\n",
    "# using most common within each icustay_id\n",
    "if IMPUTE_EACH_ID:\n",
    "    column_name = list(X.columns)\n",
    "    column_name.remove(column_name[0]) \n",
    "    for feature in column_name:\n",
    "        X.loc[X[feature].isnull(), feature] = X.icustay_id.map(fast_mode(X, ['icustay_id'], feature).set_index('icustay_id')[feature])       \n",
    "\n",
    "# imputation based on whole column\n",
    "if IMPUTE_COLUMN:\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy= IMPUTE_METHOD)\n",
    "    cols = list(X.columns)\n",
    "    cols = cols[2:23]\n",
    "    X[cols]=imp.fit_transform(X[cols])  \n",
    "\n",
    "# If no imputation method selected or only impute each id, for the remaining nan impute direclty with FILL_VALUE\n",
    "X = X.fillna(FILL_VALUE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the number of columns of X\n",
    "print(len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more comfortable to review in this order\n",
    "try:\n",
    "    cols = ['icustay_id', 'charttime','aki_stage','hadm_id','aniongap_avg','bicarbonate_avg', 'bun_avg','chloride_avg',\n",
    "            'creat','diasbp_mean', 'glucose_avg', 'heartrate_mean', 'hematocrit_avg','hemoglobin_avg', \n",
    "            'potassium_avg', 'resprate_mean', 'sodium_avg','spo2_mean', 'sysbp_mean', 'uo_rt_12hr', \n",
    "            'uo_rt_24hr', 'uo_rt_6hr','wbc_avg', 'sedative', 'vasopressor', 'vent' ]\n",
    "    X = X[cols]\n",
    "    print(\"success\")\n",
    "except:\n",
    "    try:\n",
    "        cols = ['icustay_id', 'charttime','aki_stage','creat','uo_rt_12hr', 'uo_rt_24hr', 'uo_rt_6hr']\n",
    "        X = X[cols]\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessed data to csv\n",
    "X.to_csv('data/preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print headers of X\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"binarise labels\")\n",
    "if ALL_STAGES:\n",
    "    pass\n",
    "elif CLASS1:\n",
    "    X.loc[X['aki_stage'] > 1, 'aki_stage'] = 1\n",
    "elif CLASS2:\n",
    "    X.loc[X['aki_stage'] < 2, 'aki_stage'] = 0\n",
    "    X.loc[X['aki_stage'] > 1, 'aki_stage'] = 1\n",
    "elif CLASS3:\n",
    "    X.loc[X['aki_stage'] < 3, 'aki_stage'] = 0\n",
    "    X.loc[X['aki_stage'] > 2, 'aki_stage'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHIFTING labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Shifting the labels 48 h\") # by 8 position : 6h sampling*8=48h and ffil 8 newly empty ones\n",
    "X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].shift(-(HOURS_AHEAD // int(SAMPLING_INTERVAL[:-1])))\n",
    "X = X.dropna(subset=['aki_stage'])\n",
    "X['icustay_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add categorical features (details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_detail.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start preprocessing not time dependent data\")\n",
    "if MAX_FEATURE_SET:\n",
    "    #extract datasets based on id_list\n",
    "    dataset_detail = dataset_detail.loc[dataset_detail['icustay_id'].isin(id_list)]\n",
    "    #sort by ascending order\n",
    "    dataset_detail = dataset_detail.sort_values(by=['icustay_id'])\n",
    "    subject_id = dataset_detail[\"subject_id\"].unique()\n",
    "    #transfrom categorical data to binary form\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop('gender')))\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop(\"ethnicity_grouped\")))\n",
    "    # dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop('admission_type')))\n",
    "    dataset_detail = dataset_detail.drop(['subject_id', 'hadm_id'], axis=1)\n",
    "    X =  pd.merge(X, dataset_detail, on = [\"icustay_id\"], how= \"left\", copy = False) \n",
    "    numeric_feat.append('admission_age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names =['Anion gap', 'Bicarbonate', 'Blood Urea Nitrogen', 'Chloride', 'Creatinine', 'Diastolic BP', 'Glucose', 'Heart rate', \n",
    "            'Hematocrit', 'Hemoglobin', 'Potassium', 'Respiratory rate', 'Sodium', 'Oxygen saturation', 'Systolic BP', 'Urine output 12h', 'Urine output 24h', 'Urine output 6h',\n",
    "            'White cell count', 'Sedative', 'Vasopressor', 'Ventilation', 'Age', 'Female gender', 'Male gender', 'Asian ethnicity', 'Black ethnicity', 'Hispanic ethnicity', 'Native american', \n",
    "            'Other ethnicity', 'Ethnicity unknown', 'White ethnicity', 'Elective admission', 'Emergency admission', 'Urgent admission']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save X\n",
    "X.to_csv(os.path.join(data_path, 'preprocessed_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data_ours\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"read csv files\")\n",
    "#reading csv files\n",
    "X = pd.read_csv(DATA_PATH_stages, sep= SEPARATOR)\n",
    "# print column names\n",
    "print(X.columns)\n",
    "# print number of rows and columns\n",
    "print(X.shape)\n",
    "X.drop([\"aki_stage_creat\", \"aki_stage_uo\"], axis = 1, inplace = True)\n",
    "#remove totally empty rows \n",
    "X = X.dropna(how = 'all', subset = ['creat','uo_rt_6hr','uo_rt_12hr','uo_rt_24hr','aki_stage'])\n",
    "print(\"convert charttime to timestamp\")\n",
    "X['charttime'] = pd.to_datetime(X['charttime'])\n",
    "\n",
    "#merge rows if they have exact timestamp within same icustay_id AL : it substitutes missing values with zero\n",
    "#X = X.groupby(['icustay_id', 'charttime']).sum().reset_index(['icustay_id', 'charttime'])\n",
    "\n",
    "dataset_detail = pd.read_csv(DATA_PATH_detail, sep= SEPARATOR)  #age constraint\n",
    "dataset_detail.drop(['dod', 'admittime','dischtime', 'los_hospital','ethnicity','hospital_expire_flag', 'hospstay_seq',\n",
    "       'first_hosp_stay', 'intime', 'outtime', 'los_icu', 'icustay_seq','first_icu_stay'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labs = pd.read_csv(DATA_PATH_labs, sep= SEPARATOR) # 'bands lactate platelet ptt inr pt\n",
    "# dataset_labs.drop(['albumin_min', 'albumin_max','bilirubin_min', 'bilirubin_max','bands_min', 'bands_max',\n",
    "#                    'lactate_min', 'lactate_max','platelet_min', 'platelet_max','ptt_min', 'ptt_max', \n",
    "#                    'inr_min', 'inr_max', 'pt_min', 'pt_max'], axis = 1, inplace = True)\n",
    "dataset_labs = dataset_labs.dropna(subset=['charttime'])\n",
    "dataset_labs = dataset_labs.dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "dataset_labs['charttime'] = pd.to_datetime(dataset_labs['charttime'])\n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "if  MAX_FEATURE_SET:\n",
    "    dataset_vitals = pd.read_csv(DATA_PATH_vitals, sep= SEPARATOR)  \n",
    "    dataset_vents = pd.read_csv(DATA_PATH_vents , sep= SEPARATOR)\n",
    "    #dataset_icd = pd.read_csv(DATA_PATH_icd, sep= SEPARATOR)\n",
    "    # dataset_vitals.drop([\"heartrate_min\", \"heartrate_max\",\"sysbp_min\", \"sysbp_max\",\"diasbp_min\", \"diasbp_max\",\n",
    "    #                     'meanbp_min','meanbp_max', 'meanbp_mean','tempc_min', 'tempc_max', 'tempc_mean',\n",
    "    #                     \"resprate_min\", \"resprate_max\", \"spo2_min\", \"spo2_max\", \"glucose_min\", \"glucose_max\"], axis = 1, inplace = True)    \n",
    "    print(\"convert charttime to timestamp\")\n",
    "    dataset_vitals['charttime'] = pd.to_datetime(dataset_vitals['charttime'])\n",
    "    dataset_vents['charttime'] = pd.to_datetime(dataset_vents['charttime'])\n",
    "    dataset_vitals = dataset_vitals.sort_values(by=['icustay_id', 'charttime'])\n",
    "    dataset_vents = dataset_vents.sort_values(by=['icustay_id', 'charttime'])\n",
    "    # AL drop those where all columns are nan (empty rows)\n",
    "    dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')   \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(datetime.now())\n",
    "# # Labs file: instead of min and max their avg\n",
    "# counter = 0\n",
    "# col1 = 4\n",
    "# col2 = 5\n",
    "# null_l = [] # no null values in those that are different\n",
    "# changed = 0 # 4316 records changed to avg\n",
    "\n",
    "# while counter < 11:\n",
    "#     row = 0\n",
    "# # find where min and max are different and save their row indices \n",
    "#     while row < len(dataset_labs):\n",
    "#         a = dataset_labs.iloc[row,col1]\n",
    "#         b = dataset_labs.iloc[row,col2]\n",
    "#         if a==b or (np.isnan(a) and np.isnan(b)):\n",
    "#             pass\n",
    "#         elif a!=b:\n",
    "#             changed +=1\n",
    "#             avg = (a+b)/2\n",
    "#             dataset_labs.iloc[row,col1] = avg\n",
    "#             if (np.isnan(a) and ~np.isnan(b)) or (np.isnan(b) and ~np.isnan(a)):\n",
    "#                 null_l.append(row)\n",
    "#         else:\n",
    "#             print(a)\n",
    "#             print(b)\n",
    "#         row +=1       \n",
    "#     # delete the redundant column max, update counters\n",
    "#     dataset_labs.drop(dataset_labs.columns[col2], axis=1, inplace = True)\n",
    "#     counter = counter+1\n",
    "#     col1 = col1+1\n",
    "#     col2 = col2+1\n",
    "\n",
    "# dataset_labs.columns = ['subject_id','hadm_id', 'icustay_id', 'charttime', 'aniongap_avg', 'bicarbonate_avg', \n",
    "#                         'creatinine_avg', 'chloride_avg', 'glucose_avg', 'hematocrit_avg','hemoglobin_avg',\n",
    "#                         'potassium_avg', 'sodium_avg', 'bun_avg', 'wbc_avg']\n",
    "# if len(null_l)>0:\n",
    "#     print(\"null values encountered\")\n",
    "# print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Merge creatinine and glucose.\")\n",
    "# # merge creatinine from labs and set with labels\n",
    "# creat_l = dataset_labs[['icustay_id','charttime','creatinine_avg']].copy()\n",
    "# creat_l = creat_l.dropna(subset=['creatinine_avg'])\n",
    "# creat = X[['icustay_id','charttime', 'creat']].copy()\n",
    "# creat = creat.dropna(subset=['creat'])\n",
    "# creat_l = creat_l.rename(columns={\"creatinine_avg\": \"creat\"})\n",
    "# creat = creat._append(creat_l, ignore_index=True)\n",
    "# creat.drop_duplicates(inplace = True)\n",
    "# #delete old columns\n",
    "# dataset_labs.drop([\"creatinine_avg\"], axis = 1, inplace = True)\n",
    "# dataset_labs = dataset_labs.dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "# X.drop([\"creat\"], axis = 1, inplace = True)\n",
    "# #merge new column\n",
    "# X = pd.merge(X, creat, on = [\"icustay_id\", \"charttime\"], sort = True, how= \"outer\", copy = False)\n",
    "\n",
    "# if MAX_FEATURE_SET:\n",
    "#     # merge glucose from vitals and labs\n",
    "#     glucose_v = dataset_vitals[['subject_id','hadm_id','icustay_id','charttime', 'glucose_mean']].copy()\n",
    "#     glucose_v = glucose_v.dropna(subset=['glucose_mean'])\n",
    "#     glucose = dataset_labs[['subject_id','hadm_id','icustay_id','charttime', 'glucose_avg']].copy()\n",
    "#     glucose = glucose.dropna(subset=['glucose_avg'])\n",
    "#     glucose_v = glucose_v.rename(columns={\"glucose_mean\": \"glucose_avg\"})\n",
    "#     glucose = glucose._append(glucose_v, ignore_index=True)\n",
    "#     glucose.drop_duplicates(inplace = True)\n",
    "#     #delete old columns\n",
    "#     dataset_labs.drop([\"glucose_avg\"], axis = 1, inplace = True)\n",
    "#     dataset_vitals.drop([\"glucose_mean\"], axis = 1, inplace = True)\n",
    "#     dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')\n",
    "#     #merge new column\n",
    "#     dataset_labs = pd.merge(dataset_labs, glucose, on = ['subject_id','hadm_id','icustay_id','charttime',], sort = True, how= \"outer\", copy = False)\n",
    "    \n",
    "# dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "# X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_in_chunks(X, dataset, on, chunk_size):\n",
    "    num_chunks = len(dataset) // chunk_size\n",
    "    for i in range(num_chunks):\n",
    "        chunk = dataset[i*chunk_size:(i+1)*chunk_size]\n",
    "        yield pd.merge(X, chunk, on=on, how=\"outer\", copy=False)\n",
    "\n",
    "chunk_size = 5000\n",
    "\n",
    "if MAX_FEATURE_SET:\n",
    "    X_generator = merge_in_chunks(X, dataset_labs, [\"icustay_id\", \"charttime\"], chunk_size)\n",
    "    X = pd.concat(X_generator, ignore_index=True)\n",
    "\n",
    "    X_generator = merge_in_chunks(X, dataset_vitals, [\"icustay_id\", \"charttime\",\"subject_id\", \"hadm_id\"], chunk_size)\n",
    "    X = pd.concat(X_generator, ignore_index=True)\n",
    "\n",
    "    X_generator = merge_in_chunks(X, dataset_vents, [\"icustay_id\", \"charttime\"], chunk_size)\n",
    "    X = pd.concat(X_generator, ignore_index=True)\n",
    "\n",
    "    X.drop([\"subject_id\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging labs, vitals and vents files\")\n",
    "if MAX_FEATURE_SET:\n",
    "    X = pd.merge(X, dataset_labs, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False)\n",
    "    X = pd.merge(X, dataset_vitals, on = [\"icustay_id\", \"charttime\",\"subject_id\", \"hadm_id\"], how= \"outer\", copy = False)\n",
    "    X = pd.merge(X, dataset_vents, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False) \n",
    "    X.drop([\"subject_id\"], axis = 1, inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging labs, vitals and vents files\")\n",
    "if MAX_FEATURE_SET:\n",
    "    # Split X into two halves\n",
    "    X1 = X.iloc[:len(X)//2]\n",
    "    X2 = X.iloc[len(X)//2:]\n",
    "    print(\"Split X into two halves\")\n",
    "\n",
    "    # Merge each half with the other DataFrames\n",
    "    X1 = pd.merge(X1, dataset_labs, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False)\n",
    "    print(\"Merged X1 with labs\")\n",
    "    X1 = pd.merge(X1, dataset_vitals, on = [\"icustay_id\", \"charttime\",\"subject_id\", \"hadm_id\"], how= \"outer\", copy = False)\n",
    "    print(\"Merged X1 with vitals\")\n",
    "    X1 = pd.merge(X1, dataset_vents, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False) \n",
    "    print(\"Merged X1 with vents\")\n",
    "    X1.drop([\"subject_id\"], axis = 1, inplace = True)\n",
    "\n",
    "    # Write X1 to disk and delete it from memory\n",
    "    X1.to_csv('X1.csv')\n",
    "    del X1\n",
    "    \n",
    "    print(\"Wrote X1 to disk\")\n",
    "\n",
    "    X2 = pd.merge(X2, dataset_labs, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False)\n",
    "    print(\"Merged X2 with labs\")\n",
    "    X2 = pd.merge(X2, dataset_vitals, on = [\"icustay_id\", \"charttime\",\"subject_id\", \"hadm_id\"], how= \"outer\", copy = False)\n",
    "    print(\"Merged X2 with vitals\")\n",
    "    X2 = pd.merge(X2, dataset_vents, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False) \n",
    "    print(\"Merged X2 with vents\")\n",
    "    X2.drop([\"subject_id\"], axis = 1, inplace = True)\n",
    "\n",
    "    # Write X2 to disk and delete it from memory\n",
    "    X2.to_csv('X2.csv')\n",
    "    del X2\n",
    "\n",
    "    print(\"Wrote X2 to disk\")\n",
    "    \n",
    "    # Read X1 and X2 from disk and concatenate the results\n",
    "    X1 = pd.read_csv('X1.csv')\n",
    "    print(\"Read X1 from disk\")\n",
    "    X2 = pd.read_csv('X2.csv')\n",
    "    print(\"Read X2 from disk\")\n",
    "    X = pd.concat([X1, X2])\n",
    "    print(\"Concatenated X1 and X2\")\n",
    "\n",
    "    # Delete X1 and X2 from memory\n",
    "    del X1\n",
    "    del X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "print(\"Merging labs, vitals and vents files\")\n",
    "if MAX_FEATURE_SET:\n",
    "    # Split X into two halves\n",
    "    X1 = X.iloc[:len(X)//2]\n",
    "    X2 = X.iloc[len(X)//2:]\n",
    "    print(\"Split X into two halves\")\n",
    "\n",
    "    # Convert X1 and X2 to Dask DataFrames\n",
    "    X1 = dd.from_pandas(X1, npartitions=2)\n",
    "    X2 = dd.from_pandas(X2, npartitions=2)\n",
    "\n",
    "    # Merge each half with the other DataFrames\n",
    "    X1 = dd.merge(X1, dataset_labs, on = [\"icustay_id\", \"charttime\"], how= \"outer\")\n",
    "    print(\"Merged X1 with labs\")\n",
    "    X1 = dd.merge(X1, dataset_vitals, on = [\"icustay_id\", \"charttime\",\"subject_id\", \"hadm_id\"], how= \"outer\")\n",
    "    print(\"Merged X1 with vitals\")\n",
    "    X1 = dd.merge(X1, dataset_vents, on = [\"icustay_id\", \"charttime\"], how= \"outer\")\n",
    "    print(\"Merged X1 with vents\")\n",
    "    X1 = X1.drop([\"subject_id\"], axis = 1)\n",
    "\n",
    "    X2 = dd.merge(X2, dataset_labs, on = [\"icustay_id\", \"charttime\"], how= \"outer\")\n",
    "    print(\"Merged X2 with labs\")\n",
    "    X2 = dd.merge(X2, dataset_vitals, on = [\"icustay_id\", \"charttime\",\"subject_id\", \"hadm_id\"], how= \"outer\")\n",
    "    print(\"Merged X2 with vitals\")\n",
    "    X2 = dd.merge(X2, dataset_vents, on = [\"icustay_id\", \"charttime\"], how= \"outer\")\n",
    "    print(\"Merged X2 with vents\")\n",
    "    X2 = X2.drop([\"subject_id\"], axis = 1)\n",
    "\n",
    "    # Concatenate X1 and X2\n",
    "    X = dd.concat([X1, X2])\n",
    "    print(\"Concatenated X1 and X2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read X1 from disk\n",
    "X = pd.read_csv('X1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'charttime' column\n",
    "X_numeric = X.drop('charttime', axis=1)\n",
    "\n",
    "# Calculate the correlation with 'aki_stage'\n",
    "correlation_to_aki_stage = X_numeric.corrwith(X_numeric['aki_stage'])\n",
    "\n",
    "# Plotting the correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_to_aki_stage.sort_values().plot(kind='bar')\n",
    "plt.title('Correlation of Attributes with aki_stage')\n",
    "plt.ylabel('Correlation')\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/analysis/correlation_to_aki_stage_X1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between numeric variables\n",
    "corr = X_numeric.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig('data/analysis/correlation_matrix_X1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_detail.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start preprocessing time dependent data\") \n",
    "print(\"Removing patients under the min age\")\n",
    "old_length = len(X)\n",
    "dataset_detail = dataset_detail.loc[dataset_detail['admission_age'] >= ADULTS_MIN_AGE]\n",
    "adults_icustay_id_list = dataset_detail['icustay_id'].unique()\n",
    "X = X[X.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id'], ignore_index = True)\n",
    "X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "adults_icustay_id_list = np.sort(adults_icustay_id_list)\n",
    "new_length = len(X)\n",
    "\n",
    "# print out number of patients removed\n",
    "print(\"Number of patients removed: \", old_length - new_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(old_length)\n",
    "print(new_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"drop icustay_id with time span less than 48hrs\")\n",
    "def more_than_HOURS_ahead(adults_icustay_id_list, X):\n",
    "    drop_list = []\n",
    "    los_list = [] # calculating LOS ICU based on charttime\n",
    "    long_stays_id = [] # LOS longer than MAX DAYS days\n",
    "    last_charttime_list = []\n",
    "    seq_length = X.groupby(['icustay_id'],as_index=False).size()\n",
    "    id_count = 0\n",
    "    first_row_index = 0\n",
    "\n",
    "    while id_count < len(adults_icustay_id_list):\n",
    "        icustay_id = adults_icustay_id_list[id_count]\n",
    "        last_row_index = first_row_index + seq_length.iloc[id_count,1]-1\n",
    "        first_time = X.iat[first_row_index, X.columns.get_loc('charttime')]\n",
    "        last_time = X.iat[last_row_index, X.columns.get_loc('charttime')]\n",
    "        los = round(float((last_time - first_time).total_seconds()/60/60/24),4) # in days\n",
    "        if los < HOURS_AHEAD/24:\n",
    "            drop_list.append(icustay_id)\n",
    "        else:\n",
    "            los_list.append(los)\n",
    "            if los > MAX_DAYS:\n",
    "                long_stays_id.append(icustay_id)\n",
    "                last_charttime_list.append(last_time)\n",
    "        # udpate for the next icustay_id\n",
    "        first_row_index = last_row_index+1\n",
    "        id_count +=1\n",
    "    if len(long_stays_id) != len(last_charttime_list):\n",
    "        print('ERROR')\n",
    "    print(\"%d long stays\" % len(long_stays_id))\n",
    "    # drop all the rows with the saved icustay_id\n",
    "    print(\"there are %d id-s shorter than 48 hours\" % len(drop_list))\n",
    "    X = X[~X.icustay_id.isin(drop_list)]\n",
    "    id_list = X['icustay_id'].unique()\n",
    "    X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "    \n",
    "    return id_list, X, long_stays_id,last_charttime_list\n",
    "\n",
    "id_list, X, long_stays_id,last_charttime_list  = more_than_HOURS_ahead(adults_icustay_id_list, X)\n",
    "\n",
    "long = pd.DataFrame()\n",
    "long['icustay_id']  = long_stays_id\n",
    "long['last_time']  = last_charttime_list\n",
    "\n",
    "new_length = len(X)\n",
    "print(new_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting rows that are not within MAX_DAYS (35) period\n",
    "i = 0 # long df index\n",
    "drop_long_time = []\n",
    "    \n",
    "while i < len(long_stays_id):\n",
    "    j = 0\n",
    "    all_rows = X.index[X['icustay_id'] == long.loc[i,'icustay_id']].tolist()\n",
    "    while j < len(all_rows):\n",
    "        time = X.iat[all_rows[j], X.columns.get_loc('charttime')]\n",
    "        # if keep last MAX_DAYS \n",
    "        if (long.loc[i,'last_time'] - time).total_seconds() > MAX_DAYS*24*60*60:\n",
    "            drop_long_time.append(all_rows[j])\n",
    "            j +=1\n",
    "        else:\n",
    "            break\n",
    "    i +=1       \n",
    "X.drop(X.index[drop_long_time], inplace=True) \n",
    "\n",
    "# checking for 48h min length again\n",
    "id_list, X, long_stays_id,last_charttime_list  = more_than_HOURS_ahead(id_list, X)\n",
    "dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(id_list)].sort_values(by=['icustay_id'], ignore_index = True)\n",
    "\n",
    "new_length = len(X)\n",
    "print(new_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purpose, use small amount of data first\n",
    "if TESTING:\n",
    "    rest, id_list = train_test_split(id_list, test_size= TEST_SIZE, random_state=42)\n",
    "    X = X[X.icustay_id.isin(id_list)].sort_values(by=['icustay_id'])\n",
    "    dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(id_list)].sort_values(by=['icustay_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling , imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if (TIME_SAMPLING and MOST_COMMON):\n",
    "    print(\"resampling: MOST_COMMON\")\n",
    "    # Resample the data using assigned interval,mode() for most common\n",
    "    X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL).mode().reset_index()\n",
    "elif TIME_SAMPLING:\n",
    "    print(\"resampling: MEAN & ZERO\")\n",
    "    # Sampling with different strategies per kind of variable\n",
    "    label = ['aki_stage']\n",
    "    skip = ['icustay_id', 'charttime', 'aki_stage']\n",
    "    if MAX_FEATURE_SET:\n",
    "        discrete_feat = ['sedative', 'vasopressor', 'vent', 'hadm_id']\n",
    "        skip.extend(discrete_feat)    \n",
    "    # all features that are not in skip are numeric\n",
    "    numeric_feat = list(X.columns.difference(skip))\n",
    "    \n",
    "    # Applying aggregation to features depending on their type\n",
    "    X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n",
    "    if MAX_FEATURE_SET:\n",
    "        X_discrete = X[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "    X_numeric = X[numeric_feat].mean() \n",
    "    X_label = X['aki_stage'].max()\n",
    "    print(\"Merging sampled features\")\n",
    "    try:\n",
    "        X = pd.concat([X_numeric, X_discrete,X_label], axis=1).reset_index()\n",
    "    except:\n",
    "        X = pd.concat([X_numeric,X_label], axis=1).reset_index()\n",
    "print(X.shape)\n",
    "#Label forward fill\n",
    "X['aki_stage'] = X['aki_stage'].ffill(limit=RESAMPLE_LIMIT)\n",
    "\n",
    "new_length = len(X) \n",
    "print(new_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Imputation.\")\n",
    "# do imputation of label with zero if there are still missing values\n",
    "X['aki_stage'] = X['aki_stage'].fillna(0)\n",
    "\n",
    "# using most common within each icustay_id\n",
    "if IMPUTE_EACH_ID:\n",
    "    column_name = list(X.columns)\n",
    "    column_name.remove(column_name[0]) \n",
    "    for feature in column_name:\n",
    "        X.loc[X[feature].isnull(), feature] = X.icustay_id.map(fast_mode(X, ['icustay_id'], feature).set_index('icustay_id')[feature])       \n",
    "\n",
    "# imputation based on whole column\n",
    "if IMPUTE_COLUMN:\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy= IMPUTE_METHOD)\n",
    "    cols = list(X.columns)\n",
    "    cols = cols[2:23]\n",
    "    X[cols]=imp.fit_transform(X[cols])  \n",
    "\n",
    "# If no imputation method selected or only impute each id, for the remaining nan impute direclty with FILL_VALUE\n",
    "X = X.fillna(FILL_VALUE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the number of columns of X\n",
    "print(len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more comfortable to review in this order\n",
    "try:\n",
    "    cols = ['icustay_id', 'charttime','aki_stage','hadm_id','aniongap_avg','bicarbonate_avg', 'bun_avg','chloride_avg',\n",
    "            'creat','diasbp_mean', 'glucose_avg', 'heartrate_mean', 'hematocrit_avg','hemoglobin_avg', \n",
    "            'potassium_avg', 'resprate_mean', 'sodium_avg','spo2_mean', 'sysbp_mean', 'uo_rt_12hr', \n",
    "            'uo_rt_24hr', 'uo_rt_6hr','wbc_avg', 'sedative', 'vasopressor', 'vent' ]\n",
    "    X = X[cols]\n",
    "    print(\"success\")\n",
    "except:\n",
    "    try:\n",
    "        cols = ['icustay_id', 'charttime','aki_stage','creat','uo_rt_12hr', 'uo_rt_24hr', 'uo_rt_6hr']\n",
    "        X = X[cols]\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessed data to csv\n",
    "X.to_csv('data/preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print headers of X\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"binarise labels\")\n",
    "if ALL_STAGES:\n",
    "    pass\n",
    "elif CLASS1:\n",
    "    X.loc[X['aki_stage'] > 1, 'aki_stage'] = 1\n",
    "elif CLASS2:\n",
    "    X.loc[X['aki_stage'] < 2, 'aki_stage'] = 0\n",
    "    X.loc[X['aki_stage'] > 1, 'aki_stage'] = 1\n",
    "elif CLASS3:\n",
    "    X.loc[X['aki_stage'] < 3, 'aki_stage'] = 0\n",
    "    X.loc[X['aki_stage'] > 2, 'aki_stage'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHIFTING labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Shifting the labels 48 h\") # by 8 position : 6h sampling*8=48h and ffil 8 newly empty ones\n",
    "X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].shift(-(HOURS_AHEAD // int(SAMPLING_INTERVAL[:-1])))\n",
    "X = X.dropna(subset=['aki_stage'])\n",
    "X['icustay_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add categorical features (details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_detail.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start preprocessing not time dependent data\")\n",
    "if MAX_FEATURE_SET:\n",
    "    #extract datasets based on id_list\n",
    "    dataset_detail = dataset_detail.loc[dataset_detail['icustay_id'].isin(id_list)]\n",
    "    #sort by ascending order\n",
    "    dataset_detail = dataset_detail.sort_values(by=['icustay_id'])\n",
    "    subject_id = dataset_detail[\"subject_id\"].unique()\n",
    "    #transfrom categorical data to binary form\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop('gender')))\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop(\"ethnicity_grouped\")))\n",
    "    # dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop('admission_type')))\n",
    "    dataset_detail = dataset_detail.drop(['subject_id', 'hadm_id'], axis=1)\n",
    "    X =  pd.merge(X, dataset_detail, on = [\"icustay_id\"], how= \"left\", copy = False) \n",
    "    numeric_feat.append('admission_age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names =['Anion gap', 'Bicarbonate', 'Blood Urea Nitrogen', 'Chloride', 'Creatinine', 'Diastolic BP', 'Glucose', 'Heart rate', \n",
    "            'Hematocrit', 'Hemoglobin', 'Potassium', 'Respiratory rate', 'Sodium', 'Oxygen saturation', 'Systolic BP', 'Urine output 12h', 'Urine output 24h', 'Urine output 6h',\n",
    "            'White cell count', 'Sedative', 'Vasopressor', 'Ventilation', 'Age', 'Female gender', 'Male gender', 'Asian ethnicity', 'Black ethnicity', 'Hispanic ethnicity', 'Native american', \n",
    "            'Other ethnicity', 'Ethnicity unknown', 'White ethnicity', 'Elective admission', 'Emergency admission', 'Urgent admission']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save X\n",
    "X.to_csv(os.path.join(data_path, 'preprocessed_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('data/analysis', exist_ok=True)\n",
    "\n",
    "# Summary statistics\n",
    "X.describe().to_csv('data/analysis/summary_statistics.csv')\n",
    "\n",
    "# Distribution of numeric values\n",
    "numeric_cols = ['aniongap_avg', 'bicarbonate_avg', 'bun_avg', 'chloride_avg', 'creat', 'diasbp_mean',\n",
    "                'glucose_avg', 'heartrate_mean', 'hematocrit_avg', 'hemoglobin_avg', 'potassium_avg', \n",
    "                'resprate_mean', 'sodium_avg', 'spo2_mean', 'sysbp_mean', 'uo_rt_12hr', 'uo_rt_24hr', \n",
    "                'uo_rt_6hr', 'wbc_avg', 'admission_age']\n",
    "for col in numeric_cols:\n",
    "    plt.figure()\n",
    "    sns.histplot(X[col].dropna())\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.savefig(f'data/analysis/{col}_distribution.png')\n",
    "\n",
    "# Distribution of categorical values\n",
    "categorical_cols = ['aki_stage', 'sedative', 'vasopressor', 'vent', 'F', 'M', 'asian', 'black', 'hispanic', 'native', 'other', 'unknown', 'white']\n",
    "for col in categorical_cols:\n",
    "    plt.figure()\n",
    "    sns.countplot(x=col, data=X)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.savefig(f'data/analysis/{col}_count.png')\n",
    "\n",
    "# Correlation between numeric variables\n",
    "corr = X[numeric_cols].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig('data/analysis/correlation_matrix.png')\n",
    "\n",
    "correlation_to_aki_stage = X.corrwith(X['aki_stage'])\n",
    "\n",
    "# Plotting the correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_to_aki_stage.sort_values().plot(kind='bar')\n",
    "plt.title('Correlation of Attributes with aki_stage')\n",
    "plt.ylabel('Correlation')\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/analysis/correlation_to_aki_stage_binarised.png')\n",
    "\n",
    "# Correlation between 'aki_stage' and other attributes\n",
    "for col in numeric_cols:\n",
    "    plt.figure()\n",
    "    X.groupby('aki_stage')[col].mean().plot(kind='bar')\n",
    "    plt.ylabel(col)\n",
    "    plt.title(f'Mean {col} by aki_stage')\n",
    "    plt.savefig(f'data/analysis/{col}_by_aki_stage.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print max values of colums \"admission_age\"\n",
    "print(X['admission_age'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = cap_data(X)\n",
    "X = normalise_data(X, numeric_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.sort_values(by=['icustay_id', 'charttime'])\n",
    "seq_lengths = X.groupby(['icustay_id'],as_index=False).size().sort_values(by = ['size'],ascending=False)\n",
    "sequence_length = seq_lengths.max() # the longest sequence per icustay-id\n",
    "print(sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AL re-write as try except to make it work as hadm_id is not used if only one csv file is used and none are merged\n",
    "try:\n",
    "    X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = X.shape[1]-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"divide dataset into train, test and validation sets\")\n",
    "id_train, id_test_val = train_test_split(id_list, test_size = SPLIT_SIZE, random_state = 42) # train set is 80%)\n",
    "print(\"train is %d\" % len(id_train))\n",
    "# remaining 20% split in halves as test and validation 10% and 10%\n",
    "id_valid, id_test = train_test_split(id_test_val, test_size = 0.5, random_state = 42) # test 10% valid 10%\n",
    "print(\"val and test are %d\" %len(id_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index = True) \n",
    "validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id']) \n",
    "\n",
    "test = test.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "train = train.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "validation = validation.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['icustay_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = test.groupby(['icustay_id'],as_index=False).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
