{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "#data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "# NN\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score, roc_auc_score, auc, accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from captum.attr import IntegratedGradients\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# the full files pathes are here\n",
    "DATA_PATH_stages=\"data/kdigo_stages_measured.csv\" \n",
    "DATA_PATH_labs = \"data/labs-kdigo_stages_measured.csv\" \n",
    "DATA_PATH_vitals = \"data/vitals-kdigo_stages_measured.csv\" \n",
    "DATA_PATH_vents = \"data/vents-vasopressor-sedatives-kdigo_stages_measured.csv\"\n",
    "DATA_PATH_detail=\"data/icustay_detail-kdigo_stages_measured.csv\" \n",
    "SEPARATOR=\";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Training on CPU') # On mac book GPU is not possible =() \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPUTE_EACH_ID = False # imputation within each icustay_id with most common value\n",
    "IMPUTE_COLUMN = False # imputation based on whole column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter as constant \n",
    "\n",
    "TESTING = False \n",
    "TEST_SIZE = 0.05\n",
    "\n",
    "SPLIT_SIZE = 0.2 \n",
    "MAX_DAYS = 35\n",
    "\n",
    "#which classifier to use, only run one classifier at one time \n",
    "CLASS1 = True   #AnyAKI\n",
    "#CLASS2 = False    #ModerateSevereAKI\n",
    "#CLASS3 = False    #SevereAKI\n",
    "ALL_STAGES = False # not binary label, each class separately 0,1,2,3\n",
    "    \n",
    "MAX_FEATURE_SET = True\n",
    "#DIAGNOSIS = False\n",
    "\n",
    "FIRST_TURN_POS = True # creating one label per one ICU stay id\n",
    "\n",
    "# resampling  and imputing\n",
    "TIME_SAMPLING = True \n",
    "SAMPLING_INTERVAL = '6H'\n",
    "RESAMPLE_LIMIT = 16 # 4 days*6h interval\n",
    "MOST_COMMON = False #resampling with most common\n",
    "# if MOST_COMMON is not applied,sampling with different strategies per kind of variable, \n",
    "# numeric variables use mean value, categorical variables use max value\n",
    "\n",
    "IMPUTE_METHOD = 'most_frequent' \n",
    "FILL_VALUE = 0 #fill missing value and ragged part of 3d array\n",
    "\n",
    "#Age constraints: adults\n",
    "ADULTS_MIN_AGE = 18\n",
    "ADULTS_MAX_AGE = 120\n",
    "\n",
    "NORMALIZATION = 'min-max' \n",
    "\n",
    "CAPPING_THRESHOLD_UPPER = 0.99\n",
    "CAPPING_THRESHOLD_LOWER = 0.01\n",
    "\n",
    "# How much time the prediction should occur (hours)\n",
    "HOURS_AHEAD = 48\n",
    "\n",
    "NORM_TYPE = 'min_max'\n",
    "\n",
    "RANDOM = 42\n",
    "\n",
    "#set changable info corresponding to each classifier as variables\n",
    "\n",
    "min_set =  [\"icustay_id\", \"charttime\", \"creat\", \"uo_rt_6hr\", \"uo_rt_12hr\", \"uo_rt_24hr\", \"aki_stage\"]\n",
    "\n",
    "max_set = ['icustay_id', 'charttime', 'aki_stage', 'hadm_id','aniongap_avg', 'bicarbonate_avg', \n",
    "           'bun_avg','chloride_avg', 'creat', 'diasbp_mean', 'glucose_avg', 'heartrate_mean',\n",
    "           'hematocrit_avg', 'hemoglobin_avg', 'potassium_avg', 'resprate_mean','sodium_avg', 'spo2_mean', 'sysbp_mean', \n",
    "           'uo_rt_12hr', 'uo_rt_24hr','uo_rt_6hr', 'wbc_avg', 'sedative', 'vasopressor', 'vent', 'age', 'F','M', \n",
    "           'asian', 'black', 'hispanic', 'native', 'other', 'unknown','white', 'ELECTIVE', 'EMERGENCY', 'URGENT']\n",
    "\n",
    "# LSTM\n",
    "batch_size = 5\n",
    "\n",
    "# naming model and plot\n",
    "classifier_name = \"None vs. Any AKI\"    ###change every time #Moderate vs. Severe #None vs. Any #Others vs. Severe\n",
    "plot_name = \"adult_AnyAKI_LR\"    ###change every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions used later\n",
    "\n",
    "def cap_data(df):\n",
    "    print(\"Capping between the {} and {} quantile\".format(CAPPING_THRESHOLD_LOWER, CAPPING_THRESHOLD_UPPER))\n",
    "    cap_mask = df.columns.difference(['icustay_id', 'charttime', 'aki_stage'])\n",
    "    df[cap_mask] = df[cap_mask].clip(df[cap_mask].quantile(CAPPING_THRESHOLD_LOWER),\n",
    "                                     df[cap_mask].quantile(CAPPING_THRESHOLD_UPPER),\n",
    "                                     axis=1)\n",
    "\n",
    "    return df\n",
    " \n",
    "    \n",
    "def normalise_data(df, norm_mask):\n",
    "    print(\"Normalizing in [0,1] with {} normalization\".format(NORMALIZATION))\n",
    "    \n",
    "    df[norm_mask] = (df[norm_mask] - df[norm_mask].min()) / (df[norm_mask].max() - df[norm_mask].min())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# impute missing value in resampleing data with most common based on each id\n",
    "def fast_mode(df, key_cols, value_col):\n",
    "    \"\"\" Calculate a column mode, by group, ignoring null values. \n",
    "    \n",
    "    key_cols : list of str - Columns to groupby for calculation of mode.\n",
    "    value_col : str - Column for which to calculate the mode. \n",
    "\n",
    "    Return\n",
    "    pandas.DataFrame\n",
    "        One row for the mode of value_col per key_cols group. If ties, returns the one which is sorted first. \"\"\"\n",
    "    return (df.groupby(key_cols + [value_col]).size() \n",
    "              .to_frame('counts').reset_index() \n",
    "              .sort_values('counts', ascending=False) \n",
    "              .drop_duplicates(subset=key_cols)).drop('counts',axis=1)\n",
    "\n",
    "\n",
    "#get max shape of 3d array\n",
    "def get_dimensions(array, level=0):   \n",
    "    yield level, len(array)\n",
    "    try:\n",
    "        for row in array:\n",
    "            yield from get_dimensions(row, level + 1)\n",
    "    except TypeError: #not an iterable\n",
    "        pass\n",
    "\n",
    "def get_max_shape(array):\n",
    "    dimensions = defaultdict(int)\n",
    "    for level, length in get_dimensions(array):\n",
    "        dimensions[level] = max(dimensions[level], length)\n",
    "    return [value for _, value in sorted(dimensions.items())]\n",
    "\n",
    "#pad the ragged 3d array to rectangular shape based on max size\n",
    "def iterate_nested_array(array, index=()):\n",
    "    try:\n",
    "        for idx, row in enumerate(array):\n",
    "            yield from iterate_nested_array(row, (*index, idx)) \n",
    "    except TypeError: # final level            \n",
    "        yield (*index, slice(len(array))), array # think of the types\n",
    "\n",
    "def pad(array, fill_value):\n",
    "    dimensions = get_max_shape(array)\n",
    "    result = np.full(dimensions, fill_value, dtype = np.float64)  \n",
    "    for index, value in iterate_nested_array(array):\n",
    "        result[index] = value \n",
    "    return result\n",
    "\n",
    "def bin_total(y_true, y_prob, n_bins):\n",
    "    bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "    # In sklearn.calibration.calibration_curve,\n",
    "    # the last value in the array is always 0.\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "def missing_bin(bin_array):\n",
    "    midpoint = \" \"    \n",
    "    if bin_array[0]==0:\n",
    "        midpoint = \"5%, \"\n",
    "    if bin_array[1]==0:\n",
    "        midpoint = midpoint + \"15%, \"\n",
    "    if bin_array[2]==0:\n",
    "        midpoint = midpoint + \"25%, \"\n",
    "    if bin_array[3]==0:\n",
    "        midpoint = midpoint + \"35%, \" \n",
    "    if bin_array[4]==0:\n",
    "        midpoint = midpoint + \"45%, \"\n",
    "    if bin_array[5]==0:\n",
    "        midpoint = midpoint + \"55%, \"\n",
    "    if bin_array[6]==0:\n",
    "        midpoint = midpoint + \"65%, \"\n",
    "    if bin_array[7]==0:\n",
    "        midpoint = midpoint + \"75%, \"\n",
    "    if bin_array[8]==0:\n",
    "        midpoint = midpoint + \"85%, \"\n",
    "    if bin_array[9]==0:\n",
    "        midpoint = midpoint + \"95%, \"\n",
    "    return \"The missing bins have midpoint values of \"+ str(midpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/data/preprocessed_data_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the columns in string format\n",
    "X.drop(['first_hosp_stay'], axis=1, inplace = True)\n",
    "X.drop(['first_icu_stay'], axis=1, inplace = True)\n",
    "X.drop(['ethnicity'], axis=1, inplace = True)\n",
    "X.drop(['admittime'], axis=1, inplace = True)\n",
    "X.drop(['dischtime'], axis=1, inplace = True)\n",
    "X.drop(['intime'], axis=1, inplace = True)\n",
    "X.drop(['outtime'], axis=1, inplace = True)\n",
    "X.drop(['dod'], axis=1, inplace = True)\n",
    "X.drop(['charttime'], axis=1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations\n",
    "corr = X.corr()\n",
    "\n",
    "# Create a mask for correlations above 0.9\n",
    "mask = np.abs(corr) > 0.9\n",
    "\n",
    "# Create heatmap using the mask\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(corr[mask], annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation of all features above 0.9')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations\n",
    "correlations = X.corr()['aki_stage'].drop('aki_stage').sort_values(ascending=False)\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(20,10))\n",
    "correlations.plot(kind='bar')\n",
    "plt.title('Correlation of all features with the target variable')\n",
    "plt.ylabel('Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show the distribution of "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medinf",
   "language": "python",
   "name": "medinf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
